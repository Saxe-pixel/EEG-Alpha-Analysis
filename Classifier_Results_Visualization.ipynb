{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Results Visualization\n",
    "\n",
    "This notebook visualizes and analyzes results from the EC/EO classifier pipeline across old (30 subjects) and new (100 subjects) datasets.\n",
    "\n",
    "It includes:\n",
    "- Data distributions (age, alpha power)\n",
    "- FOOOF center frequency analysis\n",
    "- Model performance metrics (ROC, confusion matrices, accuracy)\n",
    "- PSD vs FOOOF comparisons\n",
    "- Statistical tests comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, roc_auc_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import joblib\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Clinical-only Windows configuration (explicit paths)\n",
    "# -----------------\n",
    "\n",
    "def _is_windows() -> bool:\n",
    "    return platform.system() == \"Windows\"\n",
    "\n",
    "def _detect_notebook_path() -> Optional[Path]:\n",
    "    \"\"\"Detect the current notebook path (best-effort).\"\"\"\n",
    "    try:\n",
    "        vsc = globals().get(\"__vsc_ipynb_file__\", None)\n",
    "        if vsc:\n",
    "            p = Path(str(vsc)).expanduser()\n",
    "            if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for key in (\"NOTEBOOK_PATH\", \"IPYNB_PATH\"):\n",
    "        v = os.getenv(key)\n",
    "        if v:\n",
    "            try:\n",
    "                p = Path(v).expanduser()\n",
    "                if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                    return p.resolve()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "NOTEBOOK_PATH = _detect_notebook_path()\n",
    "NOTEBOOK_DIR = NOTEBOOK_PATH.parent if NOTEBOOK_PATH is not None else Path.cwd().resolve()\n",
    "\n",
    "# Where `EC_EO_Classifier.ipynb` writes model outputs (within this repo by default)\n",
    "OUTPUTS_ROOT = NOTEBOOK_DIR / \"outputs\"\n",
    "OUTPUTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Clinical raw data roots (Windows) ----------\n",
    "# Old dataset: 30 subjects (Open_marked + Closed_marked)\n",
    "OLD_OPEN_DIR = r\"E:\\\\Saxe_sandkasse\\\\30EOEC_filer\\\\Open_marked\"\n",
    "OLD_CLOSED_DIR = r\"E:\\\\Saxe_sandkasse\\\\30EOEC_filer\\\\Closed_marked\"\n",
    "\n",
    "# New dataset: 100 subjects (preprocessed epoch FIFs)\n",
    "NEW_PROCESSED_DIR = r\"G:\\\\ChristianMusaeus\\\\New_EEG\\\\Processed\"\n",
    "\n",
    "# Saved ONE_MAIN_FOOOF cache\n",
    "SAVED_FOOOF_DIR = r\"G:\\\\ChristianMusaeus\\\\saved_fooof\"\n",
    "\n",
    "# Metadata\n",
    "METADATA_OLD_CSV = r\"G:\\\\ChristianMusaeus\\\\metadata_time_filtered.csv\"\n",
    "METADATA_NEW_CSV = r\"G:\\\\ChristianMusaeus\\\\EEG_sub_data_pseudoanonym.csv\"\n",
    "\n",
    "# Notebook-specific outputs (figures + cached intermediates)\n",
    "VIZ_OUT_DIR = OUTPUTS_ROOT / \"visualizations\" / \"clinical_classifier_results\"\n",
    "VIZ_CACHE_DIR = VIZ_OUT_DIR / \"cache\"\n",
    "VIZ_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIZ_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"Outputs root:\", OUTPUTS_ROOT)\n",
    "print(\"Viz outputs:\", VIZ_OUT_DIR)\n",
    "print(\"Platform:\", platform.system())\n",
    "if not _is_windows():\n",
    "    print(\"WARNING: This notebook is configured for Windows clinical paths.\")\n",
    "\n",
    "print(\"Old open dir:\", OLD_OPEN_DIR)\n",
    "print(\"Old closed dir:\", OLD_CLOSED_DIR)\n",
    "print(\"New processed dir:\", NEW_PROCESSED_DIR)\n",
    "print(\"Saved fooof dir:\", SAVED_FOOOF_DIR)\n",
    "print(\"Metadata old:\", METADATA_OLD_CSV)\n",
    "print(\"Metadata new:\", METADATA_NEW_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Helper Functions for Data Loading\n",
    "# -----------------\n",
    "\n",
    "def load_model_outputs(output_dir: Path, is_fooof: bool = None) -> Dict:\n",
    "    \"\"\"Load model outputs from a configuration directory.\"\"\"\n",
    "    if is_fooof is None:\n",
    "        is_fooof = \"__fooof__\" in output_dir.name\n",
    "    \n",
    "    prefix = \"fooof\" if is_fooof else \"psd\"\n",
    "    data = {}\n",
    "    \n",
    "    # Load prediction files\n",
    "    for key, filename in [\n",
    "        (\"epoch_idx\", f\"{prefix}_epoch_idx.npy\"),\n",
    "        (\"y_true\", f\"{prefix}_y_true.npy\"),\n",
    "        (\"prob_ec\", f\"{prefix}_prob_ec.npy\"),\n",
    "        (\"time_idx\", f\"{prefix}_time_idx.npy\"),\n",
    "    ]:\n",
    "        filepath = output_dir / filename\n",
    "        if filepath.exists():\n",
    "            data[key] = np.load(filepath)\n",
    "        else:\n",
    "            data[key] = None\n",
    "    \n",
    "    # Load summary files\n",
    "    csv_path = output_dir / \"logreg_cv_summary.csv\"\n",
    "    if csv_path.exists():\n",
    "        data[\"cv_summary\"] = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        data[\"cv_summary\"] = None\n",
    "    \n",
    "    csv_path = output_dir / \"logreg_per_subject_metrics.csv\"\n",
    "    if csv_path.exists():\n",
    "        data[\"per_subject_metrics\"] = normalize_per_subject_metrics(pd.read_csv(csv_path))\n",
    "    else:\n",
    "        data[\"per_subject_metrics\"] = None\n",
    "    \n",
    "    # Load subject splits\n",
    "    for key, filename in [\n",
    "        (\"test_subjects\", \"test_subjects.npy\"),\n",
    "        (\"cv_test_subjects\", \"cv_test_subjects.npy\"),\n",
    "        (\"val_accuracies\", \"val_accuracies.npy\"),\n",
    "        (\"val_subject_ids\", \"val_subject_ids.npy\"),\n",
    "    ]:\n",
    "        filepath = output_dir / filename\n",
    "        if filepath.exists():\n",
    "            data[key] = np.load(filepath)\n",
    "        else:\n",
    "            data[key] = None\n",
    "    \n",
    "    data[\"is_fooof\"] = is_fooof\n",
    "    data[\"output_dir\"] = output_dir\n",
    "    data[\"model_name\"] = output_dir.name\n",
    "    \n",
    "    return data\n",
    "\n",
    "def pretty_model_name(config_name: str) -> str:\n",
    "    \"\"\"Short, consistent display name for plots.\"\"\"\n",
    "    s = str(config_name)\n",
    "    if \"train_old_test_new\" in s:\n",
    "        ds = \"Train old → Test new\"\n",
    "    elif \"train_new_test_old\" in s:\n",
    "        ds = \"Train new → Test old\"\n",
    "    elif s.startswith(\"old_dataset\"):\n",
    "        ds = \"Old dataset\"\n",
    "    elif s.startswith(\"new_dataset\"):\n",
    "        ds = \"New dataset\"\n",
    "    elif s.startswith(\"combined_datasets\"):\n",
    "        ds = \"Combined datasets\"\n",
    "    else:\n",
    "        ds = s.split(\"__\")[0]\n",
    "\n",
    "    feat = \"FOOOF\" if \"__fooof__\" in s else \"PSD\"\n",
    "    if \"__fooof__\" in s and \"__one_main_fooof__\" in s:\n",
    "        feat = \"FOOOF (ONE_MAIN)\"\n",
    "\n",
    "    pen = None\n",
    "    if \"__tune_penalty\" in s:\n",
    "        pen = \"tuned\"\n",
    "    elif \"__pen_l1\" in s:\n",
    "        pen = \"L1\"\n",
    "    elif \"__pen_l2\" in s:\n",
    "        pen = \"L2\"\n",
    "\n",
    "    parts = [ds, feat]\n",
    "    if pen:\n",
    "        parts.append(f\"pen {pen}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "def model_short_key(config_name: str) -> str:\n",
    "    \"\"\"Very short model key for tight plots.\"\"\"\n",
    "    s = str(config_name)\n",
    "    if s.startswith(\"old_dataset\") and \"__fooof__\" in s:\n",
    "        return \"old_fooof\"\n",
    "    if s.startswith(\"old_dataset\"):\n",
    "        return \"old_psd\"\n",
    "    if s.startswith(\"new_dataset\") and \"__fooof__\" in s:\n",
    "        return \"new_fooof\"\n",
    "    if s.startswith(\"new_dataset\"):\n",
    "        return \"new_psd\"\n",
    "    return s.split(\"__\")[0]\n",
    "\n",
    "def normalize_per_subject_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize per-subject metrics to have `subject_id` column.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    if \"subject_id\" not in df.columns and \"subject\" in df.columns:\n",
    "        df = df.rename(columns={\"subject\": \"subject_id\"})\n",
    "    if \"subject_id\" in df.columns:\n",
    "        df[\"subject_id\"] = pd.to_numeric(df[\"subject_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def load_metadata_old() -> pd.DataFrame:\n",
    "    \"\"\"Load old dataset metadata from CSV (clinical path).\"\"\"\n",
    "    path = Path(str(METADATA_OLD_CSV))\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Old metadata not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Many clinical CSV exports are semicolon-separated; if we only got 1 column,\n",
    "    # try again with ';' (keeps explicit path requirement, just more robust parsing).\n",
    "    if df.shape[1] == 1:\n",
    "        try:\n",
    "            df2 = pd.read_csv(path, sep=';', engine='python')\n",
    "            if df2.shape[1] > 1:\n",
    "                df = df2\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Normalize columns\n",
    "    if \"subject_id\" not in df.columns and \"Subject_ID\" in df.columns:\n",
    "        df = df.rename(columns={\"Subject_ID\": \"subject_id\"})\n",
    "    if \"age\" not in df.columns and \"Age\" in df.columns:\n",
    "        df = df.rename(columns={\"Age\": \"age\"})\n",
    "    if \"sex\" in df.columns:\n",
    "        df[\"sex\"] = df[\"sex\"].astype(str).str.strip()\n",
    "    if \"subject_id\" in df.columns:\n",
    "        df[\"subject_id\"] = pd.to_numeric(df[\"subject_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"age\" in df.columns:\n",
    "        df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_metadata_new() -> pd.DataFrame:\n",
    "    \"\"\"Load new dataset metadata from CSV (clinical path).\n",
    "\n",
    "    Expected: subject ids may appear as strings like \"Sub001\" or \"Ros_Sub001\".\n",
    "    This normalizes to numeric Int64 (e.g. 1) to match .fif parsing.\n",
    "    \"\"\"\n",
    "    path = Path(str(METADATA_NEW_CSV))\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"New metadata not found: {path}\")\n",
    "\n",
    "    # Try comma first, then semicolon; keep the parse that yields more columns.\n",
    "    df_comma = pd.read_csv(path)\n",
    "    df = df_comma\n",
    "    try:\n",
    "        df_semi = pd.read_csv(path, sep=';', engine='python')\n",
    "        if df_semi.shape[1] > df_comma.shape[1]:\n",
    "            df = df_semi\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Normalize columns\n",
    "    if 'subject_id' not in df.columns:\n",
    "        # common first-column ID\n",
    "        first = df.columns[0]\n",
    "        df = df.rename(columns={first: 'subject_id'})\n",
    "\n",
    "    # If subject_id values look like \"Sub001\" / \"Ros_Sub001\", extract digits\n",
    "    if 'subject_id' in df.columns:\n",
    "        sid = df['subject_id'].astype(str).str.strip()\n",
    "        sid = sid.str.extract(r'(\\d+)', expand=False)\n",
    "        df['subject_id'] = pd.to_numeric(sid, errors='coerce').astype('Int64')\n",
    "\n",
    "    if 'age' not in df.columns:\n",
    "        if 'Age' in df.columns:\n",
    "            df = df.rename(columns={'Age': 'age'})\n",
    "        elif 'Y' in df.columns:\n",
    "            df = df.rename(columns={'Y': 'age'})\n",
    "\n",
    "    if 'sex' not in df.columns:\n",
    "        if 'Sex' in df.columns:\n",
    "            df = df.rename(columns={'Sex': 'sex'})\n",
    "        elif 'M/F' in df.columns:\n",
    "            df = df.rename(columns={'M/F': 'sex'})\n",
    "        elif 'F/M' in df.columns:\n",
    "            df = df.rename(columns={'F/M': 'sex'})\n",
    "\n",
    "    if 'sex' in df.columns:\n",
    "        df['sex'] = df['sex'].astype(str).str.strip()\n",
    "        df['sex'] = df['sex'].replace({'M': 'Male', 'F': 'Female'})\n",
    "\n",
    "    if 'age' in df.columns:\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load Metadata\n",
    "# -----------------\n",
    "\n",
    "metadata_old = load_metadata_old()\n",
    "metadata_new = load_metadata_new()\n",
    "\n",
    "print(f\"Old metadata: {len(metadata_old)} rows\")\n",
    "print(f\"New metadata: {len(metadata_new)} rows\")\n",
    "if len(metadata_old) > 0:\n",
    "    print(f\"Old metadata columns: {metadata_old.columns.tolist()}\")\n",
    "if len(metadata_new) > 0:\n",
    "    print(f\"New metadata columns: {metadata_new.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load the two *clinical* \"small\" datasets (same discovery logic as EC_EO_Classifier.ipynb)\n",
    "# -----------------\n",
    "\n",
    "def _collect_set_files(directory: Path) -> list[Path]:\n",
    "    if directory is None or not Path(directory).exists():\n",
    "        return []\n",
    "    directory = Path(directory)\n",
    "    files = list(directory.rglob('*.set')) + list(directory.rglob('*.SET'))\n",
    "    return sorted({f.resolve() for f in files})\n",
    "\n",
    "def _parse_subject_id_from_path(p: Path) -> int:\n",
    "    m = re.search(r\"(\\d{3,})\", p.stem)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    raise ValueError(f\"Could not parse subject id from: {p.name}\")\n",
    "\n",
    "def load_old_open_closed_pairs(open_dir: str, closed_dir: str, limit_subjects: int = 30):\n",
    "    open_files = _collect_set_files(Path(open_dir))\n",
    "    closed_files = _collect_set_files(Path(closed_dir))\n",
    "    open_by_sid = {}\n",
    "    closed_by_sid = {}\n",
    "    for f in open_files:\n",
    "        try:\n",
    "            open_by_sid[_parse_subject_id_from_path(f)] = f\n",
    "        except Exception:\n",
    "            continue\n",
    "    for f in closed_files:\n",
    "        try:\n",
    "            closed_by_sid[_parse_subject_id_from_path(f)] = f\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    common = sorted(set(open_by_sid).intersection(closed_by_sid))\n",
    "    if limit_subjects and len(common) > int(limit_subjects):\n",
    "        common = common[: int(limit_subjects)]\n",
    "\n",
    "    pairs = [(sid, open_by_sid[sid], closed_by_sid[sid]) for sid in common]\n",
    "    return pairs\n",
    "\n",
    "def load_new_subject_pairs(processed_dir: str):\n",
    "    processed = Path(processed_dir)\n",
    "    files = list(processed.glob('*_epo.fif')) + list(processed.glob('*_epo.FIF'))\n",
    "    pairs: dict[int, dict[str, Path]] = {}\n",
    "    for f in files:\n",
    "        m = re.search(r\"sub(\\d+)([ab])\", f.stem, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            continue\n",
    "        sid = int(m.group(1))\n",
    "        rater = m.group(2).lower()\n",
    "        pairs.setdefault(sid, {})[rater] = f.resolve()\n",
    "\n",
    "    out: list[tuple[int, Path, Path | None]] = []\n",
    "    paired_count = 0\n",
    "    single_count = 0\n",
    "    for sid in sorted(pairs):\n",
    "        entry = pairs[sid]\n",
    "        if 'a' in entry and 'b' in entry:\n",
    "            out.append((sid, entry['a'], entry['b']))\n",
    "            paired_count += 1\n",
    "        elif 'a' in entry:\n",
    "            out.append((sid, entry['a'], None))\n",
    "            single_count += 1\n",
    "        elif 'b' in entry:\n",
    "            out.append((sid, entry['b'], None))\n",
    "            single_count += 1\n",
    "\n",
    "    print(f\"NEW subjects found: total={len(out)} (paired={paired_count}, single={single_count})\")\n",
    "    if out:\n",
    "        print(\"Example NEW subject entry:\", out[0])\n",
    "    return out\n",
    "\n",
    "# Resolve clinical datasets\n",
    "old_pairs = load_old_open_closed_pairs(OLD_OPEN_DIR, OLD_CLOSED_DIR, limit_subjects=30)\n",
    "new_pairs = load_new_subject_pairs(NEW_PROCESSED_DIR)\n",
    "\n",
    "old_subject_ids_small = [sid for (sid, _, _) in old_pairs]\n",
    "new_subject_ids_small = [sid for (sid, _, _) in new_pairs]\n",
    "\n",
    "print(f\"OLD small set: subjects={len(old_subject_ids_small)} | files={len(old_pairs)*2}\")\n",
    "print(f\"NEW small set: subjects={len(new_subject_ids_small)} | fif entries={len(new_pairs)}\")\n",
    "\n",
    "# Build old set_files ordering exactly like EC_EO_Classifier: open files first, then closed files\n",
    "old_open_files_small = [p_open for (_, p_open, _) in old_pairs]\n",
    "old_closed_files_small = [p_closed for (_, _, p_closed) in old_pairs]\n",
    "old_set_files_ordered = old_open_files_small + old_closed_files_small\n",
    "\n",
    "# Filter metadata to the small sets\n",
    "metadata_old_small = metadata_old[metadata_old.get('subject_id').isin(old_subject_ids_small)].copy() if len(metadata_old) else metadata_old\n",
    "metadata_new_small = metadata_new[metadata_new.get('subject_id').isin(new_subject_ids_small)].copy() if len(metadata_new) else metadata_new\n",
    "\n",
    "print(f\"Filtered old metadata: {len(metadata_old_small)} / {len(metadata_old)}\")\n",
    "print(f\"Filtered new metadata: {len(metadata_new_small)} / {len(metadata_new)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Build (and cache) an epoch-index → (subject, file, epoch) map for the clinical small sets\n",
    "# This is required to make per-subject epoch plots, worst-subject plots, and disagreement-epoch PSD plots real.\n",
    "# -----------------\n",
    "\n",
    "USE_INDEX_MAP_CACHE = True\n",
    "\n",
    "STANDARD_OCCIPITAL = [\"O1\", \"O2\", \"P3\", \"P4\", \"P7\", \"P8\", \"Pz\"]\n",
    "STANDARD_OCCIPITAL_SET = {ch.upper() for ch in STANDARD_OCCIPITAL}\n",
    "\n",
    "def canonical_channel_name(ch_name: str) -> str:\n",
    "    name = str(ch_name).strip()\n",
    "    name = re.sub(r\"^EEG\\s+\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"-REF$\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \"\", name)\n",
    "    return name\n",
    "\n",
    "def rename_epochs_channels_canonical(epochs):\n",
    "    new_names = [canonical_channel_name(ch) for ch in epochs.ch_names]\n",
    "    if len(set(new_names)) != len(new_names):\n",
    "        return epochs\n",
    "    mapping = {old: new for old, new in zip(epochs.ch_names, new_names) if old != new}\n",
    "    if mapping:\n",
    "        epochs.rename_channels(mapping)\n",
    "    return epochs\n",
    "\n",
    "def labels_from_epochs_events(epochs) -> np.ndarray:\n",
    "    code_to_name = {int(v): str(k).upper() for k, v in epochs.event_id.items()}\n",
    "    labels = np.full(len(epochs), -1, dtype=int)\n",
    "    for i, code in enumerate(epochs.events[:, 2].astype(int)):\n",
    "        name = code_to_name.get(int(code), \"\")\n",
    "        if name.startswith(\"EO\"):\n",
    "            labels[i] = 0\n",
    "        elif name.startswith(\"EC\"):\n",
    "            labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "def load_rejmanual_vector(set_path: Path, n_epochs_expected: int) -> np.ndarray | None:\n",
    "    \"\"\"Return reject vector (1=reject, 0=keep) or None.\"\"\"\n",
    "    try:\n",
    "        mat = loadmat(str(set_path), struct_as_record=False, squeeze_me=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "    rej = None\n",
    "    block = mat.get(\"reject\", None)\n",
    "    if block is not None and hasattr(block, \"rejmanual\"):\n",
    "        rej = np.array(block.rejmanual)\n",
    "    if rej is None and \"EEG\" in mat:\n",
    "        EEG = mat[\"EEG\"]\n",
    "        reject_section = getattr(EEG, \"reject\", None)\n",
    "        if reject_section is not None:\n",
    "            if hasattr(reject_section, \"rejmanual\"):\n",
    "                rej = np.array(reject_section.rejmanual)\n",
    "            elif isinstance(reject_section, dict) and \"rejmanual\" in reject_section:\n",
    "                rej = np.array(reject_section[\"rejmanual\"])\n",
    "    if rej is None:\n",
    "        return None\n",
    "    rej = np.asarray(rej).ravel().astype(int)\n",
    "    if rej.size != int(n_epochs_expected):\n",
    "        warnings.warn(f\"{set_path.name}: rejmanual length {rej.size} != n_epochs {n_epochs_expected}\")\n",
    "        return None\n",
    "    return (rej != 0).astype(int)\n",
    "\n",
    "def build_index_map_old(set_files_ordered: list[Path]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    cursor = 0\n",
    "    for i, path in enumerate(set_files_ordered, start=1):\n",
    "        label = 0 if (\"open_marked\" in str(path).lower() or \"eyesopen\" in str(path).lower()) else 1\n",
    "        try:\n",
    "            epochs = mne.io.read_epochs_eeglab(str(path), verbose='ERROR')\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f\"Failed to read {path}: {exc}\")\n",
    "            continue\n",
    "        epochs = rename_epochs_channels_canonical(epochs)\n",
    "        n_epochs = len(epochs)\n",
    "        subj = _parse_subject_id_from_path(path)\n",
    "\n",
    "        rej = load_rejmanual_vector(path, n_epochs_expected=n_epochs)\n",
    "        if rej is None:\n",
    "            warnings.warn(f\"{path.name}: missing rejmanual; skipping\")\n",
    "            continue\n",
    "        keep_mask = (rej == 0)\n",
    "        if not keep_mask.any():\n",
    "            continue\n",
    "\n",
    "        data = epochs.get_data()\n",
    "        finite_mask = np.all(np.isfinite(data), axis=(1,2))\n",
    "        keep = keep_mask & finite_mask\n",
    "        kept_idx = np.flatnonzero(keep)\n",
    "        for epoch_orig_idx in kept_idx:\n",
    "            rows.append({\n",
    "                'global_idx': int(cursor),\n",
    "                'dataset': 'old',\n",
    "                'subject_id': int(subj),\n",
    "                'file': str(path),\n",
    "                'epoch_orig_idx': int(epoch_orig_idx),\n",
    "                'y_true': int(label),\n",
    "                'condition': 'EC' if int(label)==1 else 'EO',\n",
    "            })\n",
    "            cursor += 1\n",
    "\n",
    "        if (i % 10) == 0:\n",
    "            print(f\"  [old] {i}/{len(set_files_ordered)} files | samples so far: {cursor}\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_index_map_new(subject_pairs: list[tuple[int, Path, Path | None]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    cursor = 0\n",
    "    for i, (sid, file_a, file_b) in enumerate(subject_pairs, start=1):\n",
    "        path = Path(file_a)\n",
    "        try:\n",
    "            epochs = mne.read_epochs(str(path), preload=False, verbose='ERROR')\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f\"Failed to read NEW epochs {path}: {exc}\")\n",
    "            continue\n",
    "        epochs = rename_epochs_channels_canonical(epochs)\n",
    "        labels_a = labels_from_epochs_events(epochs)\n",
    "        labels_all = labels_a\n",
    "\n",
    "        path_b = Path(file_b) if file_b else None\n",
    "        if path_b is not None:\n",
    "            if not path_b.exists():\n",
    "                path_b = None\n",
    "            else:\n",
    "                try:\n",
    "                    epochs_b = mne.read_epochs(str(path_b), preload=False, verbose='ERROR')\n",
    "                    labels_b = labels_from_epochs_events(epochs_b)\n",
    "                    if labels_b.shape == labels_a.shape:\n",
    "                        union = labels_a.copy()\n",
    "                        take_from_b = (union < 0)\n",
    "                        union[take_from_b] = labels_b[take_from_b]\n",
    "                        conflict = (labels_a >= 0) & (labels_b >= 0) & (labels_a != labels_b)\n",
    "                        union[conflict] = -1\n",
    "                        labels_all = union\n",
    "                except Exception:\n",
    "                    path_b = None\n",
    "\n",
    "        keep_mask_labels = (labels_all >= 0)\n",
    "        if not keep_mask_labels.any():\n",
    "            continue\n",
    "\n",
    "        data = epochs.get_data()\n",
    "        finite_mask = np.all(np.isfinite(data), axis=(1,2))\n",
    "        keep = keep_mask_labels & finite_mask\n",
    "        kept_idx = np.flatnonzero(keep)\n",
    "\n",
    "        for epoch_orig_idx in kept_idx:\n",
    "            y = int(labels_all[epoch_orig_idx])\n",
    "            rows.append({\n",
    "                'global_idx': int(cursor),\n",
    "                'dataset': 'new',\n",
    "                'subject_id': int(sid),\n",
    "                'file': str(path),\n",
    "                'file_b': str(path_b) if path_b is not None else None,\n",
    "                'epoch_orig_idx': int(epoch_orig_idx),\n",
    "                'y_true': int(y),\n",
    "                'condition': 'EC' if int(y)==1 else 'EO',\n",
    "            })\n",
    "            cursor += 1\n",
    "\n",
    "        if (i % 25) == 0:\n",
    "            print(f\"  [new] {i}/{len(subject_pairs)} subjects | samples so far: {cursor}\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Cache + build\n",
    "idx_old_cache = VIZ_CACHE_DIR / 'index_map_old.pkl'\n",
    "idx_new_cache = VIZ_CACHE_DIR / 'index_map_new.pkl'\n",
    "\n",
    "if USE_INDEX_MAP_CACHE and idx_old_cache.exists():\n",
    "    index_map_old = pd.read_pickle(idx_old_cache)\n",
    "    print('Loaded index_map_old cache:', idx_old_cache)\n",
    "else:\n",
    "    print('Building index_map_old...')\n",
    "    index_map_old = build_index_map_old(old_set_files_ordered)\n",
    "    index_map_old.to_pickle(idx_old_cache)\n",
    "    print('Wrote:', idx_old_cache, '| rows:', len(index_map_old))\n",
    "\n",
    "if USE_INDEX_MAP_CACHE and idx_new_cache.exists():\n",
    "    index_map_new = pd.read_pickle(idx_new_cache)\n",
    "    print('Loaded index_map_new cache:', idx_new_cache)\n",
    "else:\n",
    "    print('Building index_map_new...')\n",
    "    index_map_new = build_index_map_new(new_pairs)\n",
    "    index_map_new.to_pickle(idx_new_cache)\n",
    "    print('Wrote:', idx_new_cache, '| rows:', len(index_map_new))\n",
    "\n",
    "print('index_map_old rows:', len(index_map_old))\n",
    "print('index_map_new rows:', len(index_map_new))\n",
    "\n",
    "def attach_subject_mapping(model_data: Dict, index_map: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Attach subject/file/epoch info to model outputs using global_idx.\"\"\"\n",
    "    epoch_idx = model_data.get('epoch_idx')\n",
    "    y_true = model_data.get('y_true')\n",
    "    prob_ec = model_data.get('prob_ec')\n",
    "    time_idx = model_data.get('time_idx')\n",
    "\n",
    "    if epoch_idx is None or y_true is None or prob_ec is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'global_idx': np.asarray(epoch_idx, dtype=int),\n",
    "        'y_true_model': np.asarray(y_true, dtype=int),\n",
    "        'prob_ec': np.asarray(prob_ec, dtype=float),\n",
    "    })\n",
    "    if time_idx is not None:\n",
    "        df['time_idx'] = np.asarray(time_idx, dtype=int)\n",
    "\n",
    "    df['y_pred'] = (df['prob_ec'] >= 0.5).astype(int)\n",
    "\n",
    "    meta_cols = [c for c in ['global_idx','subject_id','file','epoch_orig_idx','y_true','condition'] if c in index_map.columns]\n",
    "    merged = df.merge(index_map[meta_cols], on='global_idx', how='left')\n",
    "\n",
    "    missing = merged['subject_id'].isna().sum() if 'subject_id' in merged.columns else len(merged)\n",
    "    if missing:\n",
    "        print(f\"WARNING: {missing} predictions could not be mapped to subjects (check dataset ordering vs model output).\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load Model Outputs from 4 Configurations\n",
    "# -----------------\n",
    "\n",
    "MODEL_CONFIGS = [\n",
    "    \"old_dataset__fooof__allch__cv2__time_align_conditions__one_main_fooof__mainfooof_all_epochs__pen_l2\",\n",
    "    \"old_dataset__no_fooof__allch__cv2__time_align_conditions__pen_l2\",\n",
    "    \"new_dataset__fooof__allch__cv2__time_align_conditions__one_main_fooof__mainfooof_all_epochs__pen_l2\",\n",
    "    \"new_dataset__no_fooof__allch__cv2__time_align_conditions__pen_l2\",\n",
    "]\n",
    "\n",
    "model_outputs = {}\n",
    "\n",
    "for config_name in MODEL_CONFIGS:\n",
    "    config_dir = OUTPUTS_ROOT / config_name\n",
    "    if config_dir.exists():\n",
    "        print(f\"Loading {config_name}...\")\n",
    "        model_outputs[config_name] = load_model_outputs(config_dir)\n",
    "        print(f\"  ✓ Loaded\")\n",
    "    else:\n",
    "        print(f\"  ✗ Not found: {config_dir}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(model_outputs)} model configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load FOOOF Center Frequencies (subject-level) from saved_fooof\n",
    "# Prefer alpha_profiles.csv / feature_manifest.csv produced by Precompute_ONE_MAIN_FOOOF.\n",
    "# -----------------\n",
    "\n",
    "saved_root = Path(str(SAVED_FOOOF_DIR))\n",
    "\n",
    "fooof_profiles = []\n",
    "if saved_root.exists():\n",
    "    # Fast path: load alpha_profiles.csv files\n",
    "    for csv_path in saved_root.rglob('alpha_profiles.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df['source'] = str(csv_path)\n",
    "            df['config_dir'] = str(csv_path.parent.name)\n",
    "            fooof_profiles.append(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Fallback: feature_manifest.csv (has per-file alpha_cf/alpha_bw)\n",
    "    if not fooof_profiles:\n",
    "        for csv_path in saved_root.rglob('feature_manifest.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['source'] = str(csv_path)\n",
    "                df['config_dir'] = str(csv_path.parent.name)\n",
    "                fooof_profiles.append(df)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "if not fooof_profiles:\n",
    "    print(f\"No saved_fooof profile CSVs found under: {saved_root}\")\n",
    "    profiles_df = pd.DataFrame()\n",
    "else:\n",
    "    profiles_df = pd.concat(fooof_profiles, ignore_index=True)\n",
    "\n",
    "# Normalize columns\n",
    "if len(profiles_df):\n",
    "    if 'subject' in profiles_df.columns and 'subject_id' not in profiles_df.columns:\n",
    "        profiles_df = profiles_df.rename(columns={'subject': 'subject_id'})\n",
    "    if 'subject_id' in profiles_df.columns:\n",
    "        profiles_df['subject_id'] = pd.to_numeric(profiles_df['subject_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # alpha_cf may be stored as alpha_cf or cf depending on file\n",
    "    if 'alpha_cf' not in profiles_df.columns:\n",
    "        for cand in ['cf', 'center_frequency', 'alpha_center_frequency']:\n",
    "            if cand in profiles_df.columns:\n",
    "                profiles_df = profiles_df.rename(columns={cand: 'alpha_cf'})\n",
    "                break\n",
    "\n",
    "    if 'alpha_cf' in profiles_df.columns:\n",
    "        profiles_df['alpha_cf'] = pd.to_numeric(profiles_df['alpha_cf'], errors='coerce')\n",
    "\n",
    "# Split to old/new using subject_id membership (robust even if filenames don't encode dataset)\n",
    "fooof_cf_by_subject_old = pd.Series(dtype=float)\n",
    "fooof_cf_by_subject_new = pd.Series(dtype=float)\n",
    "\n",
    "if len(profiles_df) and 'alpha_cf' in profiles_df.columns and 'subject_id' in profiles_df.columns:\n",
    "    old_set = set(int(x) for x in old_subject_ids_small)\n",
    "    new_set = set(int(x) for x in new_subject_ids_small)\n",
    "    small_set = old_set.union(new_set)\n",
    "\n",
    "    df_small = profiles_df[profiles_df['subject_id'].isin(list(small_set))].copy()\n",
    "    cf_by_subject = df_small.groupby('subject_id')['alpha_cf'].mean()\n",
    "    fooof_cf_by_subject_old = cf_by_subject[cf_by_subject.index.isin(list(old_set))].dropna()\n",
    "    fooof_cf_by_subject_new = cf_by_subject[cf_by_subject.index.isin(list(new_set))].dropna()\n",
    "\n",
    "print(f\"Saved-FOOOF CF subjects (old): {len(fooof_cf_by_subject_old)}\")\n",
    "print(f\"Saved-FOOOF CF subjects (new): {len(fooof_cf_by_subject_new)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "### 3.1 Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Age Distribution Visualization (small clinical sets only)\n",
    "# -----------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# Old dataset (30 subjects)\n",
    "ax = axes[0]\n",
    "if len(metadata_old_small) > 0 and 'age' in metadata_old_small.columns:\n",
    "    ages = metadata_old_small['age'].dropna().astype(float)\n",
    "    ax.hist(ages, bins=20, color='steelblue', edgecolor='black', alpha=0.75)\n",
    "    ax.set_title(f\"Old dataset (small set) age distribution (N={len(ages)})\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No old metadata for small set', ha='center', va='center', transform=ax.transAxes)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Number of subjects')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# New dataset (100 subjects)\n",
    "ax = axes[1]\n",
    "if len(metadata_new_small) > 0 and 'age' in metadata_new_small.columns:\n",
    "    ages = metadata_new_small['age'].dropna().astype(float)\n",
    "    ax.hist(ages, bins=30, color='coral', edgecolor='black', alpha=0.75)\n",
    "    ax.set_title(f\"New dataset (small set) age distribution (N={len(ages)})\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No new metadata for small set', ha='center', va='center', transform=ax.transAxes)\n",
    "ax.set_xlabel('Age')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alpha Power Analysis\n",
    "\n",
    "Note: Alpha power computation requires loading raw epoch data. This section will compute PSD from epochs if data files are accessible, otherwise it will use available computed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Alpha Power Computation and Visualization (clinical raw files)\n",
    "# -----------------\n",
    "\n",
    "ALPHA_BAND = (8.0, 13.0)\n",
    "N_EPOCHS_PER_CLASS = 60\n",
    "MIN_EPOCHS_PER_CLASS = 6\n",
    "PSD_TARGET_SECS = 2.0\n",
    "\n",
    "# Consistent EC/EO colors across all panels\n",
    "COND_ORDER = ['EO', 'EC']\n",
    "COND_PALETTE = {'EO': 'tab:blue', 'EC': 'tab:red'}\n",
    "\n",
    "alpha_cache = VIZ_CACHE_DIR / 'alpha_power_small_sets.csv'\n",
    "\n",
    "def _psd_array_welch_clean(data: np.ndarray, sfreq: float, fmin: float, fmax: float, target_secs: float = 2.0):\n",
    "    n_epochs, _, n_times = data.shape\n",
    "    n_per_seg = max(8, min(n_times, int(round(target_secs * sfreq))))\n",
    "    n_overlap = n_per_seg // 2 if n_per_seg >= 16 else 0\n",
    "    psds, freqs = mne.time_frequency.psd_array_welch(\n",
    "        data,\n",
    "        sfreq=sfreq,\n",
    "        fmin=float(fmin),\n",
    "        fmax=float(fmax),\n",
    "        n_per_seg=n_per_seg,\n",
    "        n_overlap=n_overlap,\n",
    "        window='hann',\n",
    "        average='mean',\n",
    "        verbose=False,\n",
    "    )\n",
    "    return psds, freqs\n",
    "\n",
    "def _alpha_power_epochwise(epochs, epoch_indices: np.ndarray, picks: np.ndarray) -> float:\n",
    "    if epoch_indices.size == 0 or picks.size == 0:\n",
    "        return float('nan')\n",
    "    data = epochs.get_data()[epoch_indices][:, picks, :]\n",
    "    sfreq = float(epochs.info['sfreq'])\n",
    "    psds, freqs = _psd_array_welch_clean(data, sfreq=sfreq, fmin=ALPHA_BAND[0], fmax=ALPHA_BAND[1], target_secs=PSD_TARGET_SECS)\n",
    "    # psds: (n_epochs, n_channels, n_freqs)\n",
    "    mean_power = psds.mean(axis=-1)  # (n_epochs, n_channels)\n",
    "    return float(np.nanmean(mean_power)) * 1e12\n",
    "\n",
    "\n",
    "# Relative alpha power: (alpha band power) / (total 1-40 Hz power)\n",
    "REL_BAND = (1.0, 40.0)\n",
    "\n",
    "def _band_power_epochwise(epochs, epoch_indices: np.ndarray, picks: np.ndarray, band: tuple[float,float]) -> float:\n",
    "    if epoch_indices.size == 0 or picks.size == 0:\n",
    "        return float('nan')\n",
    "    data = epochs.get_data()[epoch_indices][:, picks, :]\n",
    "    sfreq = float(epochs.info['sfreq'])\n",
    "    psds, freqs = _psd_array_welch_clean(data, sfreq=sfreq, fmin=float(band[0]), fmax=float(band[1]), target_secs=PSD_TARGET_SECS)\n",
    "    # Integrate across freqs (approx band power), then mean over epochs and channels\n",
    "    pow_ep_ch = psds.sum(axis=-1)\n",
    "    return float(np.nanmean(pow_ep_ch))\n",
    "\n",
    "def _relative_alpha_epochwise(epochs, epoch_indices: np.ndarray, picks: np.ndarray) -> float:\n",
    "    alpha_pow = _band_power_epochwise(epochs, epoch_indices, picks, ALPHA_BAND)\n",
    "    total_pow = _band_power_epochwise(epochs, epoch_indices, picks, REL_BAND)\n",
    "    if not np.isfinite(alpha_pow) or not np.isfinite(total_pow) or total_pow == 0:\n",
    "        return float('nan')\n",
    "    return float(alpha_pow / total_pow)\n",
    "\n",
    "def _picks_all_eeg(epochs) -> np.ndarray:\n",
    "    picks = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude='bads')\n",
    "    if len(picks) == 0:\n",
    "        picks = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=[])\n",
    "    return np.asarray(picks, dtype=int)\n",
    "\n",
    "def _picks_occipital(epochs) -> np.ndarray:\n",
    "    names = [canonical_channel_name(ch).upper() for ch in epochs.ch_names]\n",
    "    occ = [i for i,n in enumerate(names) if str(n).upper() in STANDARD_OCCIPITAL_SET]\n",
    "    return np.asarray(occ, dtype=int)\n",
    "\n",
    "def _load_epochs_for_file(path_str: str):\n",
    "    path = Path(str(path_str))\n",
    "    if path.suffix.lower() == '.set':\n",
    "        epochs = mne.io.read_epochs_eeglab(str(path), verbose='ERROR')\n",
    "    else:\n",
    "        epochs = mne.read_epochs(str(path), preload=False, verbose='ERROR')\n",
    "    epochs = rename_epochs_channels_canonical(epochs)\n",
    "    return epochs\n",
    "\n",
    "\n",
    "# ---- Attach metadata (age/sex) without overwriting new-set ages ----\n",
    "\n",
    "def _build_metadata_small() -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for df in [metadata_old_small, metadata_new_small]:\n",
    "        try:\n",
    "            if df is not None and len(df):\n",
    "                cols = [c for c in ['subject_id','age','sex'] if c in df.columns]\n",
    "                if 'subject_id' in cols:\n",
    "                    parts.append(df[cols].drop_duplicates())\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=['subject_id','age','sex'])\n",
    "    meta = pd.concat(parts, ignore_index=True).drop_duplicates(subset=['subject_id'])\n",
    "    meta['subject_id'] = pd.to_numeric(meta['subject_id'], errors='coerce').astype('Int64')\n",
    "    if 'age' in meta.columns:\n",
    "        meta['age'] = pd.to_numeric(meta['age'], errors='coerce')\n",
    "    return meta\n",
    "\n",
    "def _attach_metadata_small(alpha_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if alpha_df is None or len(alpha_df) == 0:\n",
    "        return alpha_df\n",
    "    meta = _build_metadata_small()\n",
    "    if meta is None or len(meta) == 0:\n",
    "        return alpha_df\n",
    "\n",
    "    out = alpha_df.copy()\n",
    "    # Drop legacy merge artifacts if present\n",
    "    for c in ['age_meta','sex_meta','age_meta2','sex_meta2','age_meta_x','age_meta_y','sex_meta_x','sex_meta_y']:\n",
    "        if c in out.columns:\n",
    "            out = out.drop(columns=[c])\n",
    "\n",
    "    out['subject_id'] = pd.to_numeric(out['subject_id'], errors='coerce').astype('Int64')\n",
    "    out = out.merge(meta, on='subject_id', how='left', suffixes=('', '_meta'))\n",
    "\n",
    "    # Coalesce if we already had age/sex in the cache\n",
    "    for col in ['age','sex']:\n",
    "        alt = f'{col}_meta'\n",
    "        if col in out.columns and alt in out.columns:\n",
    "            out[col] = out[col].fillna(out[alt])\n",
    "            out = out.drop(columns=[alt])\n",
    "\n",
    "    return out\n",
    "\n",
    "if alpha_cache.exists():\n",
    "    alpha_df = pd.read_csv(alpha_cache)\n",
    "    alpha_df = _attach_metadata_small(alpha_df)\n",
    "    # If cache was created by an older version (new-set ages ended up in age_meta2), this fixes it.\n",
    "    if 'dataset' in alpha_df.columns and 'age' in alpha_df.columns:\n",
    "        if alpha_df.loc[alpha_df['dataset']=='new','age'].isna().all() and len(_build_metadata_small()):\n",
    "            print('WARNING: new dataset ages missing after attach; check METADATA_NEW_CSV subject_id normalization.')\n",
    "    # Rewrite cache so future runs have consistent columns\n",
    "    alpha_df.to_csv(alpha_cache, index=False)\n",
    "    print('Loaded alpha cache:', alpha_cache, '| rows:', len(alpha_df))\n",
    "else:\n",
    "    print('Computing alpha power for small sets... (this can take a while the first time)')\n",
    "    records = []\n",
    "\n",
    "    # Old dataset: use index_map_old (already filtered to keep/finite)\n",
    "    for sid in sorted(set(index_map_old['subject_id'].dropna().astype(int).tolist())):\n",
    "        for cond_label, cond_name in [(0,'EO'), (1,'EC')]:\n",
    "            rows = index_map_old[(index_map_old['subject_id']==sid) & (index_map_old['y_true']==cond_label)]\n",
    "            if rows.empty:\n",
    "                continue\n",
    "            # Each condition is a separate file in old dataset\n",
    "            file_path = rows['file'].iloc[0]\n",
    "            epoch_idxs = rows['epoch_orig_idx'].to_numpy(dtype=int)\n",
    "            epoch_idxs = np.sort(epoch_idxs)[:N_EPOCHS_PER_CLASS]\n",
    "            if epoch_idxs.size < MIN_EPOCHS_PER_CLASS:\n",
    "                continue\n",
    "            epochs = _load_epochs_for_file(file_path)\n",
    "            picks_all = _picks_all_eeg(epochs)\n",
    "            picks_occ = _picks_occipital(epochs)\n",
    "            val_all = _alpha_power_epochwise(epochs, epoch_idxs, picks_all)\n",
    "            val_occ = _alpha_power_epochwise(epochs, epoch_idxs, picks_occ) if picks_occ.size else float('nan')\n",
    "            rel_all = _relative_alpha_epochwise(epochs, epoch_idxs, picks_all)\n",
    "            rel_occ = _relative_alpha_epochwise(epochs, epoch_idxs, picks_occ) if picks_occ.size else float('nan')\n",
    "            records.append(dict(dataset='old', subject_id=int(sid), condition=str(cond_name), mean_alpha_all=val_all, mean_alpha_occ=val_occ, mean_rel_all=rel_all, mean_rel_occ=rel_occ))\n",
    "\n",
    "    # New dataset\n",
    "    for sid in sorted(set(index_map_new['subject_id'].dropna().astype(int).tolist())):\n",
    "        rows_sub = index_map_new[index_map_new['subject_id']==sid]\n",
    "        if rows_sub.empty:\n",
    "            continue\n",
    "        file_path = rows_sub['file'].iloc[0]\n",
    "        epochs = _load_epochs_for_file(file_path)\n",
    "        picks_all = _picks_all_eeg(epochs)\n",
    "        picks_occ = _picks_occipital(epochs)\n",
    "        for cond_label, cond_name in [(0,'EO'), (1,'EC')]:\n",
    "            rows = rows_sub[rows_sub['y_true']==cond_label]\n",
    "            epoch_idxs = np.sort(rows['epoch_orig_idx'].to_numpy(dtype=int))[:N_EPOCHS_PER_CLASS]\n",
    "            if epoch_idxs.size < MIN_EPOCHS_PER_CLASS:\n",
    "                continue\n",
    "            val_all = _alpha_power_epochwise(epochs, epoch_idxs, picks_all)\n",
    "            val_occ = _alpha_power_epochwise(epochs, epoch_idxs, picks_occ) if picks_occ.size else float('nan')\n",
    "            rel_all = _relative_alpha_epochwise(epochs, epoch_idxs, picks_all)\n",
    "            rel_occ = _relative_alpha_epochwise(epochs, epoch_idxs, picks_occ) if picks_occ.size else float('nan')\n",
    "            records.append(dict(dataset='new', subject_id=int(sid), condition=str(cond_name), mean_alpha_all=val_all, mean_alpha_occ=val_occ, mean_rel_all=rel_all, mean_rel_occ=rel_occ))\n",
    "\n",
    "    alpha_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    alpha_df = _attach_metadata_small(alpha_df)\n",
    "\n",
    "    alpha_df.to_csv(alpha_cache, index=False)\n",
    "    print('Wrote alpha cache:', alpha_cache)\n",
    "\n",
    "# Plot\n",
    "if len(alpha_df) == 0:\n",
    "    print('No alpha power rows computed.')\n",
    "else:\n",
    "    for metric, title in [(\"mean_alpha_all\", \"All channels\"), (\"mean_alpha_occ\", \"Occipital ROI\")]:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "        for ax, ds, meta, color in [\n",
    "            (axes[0], 'old', metadata_old_small, 'steelblue'),\n",
    "            (axes[1], 'new', metadata_new_small, 'coral'),\n",
    "        ]:\n",
    "            df_ds = alpha_df[(alpha_df['dataset']==ds) & alpha_df['age'].notna()].copy()\n",
    "            if df_ds.empty:\n",
    "                ax.text(0.5,0.5,f'No {ds} alpha+age data',ha='center',va='center',transform=ax.transAxes)\n",
    "                continue\n",
    "            sns.lineplot(x='age', y=metric, hue='condition', hue_order=COND_ORDER, palette=COND_PALETTE, data=df_ds, estimator='mean', errorbar=('ci',95), marker='o', linewidth=2, ax=ax)\n",
    "            ax.set_title(f\"{ds.upper()} dataset: {title}\")\n",
    "            ax.set_xlabel('Age')\n",
    "            ax.set_ylabel('Mean alpha power (µV²/Hz)')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(title='Condition')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# Relative alpha plots (same layout as absolute)\n",
    "for metric, title in [(\"mean_rel_all\", \"All channels\"), (\"mean_rel_occ\", \"Occipital ROI\")]:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "    for ax, ds in [(axes[0], 'old'), (axes[1], 'new')]:\n",
    "        df_ds = alpha_df[(alpha_df['dataset']==ds) & alpha_df.get('age').notna()].copy() if 'age' in alpha_df.columns else pd.DataFrame()\n",
    "        if df_ds.empty or metric not in df_ds.columns:\n",
    "            ax.text(0.5,0.5,f'No {ds} relative alpha+age data',ha='center',va='center',transform=ax.transAxes)\n",
    "            continue\n",
    "        sns.lineplot(x='age', y=metric, hue='condition', hue_order=COND_ORDER, palette=COND_PALETTE,\n",
    "                     data=df_ds, estimator='mean', errorbar=('ci',95), marker='o', linewidth=2, ax=ax)\n",
    "        ax.set_title(f\"{ds.upper()} dataset: {title} (relative)\")\n",
    "        ax.set_xlabel('Age')\n",
    "        ax.set_ylabel('Relative alpha power (8–13 / 1–40 Hz)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(title='Condition')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Labeled Epochs Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Labeled Epochs Visualization\n",
    "# -----------------\n",
    "\n",
    "def count_epochs_by_class(model_data: Dict) -> Dict:\n",
    "    \"\"\"Count epochs by class (EC=1, EO=0) for true labels and predictions.\"\"\"\n",
    "    counts = {\"true\": {\"EC\": 0, \"EO\": 0}, \"pred\": {\"EC\": 0, \"EO\": 0}}\n",
    "\n",
    "    if model_data.get(\"y_true\") is not None:\n",
    "        y_true = model_data[\"y_true\"]\n",
    "        counts[\"true\"][\"EC\"] = int(np.sum(y_true == 1))\n",
    "        counts[\"true\"][\"EO\"] = int(np.sum(y_true == 0))\n",
    "\n",
    "    if model_data.get(\"prob_ec\") is not None:\n",
    "        prob_ec = model_data[\"prob_ec\"]\n",
    "        y_pred = (prob_ec >= 0.5).astype(int)\n",
    "        counts[\"pred\"][\"EC\"] = int(np.sum(y_pred == 1))\n",
    "        counts[\"pred\"][\"EO\"] = int(np.sum(y_pred == 0))\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Collect counts for each model\n",
    "old_fooof_model = None\n",
    "old_psd_model = None\n",
    "new_fooof_model = None\n",
    "new_psd_model = None\n",
    "\n",
    "for name, data in model_outputs.items():\n",
    "    if \"old_dataset__fooof\" in name:\n",
    "        old_fooof_model = data\n",
    "    elif \"old_dataset__no_fooof\" in name:\n",
    "        old_psd_model = data\n",
    "    elif \"new_dataset__fooof\" in name:\n",
    "        new_fooof_model = data\n",
    "    elif \"new_dataset__no_fooof\" in name:\n",
    "        new_psd_model = data\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# True labels\n",
    "ax = axes[0]\n",
    "datasets = [\"Old\", \"New\"]\n",
    "\n",
    "ec_counts = []\n",
    "eo_counts = []\n",
    "if old_fooof_model and old_fooof_model.get(\"y_true\") is not None:\n",
    "    y_true_old = old_fooof_model[\"y_true\"]\n",
    "    ec_counts.append(int(np.sum(y_true_old == 1)))\n",
    "    eo_counts.append(int(np.sum(y_true_old == 0)))\n",
    "else:\n",
    "    ec_counts.append(0); eo_counts.append(0)\n",
    "\n",
    "if new_fooof_model and new_fooof_model.get(\"y_true\") is not None:\n",
    "    y_true_new = new_fooof_model[\"y_true\"]\n",
    "    ec_counts.append(int(np.sum(y_true_new == 1)))\n",
    "    eo_counts.append(int(np.sum(y_true_new == 0)))\n",
    "else:\n",
    "    ec_counts.append(0); eo_counts.append(0)\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, ec_counts, width, label=\"EC\", color=\"red\", alpha=0.7)\n",
    "ax.bar(x + width/2, eo_counts, width, label=\"EO\", color=\"blue\", alpha=0.7)\n",
    "ax.set_xlabel(\"Dataset\")\n",
    "ax.set_ylabel(\"Number of Epochs\")\n",
    "ax.set_title(\"True Labels: EC vs EO\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Predictions\n",
    "ax = axes[1]\n",
    "ec_counts_pred = []\n",
    "eo_counts_pred = []\n",
    "\n",
    "if old_fooof_model and old_fooof_model.get(\"prob_ec\") is not None:\n",
    "    prob_ec_old = old_fooof_model[\"prob_ec\"]\n",
    "    y_pred_old = (prob_ec_old >= 0.5).astype(int)\n",
    "    ec_counts_pred.append(int(np.sum(y_pred_old == 1)))\n",
    "    eo_counts_pred.append(int(np.sum(y_pred_old == 0)))\n",
    "else:\n",
    "    ec_counts_pred.append(0); eo_counts_pred.append(0)\n",
    "\n",
    "if new_fooof_model and new_fooof_model.get(\"prob_ec\") is not None:\n",
    "    prob_ec_new = new_fooof_model[\"prob_ec\"]\n",
    "    y_pred_new = (prob_ec_new >= 0.5).astype(int)\n",
    "    ec_counts_pred.append(int(np.sum(y_pred_new == 1)))\n",
    "    eo_counts_pred.append(int(np.sum(y_pred_new == 0)))\n",
    "else:\n",
    "    ec_counts_pred.append(0); eo_counts_pred.append(0)\n",
    "\n",
    "ax.bar(x - width/2, ec_counts_pred, width, label=\"EC\", color=\"red\", alpha=0.7)\n",
    "ax.bar(x + width/2, eo_counts_pred, width, label=\"EO\", color=\"blue\", alpha=0.7)\n",
    "ax.set_xlabel(\"Dataset\")\n",
    "ax.set_ylabel(\"Number of Epochs\")\n",
    "ax.set_title(\"Predictions: EC vs EO\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Force identical y-axis limits\n",
    "max_y = max(ec_counts + eo_counts + ec_counts_pred + eo_counts_pred)\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0, max_y * 1.05 if max_y else 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Per-Subject Labeled Epochs Visualization (real counts via index_map)\n",
    "# -----------------\n",
    "\n",
    "def _pick_model(model_outputs: Dict, prefix: str) -> Dict | None:\n",
    "    for name, data in model_outputs.items():\n",
    "        if prefix in name:\n",
    "            return data\n",
    "    return None\n",
    "\n",
    "old_model = _pick_model(model_outputs, 'old_dataset__fooof')\n",
    "new_model = _pick_model(model_outputs, 'new_dataset__fooof')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "for ax, model_data, idx_map, title in [\n",
    "    (axes[0], old_model, index_map_old, 'OLD (30 subjects)'),\n",
    "    (axes[1], new_model, index_map_new, 'NEW (100 subjects)'),\n",
    "]:\n",
    "    if model_data is None:\n",
    "        ax.text(0.5,0.5,'Missing model outputs',ha='center',va='center',transform=ax.transAxes)\n",
    "        continue\n",
    "    df = attach_subject_mapping(model_data, idx_map)\n",
    "    if df.empty or 'subject_id' not in df.columns:\n",
    "        ax.text(0.5,0.5,'Could not map predictions to subjects',ha='center',va='center',transform=ax.transAxes)\n",
    "        continue\n",
    "\n",
    "    # Count predicted labels per subject\n",
    "    counts = (\n",
    "        df.dropna(subset=['subject_id'])\n",
    "          .assign(subject_id=lambda d: d['subject_id'].astype(int))\n",
    "          .groupby(['subject_id','y_pred']).size().unstack(fill_value=0)\n",
    "          .rename(columns={0:'EO_pred',1:'EC_pred'})\n",
    "          .reset_index()\n",
    "          .sort_values('subject_id')\n",
    "    )\n",
    "\n",
    "    # Plot all subjects (can be wide for NEW)\n",
    "    max_subjects = 30 if title.startswith('OLD') else 50\n",
    "    counts_plot = counts.head(max_subjects)\n",
    "\n",
    "    x = np.arange(len(counts_plot))\n",
    "    width = 0.42\n",
    "    ax.bar(x - width/2, counts_plot.get('EC_pred',0), width, label='EC', color='red', alpha=0.7)\n",
    "    ax.bar(x + width/2, counts_plot.get('EO_pred',0), width, label='EO', color='blue', alpha=0.7)\n",
    "    ax.set_title(f\"{title}: predicted EC/EO epochs per subject (first {len(counts_plot)})\")\n",
    "    ax.set_xlabel('Subject')\n",
    "    ax.set_ylabel('Epoch count')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([str(int(s)) for s in counts_plot['subject_id'].tolist()], rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.legend()\n",
    "\n",
    "# Align y-limits\n",
    "ylim = max(ax.get_ylim()[1] for ax in axes)\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0, ylim)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FOOOF Visualization\n",
    "\n",
    "### 4.1 Center Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# FOOOF Center Frequency Distribution (subject-level)\n",
    "# -----------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# Old\n",
    "ax = axes[0]\n",
    "cf_old = fooof_cf_by_subject_old.values if hasattr(fooof_cf_by_subject_old, 'values') else np.array([])\n",
    "cf_old = cf_old[np.isfinite(cf_old)]\n",
    "if cf_old.size:\n",
    "    ax.hist(cf_old, bins=25, color='steelblue', edgecolor='black', alpha=0.75)\n",
    "    ax.set_title(f\"Old dataset CF (N={cf_old.size})\")\n",
    "    ax.set_xlabel('Center frequency (Hz)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5,0.5,'No CF data for old dataset',ha='center',va='center',transform=ax.transAxes)\n",
    "\n",
    "# New\n",
    "ax = axes[1]\n",
    "cf_new = fooof_cf_by_subject_new.values if hasattr(fooof_cf_by_subject_new, 'values') else np.array([])\n",
    "cf_new = cf_new[np.isfinite(cf_new)]\n",
    "if cf_new.size:\n",
    "    ax.hist(cf_new, bins=25, color='coral', edgecolor='black', alpha=0.75)\n",
    "    ax.set_title(f\"New dataset CF (N={cf_new.size})\")\n",
    "    ax.set_xlabel('Center frequency (Hz)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5,0.5,'No CF data for new dataset',ha='center',va='center',transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Center Frequency vs Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# FOOOF Center Frequency vs Age (subject-level)\n",
    "# -----------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# Old\n",
    "ax = axes[0]\n",
    "if len(fooof_cf_by_subject_old) and len(metadata_old_small):\n",
    "    df = pd.DataFrame({'subject_id': fooof_cf_by_subject_old.index.astype(int), 'alpha_cf': fooof_cf_by_subject_old.values})\n",
    "    df = df.merge(metadata_old_small[['subject_id','age']], on='subject_id', how='left')\n",
    "    df = df[df['age'].notna() & df['alpha_cf'].notna()]\n",
    "    if len(df):\n",
    "        sns.regplot(x='age', y='alpha_cf', data=df, ax=ax, scatter_kws={'alpha':0.5, 's':25}, line_kws={'color':'black'})\n",
    "        ax.set_title('Old dataset: CF vs Age')\n",
    "        ax.set_xlabel('Age')\n",
    "        ax.set_ylabel('Center frequency (Hz)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5,0.5,'No age+CF rows (old)',ha='center',va='center',transform=ax.transAxes)\n",
    "else:\n",
    "    ax.text(0.5,0.5,'Missing old CF/metadata',ha='center',va='center',transform=ax.transAxes)\n",
    "\n",
    "# New\n",
    "ax = axes[1]\n",
    "if len(fooof_cf_by_subject_new) and len(metadata_new_small):\n",
    "    df = pd.DataFrame({'subject_id': fooof_cf_by_subject_new.index.astype(int), 'alpha_cf': fooof_cf_by_subject_new.values})\n",
    "    df = df.merge(metadata_new_small[['subject_id','age']], on='subject_id', how='left')\n",
    "    df = df[df['age'].notna() & df['alpha_cf'].notna()]\n",
    "    if len(df):\n",
    "        sns.regplot(x='age', y='alpha_cf', data=df, ax=ax, scatter_kws={'alpha':0.5, 's':25}, line_kws={'color':'black'})\n",
    "        ax.set_title('New dataset: CF vs Age')\n",
    "        ax.set_xlabel('Age')\n",
    "        ax.set_ylabel('Center frequency (Hz)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5,0.5,'No age+CF rows (new)',ha='center',va='center',transform=ax.transAxes)\n",
    "else:\n",
    "    ax.text(0.5,0.5,'Missing new CF/metadata',ha='center',va='center',transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Analysis\n",
    "\n",
    "For each model configuration, we'll analyze:\n",
    "- Aggregated test results (accuracy, classification report)\n",
    "- ROC curve (with AUC)\n",
    "- Confusion matrix\n",
    "- Accuracy histograms (per training and test subject)\n",
    "- Accuracy before/after time adjustment smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Model Performance Analysis Helper Functions\n",
    "# -----------------\n",
    "\n",
    "def analyze_model_performance(model_data: Dict, model_name: str):\n",
    "    \"\"\"Analyze and print model performance metrics.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {pretty_model_name(model_name)}\")\n",
    "    print(f\"  folder: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if model_data.get(\"y_true\") is None or model_data.get(\"prob_ec\") is None:\n",
    "        print(\"Missing prediction data (y_true or prob_ec)\")\n",
    "        return None\n",
    "    \n",
    "    y_true = model_data[\"y_true\"]\n",
    "    prob_ec = model_data[\"prob_ec\"]\n",
    "    y_pred = (prob_ec >= 0.5).astype(int)\n",
    "    \n",
    "    # Aggregated test results\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nAggregated Test Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"EO\", \"EC\"], digits=4))\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, prob_ec)\n",
    "        print(f\"\\nROC AUC: {auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nROC AUC: Error computing - {e}\")\n",
    "        auc = None\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Time adjustment smoothing (if available in cv_summary)\n",
    "    if model_data.get(\"cv_summary\") is not None:\n",
    "        df_summary = model_data[\"cv_summary\"]\n",
    "        if \"accuracy_raw\" in df_summary.columns and \"accuracy_smoothed\" in df_summary.columns:\n",
    "            acc_raw = df_summary[\"accuracy_raw\"].mean()\n",
    "            acc_smooth = df_summary[\"accuracy_smoothed\"].mean()\n",
    "            delta = acc_smooth - acc_raw\n",
    "            print(f\"\\nTime Adjustment Smoothing:\")\n",
    "            print(f\"  Accuracy (raw): {acc_raw:.4f}\")\n",
    "            print(f\"  Accuracy (smoothed): {acc_smooth:.4f}\")\n",
    "            print(f\"  Delta: {delta:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"cm\": cm,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"prob_ec\": prob_ec,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Analyze Each Model (and print chosen hyperparameters)\n",
    "# -----------------\n",
    "\n",
    "model_performances = {}\n",
    "\n",
    "def _print_hyperparams(model_name: str, model_data: Dict):\n",
    "    cv = model_data.get('cv_summary')\n",
    "    if cv is None or len(cv) == 0:\n",
    "        print(f\"{model_name}: (no cv_summary)\")\n",
    "        return\n",
    "\n",
    "    cols = [c for c in ['selected_C','selected_n_bins'] if c in cv.columns]\n",
    "    if not cols:\n",
    "        print(f\"{model_name}: (cv_summary missing selected hyperparam columns)\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Hyperparameters for: {model_short_key(model_name)}\")\n",
    "    for c in cols:\n",
    "        vals = cv[c].dropna().astype(str)\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "        mode = vals.value_counts().index[0]\n",
    "        uniq = sorted(vals.unique().tolist())\n",
    "        print(f\"  {c}: mode={mode} | unique={uniq}\")\n",
    "\n",
    "for model_name, model_data in model_outputs.items():\n",
    "    _print_hyperparams(model_name, model_data)\n",
    "    perf = analyze_model_performance(model_data, model_name)\n",
    "    if perf is not None:\n",
    "        model_performances[model_name] = perf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# ROC Curves for All Models\n",
    "# -----------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = {\"old\": \"steelblue\", \"new\": \"coral\"}\n",
    "styles = {\"fooof\": \"-\", \"no_fooof\": \"--\"}\n",
    "\n",
    "for idx, (model_name, perf) in enumerate(model_performances.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    y_true = perf[\"y_true\"]\n",
    "    prob_ec = perf[\"prob_ec\"]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, prob_ec)\n",
    "    auc_score = perf.get(\"auc\")\n",
    "    \n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f\"ROC (AUC = {auc_score:.3f})\" if auc_score else \"ROC\")\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label=\"Random\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(pretty_model_name(model_name))\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Confusion Matrices for All Models\n",
    "# -----------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, perf) in enumerate(model_performances.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    cm = perf[\"cm\"]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "                xticklabels=[\"EO\", \"EC\"], yticklabels=[\"EO\", \"EC\"])\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(pretty_model_name(model_name))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Accuracy Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Accuracy per subject (bar charts: one bar per subject)\n",
    "# -----------------\n",
    "\n",
    "def _bar_by_subject(ax, df: pd.DataFrame, title: str, color: str = 'steelblue'):\n",
    "    if df is None or len(df) == 0:\n",
    "        ax.text(0.5,0.5,'No data',ha='center',va='center',transform=ax.transAxes)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "\n",
    "    df = df.dropna(subset=['subject_id','accuracy']).copy()\n",
    "    df['subject_id'] = df['subject_id'].astype(int)\n",
    "    df = df.groupby('subject_id', as_index=False)['accuracy'].mean().sort_values('subject_id')\n",
    "\n",
    "    x = np.arange(len(df))\n",
    "    ax.bar(x, df['accuracy'].to_numpy(dtype=float), color=color, alpha=0.85)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('subject_id')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    ax.set_ylim(0, 1.02)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Avoid unreadable x labels for many subjects\n",
    "    step = max(1, int(np.ceil(len(df) / 25)))\n",
    "    ticks = x[::step]\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels([str(int(s)) for s in df['subject_id'].to_numpy()[::step]], rotation=45, ha='right')\n",
    "\n",
    "# Training/validation subjects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (model_name, model_data) in zip(axes, model_outputs.items()):\n",
    "    val_ids = model_data.get('val_subject_ids')\n",
    "    val_acc = model_data.get('val_accuracies')\n",
    "    if val_ids is None or val_acc is None:\n",
    "        ax.text(0.5,0.5,'Missing val_subject_ids/val_accuracies',ha='center',va='center',transform=ax.transAxes)\n",
    "        ax.set_title(model_short_key(model_name))\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame({'subject_id': np.asarray(val_ids, dtype=int), 'accuracy': np.asarray(val_acc, dtype=float)})\n",
    "    _bar_by_subject(ax, df, f\"{model_short_key(model_name)} (train/val)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test subjects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (model_name, model_data) in zip(axes, model_outputs.items()):\n",
    "    dfm = model_data.get('per_subject_metrics')\n",
    "    if dfm is None or len(dfm) == 0:\n",
    "        ax.text(0.5,0.5,'Missing per-subject metrics',ha='center',va='center',transform=ax.transAxes)\n",
    "        ax.set_title(model_short_key(model_name))\n",
    "        continue\n",
    "\n",
    "    dfm = normalize_per_subject_metrics(dfm)\n",
    "    if 'subject_id' not in dfm.columns or 'accuracy' not in dfm.columns:\n",
    "        ax.text(0.5,0.5,'Bad per-subject metrics schema',ha='center',va='center',transform=ax.transAxes)\n",
    "        ax.set_title(model_short_key(model_name))\n",
    "        continue\n",
    "\n",
    "    _bar_by_subject(ax, dfm[['subject_id','accuracy']], f\"{model_short_key(model_name)} (test)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Accuracy Before/After Time Adjustment Smoothing\n",
    "# -----------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "ax_idx = 0\n",
    "for model_name, model_data in model_outputs.items():\n",
    "    if model_data.get(\"cv_summary\") is not None:\n",
    "        df = model_data[\"cv_summary\"]\n",
    "        if \"accuracy_raw\" in df.columns and \"accuracy_smoothed\" in df.columns:\n",
    "            ax = axes[ax_idx]\n",
    "            \n",
    "            acc_raw = df[\"accuracy_raw\"].values\n",
    "            acc_smooth = df[\"accuracy_smoothed\"].values\n",
    "            delta = acc_smooth - acc_raw\n",
    "            \n",
    "            x = np.arange(len(acc_raw))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, acc_raw, width, label=\"Raw\", color=\"steelblue\", alpha=0.7)\n",
    "            ax.bar(x + width/2, acc_smooth, width, label=\"Smoothed\", color=\"coral\", alpha=0.7)\n",
    "            ax.set_xlabel(\"Fold/Subject\")\n",
    "            ax.set_ylabel(\"Accuracy\")\n",
    "            ax.set_title(f\"{pretty_model_name(model_name)}\\nRaw vs Smoothed Accuracy\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "            \n",
    "            # Delta plot\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(x, delta, \"g-o\", markersize=4, label=\"Delta\", alpha=0.6)\n",
    "            ax2.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "            ax2.set_ylabel(\"Delta Accuracy\", color=\"green\")\n",
    "            ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
    "            \n",
    "            ax_idx += 1\n",
    "            if ax_idx >= 4:\n",
    "                break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Cross-Model Comparison (Box Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Box Plot: Accuracies per Subject for Each Model (short labels + outliers)\n",
    "# -----------------\n",
    "\n",
    "per_model = []\n",
    "labels = []\n",
    "per_model_df = {}\n",
    "\n",
    "for model_name, model_data in model_outputs.items():\n",
    "    dfm = model_data.get('per_subject_metrics')\n",
    "    if dfm is None or len(dfm) == 0:\n",
    "        continue\n",
    "    dfm = normalize_per_subject_metrics(dfm)\n",
    "    if 'subject_id' not in dfm.columns or 'accuracy' not in dfm.columns:\n",
    "        continue\n",
    "    dfm = dfm.dropna(subset=['subject_id']).copy()\n",
    "    dfm['subject_id'] = dfm['subject_id'].astype(int)\n",
    "\n",
    "    key = model_short_key(model_name)\n",
    "    per_model.append(dfm['accuracy'].to_numpy(dtype=float))\n",
    "    labels.append(key)\n",
    "    per_model_df[key] = dfm[['subject_id','accuracy']].copy()\n",
    "\n",
    "if not per_model:\n",
    "    print('No per-subject metrics available for boxplot.')\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    bp = ax.boxplot(per_model, labels=labels, patch_artist=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "        patch.set_alpha(0.75)\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy per subject (test)')\n",
    "    ax.set_ylim(0, 1.02)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print outliers (1.5*IQR rule)\n",
    "    print('\\nOutliers per model (1.5*IQR):')\n",
    "    for key, dfm in per_model_df.items():\n",
    "        acc = dfm['accuracy'].to_numpy(dtype=float)\n",
    "        q1 = np.nanpercentile(acc, 25)\n",
    "        q3 = np.nanpercentile(acc, 75)\n",
    "        iqr = q3 - q1\n",
    "        lo = q1 - 1.5 * iqr\n",
    "        hi = q3 + 1.5 * iqr\n",
    "        out = dfm[(dfm['accuracy'] < lo) | (dfm['accuracy'] > hi)].sort_values('accuracy')\n",
    "        if out.empty:\n",
    "            print(f\"  {key}: (none)\")\n",
    "        else:\n",
    "            print(f\"  {key}: {len(out)} outlier subject(s)\")\n",
    "            print(out.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Challenging Subjects Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Top 5 Worst-Performing Subjects per Model\n",
    "# -----------------\n",
    "\n",
    "def get_worst_subjects(model_data: Dict, metadata_df: pd.DataFrame, n=5):\n",
    "    df = model_data.get('per_subject_metrics')\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    df = normalize_per_subject_metrics(df)\n",
    "    if 'subject_id' not in df.columns or 'accuracy' not in df.columns:\n",
    "        return None\n",
    "\n",
    "    out = df.dropna(subset=['subject_id']).copy()\n",
    "    out['subject_id'] = out['subject_id'].astype(int)\n",
    "    out = out.nsmallest(int(n), 'accuracy')[['subject_id','accuracy']]\n",
    "\n",
    "    if len(metadata_df) and 'subject_id' in metadata_df.columns and 'age' in metadata_df.columns:\n",
    "        out = out.merge(metadata_df[['subject_id','age']], on='subject_id', how='left')\n",
    "\n",
    "    return out\n",
    "\n",
    "for model_name, model_data in model_outputs.items():\n",
    "    key = model_short_key(model_name)\n",
    "    meta = metadata_old_small if key.startswith('old_') else metadata_new_small\n",
    "\n",
    "    worst = get_worst_subjects(model_data, meta, n=5)\n",
    "    print('\\n' + '='*60)\n",
    "    print(f\"Top 5 worst subjects: {key}\")\n",
    "    print('='*60)\n",
    "    # Add CF for FOOOF models (subject-level alpha center frequency)\n",
    "    if model_data.get('is_fooof'):\n",
    "        try:\n",
    "            cf_series = fooof_cf_by_subject_old if key.startswith('old_') else fooof_cf_by_subject_new\n",
    "            if cf_series is not None and len(cf_series):\n",
    "                cf_map = {int(k): float(v) for k,v in zip(cf_series.index.astype(int), cf_series.values) if pd.notna(v)}\n",
    "                worst['alpha_cf'] = worst['subject_id'].astype(int).map(cf_map)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if worst is None or len(worst) == 0:\n",
    "        print('No per-subject metrics available')\n",
    "    else:\n",
    "        print(worst.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PSD vs FOOOF Comparison\n",
    "\n",
    "### 7.1 Confidence Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# PSD vs FOOOF Confidence Comparison (Old Dataset)\n",
    "# -----------------\n",
    "\n",
    "def plot_psd_vs_fooof(fooof_data: Dict, psd_data: Dict, dataset_name: str):\n",
    "    \"\"\"Create scatterplot with marginal histograms comparing FOOOF vs PSD predictions.\"\"\"\n",
    "    # Load and align predictions\n",
    "    fooof_idx = fooof_data.get(\"epoch_idx\")\n",
    "    fooof_y = fooof_data.get(\"y_true\")\n",
    "    fooof_p = fooof_data.get(\"prob_ec\")\n",
    "    \n",
    "    psd_idx = psd_data.get(\"epoch_idx\")\n",
    "    psd_y = psd_data.get(\"y_true\")\n",
    "    psd_p = psd_data.get(\"prob_ec\")\n",
    "    \n",
    "    if fooof_idx is None or fooof_y is None or fooof_p is None:\n",
    "        print(f\"Skipping {dataset_name} - missing FOOOF data\")\n",
    "        return\n",
    "    if psd_idx is None or psd_y is None or psd_p is None:\n",
    "        print(f\"Skipping {dataset_name} - missing PSD data\")\n",
    "        return\n",
    "    \n",
    "    # Sanity checks: same epochs and labels\n",
    "    if not np.array_equal(fooof_idx, psd_idx):\n",
    "        print(f\"Warning: {dataset_name} - epoch indices don't match, aligning...\")\n",
    "        # Align by finding common indices\n",
    "        common_idx = np.intersect1d(fooof_idx, psd_idx)\n",
    "        fooof_mask = np.isin(fooof_idx, common_idx)\n",
    "        psd_mask = np.isin(psd_idx, common_idx)\n",
    "        \n",
    "        fooof_y = fooof_y[fooof_mask]\n",
    "        fooof_p = fooof_p[fooof_mask]\n",
    "        psd_y = psd_y[psd_mask]\n",
    "        psd_p = psd_p[psd_mask]\n",
    "        \n",
    "        if not np.array_equal(fooof_y, psd_y):\n",
    "            print(f\"Warning: {dataset_name} - labels don't match after alignment\")\n",
    "    \n",
    "    y_true = fooof_y\n",
    "    x = fooof_p  # FOOOF model P(EC)\n",
    "    y = psd_p    # PSD model P(EC)\n",
    "    \n",
    "    # Masks and colors: EO=0 (blue), EC=1 (red)\n",
    "    eo_mask = (y_true == 0)\n",
    "    ec_mask = (y_true == 1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4, 1], height_ratios=[1, 4],\n",
    "                           wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    ax_scatter = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax_scatter)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax_scatter)\n",
    "    \n",
    "    # Scatter\n",
    "    ax_scatter.scatter(x[eo_mask], y[eo_mask], c=\"blue\", alpha=0.5, label=\"EO\", s=20)\n",
    "    ax_scatter.scatter(x[ec_mask], y[ec_mask], c=\"red\", alpha=0.5, label=\"EC\", s=20)\n",
    "    ax_scatter.set_xlabel(\"P(EC) – FOOOF features\")\n",
    "    ax_scatter.set_ylabel(\"P(EC) – PSD features\")\n",
    "    ax_scatter.legend(loc=\"lower right\")\n",
    "    ax_scatter.grid(alpha=0.2)\n",
    "    ax_scatter.set_title(f\"{dataset_name}: FOOOF vs PSD Predictions\")\n",
    "    \n",
    "    # Marginal histograms (x-axis)\n",
    "    ax_histx.hist(x[eo_mask], bins=30, color=\"blue\", alpha=0.4, density=True)\n",
    "    ax_histx.hist(x[ec_mask], bins=30, color=\"red\", alpha=0.4, density=True)\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histx.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Marginal histograms (y-axis)\n",
    "    ax_histy.hist(y[eo_mask], bins=30, orientation=\"horizontal\", color=\"blue\", alpha=0.4, density=True)\n",
    "    ax_histy.hist(y[ec_mask], bins=30, orientation=\"horizontal\", color=\"red\", alpha=0.4, density=True)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "    ax_histy.set_xlabel(\"Density\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Old dataset comparison\n",
    "if old_fooof_model and old_psd_model:\n",
    "    plot_psd_vs_fooof(old_fooof_model, old_psd_model, \"Old Dataset\")\n",
    "\n",
    "# New dataset comparison\n",
    "if new_fooof_model and new_psd_model:\n",
    "    plot_psd_vs_fooof(new_fooof_model, new_psd_model, \"New Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Disagreement Analysis: Example Epoch where Models Disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Find and Plot Epoch where FOOOF is confident but PSD predicts opposite (with PSD visualization)\n",
    "# -----------------\n",
    "\n",
    "def _get_model_by_prefix(prefix: str) -> Dict | None:\n",
    "    for name, data in model_outputs.items():\n",
    "        if prefix in name:\n",
    "            return data\n",
    "    return None\n",
    "\n",
    "def _find_confident_disagreement(fooof_data: Dict, psd_data: Dict, idx_map: pd.DataFrame):\n",
    "    df_f = attach_subject_mapping(fooof_data, idx_map)\n",
    "    if df_f.empty:\n",
    "        print('No mapped FOOOF predictions (attach_subject_mapping returned empty).')\n",
    "        return None\n",
    "\n",
    "    df_p = pd.DataFrame({\n",
    "        'global_idx': np.asarray(psd_data.get('epoch_idx'), dtype=int),\n",
    "        'prob_ec_psd': np.asarray(psd_data.get('prob_ec'), dtype=float),\n",
    "    })\n",
    "\n",
    "    merged = df_f.merge(df_p, on='global_idx', how='inner')\n",
    "    if merged.empty:\n",
    "        print('No overlap between FOOOF and PSD epoch_idx arrays after mapping.')\n",
    "        return None\n",
    "\n",
    "    merged['y_pred_psd'] = (merged['prob_ec_psd'] >= 0.5).astype(int)\n",
    "    merged['confident_fooof'] = (merged['prob_ec'] > 0.90) | (merged['prob_ec'] < 0.10)\n",
    "    merged['disagree'] = (merged['y_pred'] != merged['y_pred_psd']) & merged['confident_fooof']\n",
    "\n",
    "    cand = merged[merged['disagree']]\n",
    "    print(f\"Disagreement candidates: {len(cand)} / merged={len(merged)}\")\n",
    "    if cand.empty:\n",
    "        return None\n",
    "    return cand.iloc[0].to_dict()\n",
    "\n",
    "def _plot_epoch_psd(path: str, epoch_orig_idx: int, title: str):\n",
    "    epochs = mne.io.read_epochs_eeglab(path, verbose='ERROR') if str(path).lower().endswith('.set') else mne.read_epochs(path, verbose='ERROR')\n",
    "    epochs = rename_epochs_channels_canonical(epochs)\n",
    "    epoch_orig_idx = int(epoch_orig_idx)\n",
    "    data = epochs.get_data()[epoch_orig_idx:epoch_orig_idx+1]\n",
    "    sfreq = float(epochs.info['sfreq'])\n",
    "\n",
    "    picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude='bads')\n",
    "    if len(picks_all) == 0:\n",
    "        picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=[])\n",
    "\n",
    "    n_per_seg = int(min(data.shape[-1], max(8, int(round(2.0*sfreq)))))\n",
    "    n_overlap = int(round(1.0*sfreq))\n",
    "    if n_per_seg <= 1:\n",
    "        n_overlap = 0\n",
    "    else:\n",
    "        n_overlap = min(n_overlap, n_per_seg - 1)\n",
    "\n",
    "    psd, freqs = mne.time_frequency.psd_array_welch(\n",
    "        data[:, picks_all, :], sfreq=sfreq, fmin=1.0, fmax=45.0,\n",
    "        n_per_seg=n_per_seg, n_overlap=n_overlap,\n",
    "        average='mean', window='hann', verbose=False,\n",
    "    )\n",
    "    mean_psd = psd.mean(axis=(0,1))\n",
    "\n",
    "    names = [canonical_channel_name(ch).upper() for ch in epochs.ch_names]\n",
    "    occ = [i for i,n in enumerate(names) if str(n).upper() in STANDARD_OCCIPITAL_SET]\n",
    "    occ_psd = None\n",
    "    if occ:\n",
    "        psd_occ, _ = mne.time_frequency.psd_array_welch(\n",
    "            data[:, occ, :], sfreq=sfreq, fmin=1.0, fmax=45.0,\n",
    "            n_per_seg=n_per_seg, n_overlap=n_overlap,\n",
    "            average='mean', window='hann', verbose=False,\n",
    "        )\n",
    "        occ_psd = psd_occ.mean(axis=(0,1))\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(freqs, mean_psd, label='All channels (avg)')\n",
    "    if occ_psd is not None:\n",
    "        plt.plot(freqs, occ_psd, label='Occipital ROI (avg)')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('PSD')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Old dataset\n",
    "old_fooof = _get_model_by_prefix('old_dataset__fooof')\n",
    "old_psd = _get_model_by_prefix('old_dataset__no_fooof')\n",
    "if old_fooof and old_psd:\n",
    "    row = _find_confident_disagreement(old_fooof, old_psd, index_map_old)\n",
    "    if row:\n",
    "        print('OLD disagreement:', {k: row.get(k) for k in ['subject_id','epoch_orig_idx','file','y_true_model','prob_ec','prob_ec_psd']})\n",
    "        _plot_epoch_psd(row['file'], row['epoch_orig_idx'], f\"OLD disagreement | subj {int(row['subject_id'])} | epoch {int(row['epoch_orig_idx'])}\")\n",
    "    else:\n",
    "        print('No confident disagreement found for OLD dataset')\n",
    "\n",
    "# New dataset\n",
    "new_fooof = _get_model_by_prefix('new_dataset__fooof')\n",
    "new_psd = _get_model_by_prefix('new_dataset__no_fooof')\n",
    "if new_fooof and new_psd:\n",
    "    row = _find_confident_disagreement(new_fooof, new_psd, index_map_new)\n",
    "    if row:\n",
    "        print('NEW disagreement:', {k: row.get(k) for k in ['subject_id','epoch_orig_idx','file','y_true_model','prob_ec','prob_ec_psd']})\n",
    "        _plot_epoch_psd(row['file'], row['epoch_orig_idx'], f\"NEW disagreement | subj {int(row['subject_id'])} | epoch {int(row['epoch_orig_idx'])}\")\n",
    "    else:\n",
    "        print('No confident disagreement found for NEW dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confident Model Performance\n",
    "\n",
    "Analyzing performance on predictions where models are confident (>0.90 for EC, <0.10 for EO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Confident Model Performance Analysis\n",
    "# -----------------\n",
    "\n",
    "def analyze_confident_predictions(model_data: Dict, model_name: str):\n",
    "    \"\"\"Analyze performance on confident predictions only.\"\"\"\n",
    "    if model_data.get(\"y_true\") is None or model_data.get(\"prob_ec\") is None:\n",
    "        return None\n",
    "    \n",
    "    y_true = model_data[\"y_true\"]\n",
    "    prob_ec = model_data[\"prob_ec\"]\n",
    "    \n",
    "    # Filter confident predictions: EC > 0.90 or EO < 0.10\n",
    "    confident_mask = (prob_ec > 0.90) | (prob_ec < 0.10)\n",
    "    \n",
    "    if np.sum(confident_mask) == 0:\n",
    "        print(f\"\\n{model_name}: No confident predictions found\")\n",
    "        return None\n",
    "    \n",
    "    y_true_conf = y_true[confident_mask]\n",
    "    prob_ec_conf = prob_ec[confident_mask]\n",
    "    y_pred_conf = (prob_ec_conf >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Confident Predictions: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total confident predictions: {np.sum(confident_mask)} / {len(y_true)} ({100*np.sum(confident_mask)/len(y_true):.1f}%)\")\n",
    "    \n",
    "    # Aggregated results\n",
    "    accuracy = accuracy_score(y_true_conf, y_pred_conf)\n",
    "    print(f\"\\nAccuracy on confident predictions: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf, target_names=[\"EO\", \"EC\"], digits=4))\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_conf, prob_ec_conf)\n",
    "        print(f\"\\nROC AUC: {auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nROC AUC: Error - {e}\")\n",
    "        auc = None\n",
    "    \n",
    "    # Per-subject accuracy (if per_subject_metrics available)\n",
    "    # This would need to filter per-subject metrics by confident epochs\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"n_confident\": np.sum(confident_mask),\n",
    "        \"y_true\": y_true_conf,\n",
    "        \"y_pred\": y_pred_conf,\n",
    "        \"prob_ec\": prob_ec_conf,\n",
    "    }\n",
    "\n",
    "# Analyze confident predictions for all models\n",
    "confident_performances = {}\n",
    "\n",
    "for model_name, model_data in model_outputs.items():\n",
    "    perf = analyze_confident_predictions(model_data, model_name)\n",
    "    if perf is not None:\n",
    "        confident_performances[model_name] = perf\n",
    "\n",
    "# Confusion matrices for confident predictions (one per model)\n",
    "if confident_performances:\n",
    "    keys = list(confident_performances.keys())\n",
    "    n = len(keys)\n",
    "    ncols = 2\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 4*nrows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "    for ax, model_name in zip(axes, keys):\n",
    "        perf = confident_performances[model_name]\n",
    "        y_true = perf['y_true']\n",
    "        y_pred = perf['y_pred']\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['EO','EC'], yticklabels=['EO','EC'], ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "        ax.set_title(f\"{model_short_key(model_name)} | n={perf.get('n_confident', len(y_true))}\")\n",
    "\n",
    "    # Hide unused axes\n",
    "    for ax in axes[len(keys):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curves for confident predictions\n",
    "if confident_performances:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (model_name, perf) in enumerate(confident_performances.items()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        y_true = perf[\"y_true\"]\n",
    "        prob_ec = perf[\"prob_ec\"]\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true, prob_ec)\n",
    "        auc_score = perf.get(\"auc\")\n",
    "        \n",
    "        ax.plot(fpr, tpr, linewidth=2, label=f\"ROC (AUC = {auc_score:.3f})\" if auc_score else \"ROC\")\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label=\"Random\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.set_title(f\"{pretty_model_name(model_name)}\\n(Confident Predictions Only)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Tests\n",
    "\n",
    "Comparing models to each other and to a baseline (majority class predictor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Statistical Tests: Model Comparisons\n",
    "# -----------------\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_baseline_predictions(y_true):\n",
    "    \"\"\"Get baseline predictions (majority class).\"\"\"\n",
    "    majority_class = Counter(y_true).most_common(1)[0][0]\n",
    "    return np.array([majority_class] * len(y_true))\n",
    "\n",
    "def run_mcnemar_test(y_true, pred_a, pred_b, name_a, name_b):\n",
    "    \"\"\"Run McNemar's test comparing two models.\"\"\"\n",
    "    # Contingency table: [[both correct, a only], [b only, both wrong]]\n",
    "    a_correct = (pred_a == y_true)\n",
    "    b_correct = (pred_b == y_true)\n",
    "    \n",
    "    both_correct = np.sum(a_correct & b_correct)\n",
    "    a_only = np.sum(a_correct & ~b_correct)\n",
    "    b_only = np.sum(~a_correct & b_correct)\n",
    "    both_wrong = np.sum(~a_correct & ~b_correct)\n",
    "    \n",
    "    table = [[both_correct, a_only],\n",
    "             [b_only, both_wrong]]\n",
    "    \n",
    "    try:\n",
    "        result = mcnemar(table, exact=True)\n",
    "        return {\n",
    "            \"table\": table,\n",
    "            \"statistic\": result.statistic,\n",
    "            \"pvalue\": result.pvalue,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in McNemar test: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_wilcoxon_test(acc_a, acc_b, name_a, name_b):\n",
    "    \"\"\"Run Wilcoxon signed-rank test on per-subject accuracies.\"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(acc_a) | np.isnan(acc_b))\n",
    "    acc_a_clean = acc_a[mask]\n",
    "    acc_b_clean = acc_b[mask]\n",
    "    \n",
    "    if len(acc_a_clean) < 3:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        statistic, pvalue = stats.wilcoxon(acc_a_clean, acc_b_clean)\n",
    "        return {\n",
    "            \"statistic\": statistic,\n",
    "            \"pvalue\": pvalue,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Wilcoxon test: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL TESTS: Model Comparisons\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare each model to baseline\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Model vs Baseline (Majority Class)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "baseline_results = {}\n",
    "for model_name, perf in model_performances.items():\n",
    "    y_true = perf[\"y_true\"]\n",
    "    y_pred = perf[\"y_pred\"]\n",
    "    baseline_pred = get_baseline_predictions(y_true)\n",
    "    \n",
    "    baseline_acc = accuracy_score(y_true, baseline_pred)\n",
    "    model_acc = perf[\"accuracy\"]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Model accuracy: {model_acc:.4f}\")\n",
    "    print(f\"  Baseline accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"  Improvement: {model_acc - baseline_acc:.4f}\")\n",
    "    \n",
    "    # McNemar test vs baseline\n",
    "    result = run_mcnemar_test(y_true, y_pred, baseline_pred, model_name, \"Baseline\")\n",
    "    if result:\n",
    "        print(f\"  McNemar test vs baseline:\")\n",
    "        print(f\"    Statistic: {result['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {result['pvalue']:.6f}\")\n",
    "        print(f\"    Significant: {'Yes' if result['pvalue'] < 0.05 else 'No'} (α=0.05)\")\n",
    "    \n",
    "    baseline_results[model_name] = {\n",
    "        \"baseline_acc\": baseline_acc,\n",
    "        \"model_acc\": model_acc,\n",
    "        \"mcnemar\": result,\n",
    "    }\n",
    "\n",
    "# Pairwise model comparisons (epoch-level using McNemar)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Pairwise Model Comparisons (McNemar's Test)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "model_list = list(model_performances.items())\n",
    "for i in range(len(model_list)):\n",
    "    for j in range(i+1, len(model_list)):\n",
    "        name_a, perf_a = model_list[i]\n",
    "        name_b, perf_b = model_list[j]\n",
    "        \n",
    "        y_true_a = perf_a[\"y_true\"]\n",
    "        y_true_b = perf_b[\"y_true\"]\n",
    "        y_pred_a = perf_a[\"y_pred\"]\n",
    "        y_pred_b = perf_b[\"y_pred\"]\n",
    "        \n",
    "        # Check if same test set (same length and labels)\n",
    "        if len(y_true_a) == len(y_true_b) and np.array_equal(y_true_a, y_true_b):\n",
    "            print(f\"\\n{name_a} vs {name_b}:\")\n",
    "            result = run_mcnemar_test(y_true_a, y_pred_a, y_pred_b, name_a, name_b)\n",
    "            if result:\n",
    "                print(f\"  McNemar statistic: {result['statistic']:.4f}\")\n",
    "                print(f\"  p-value: {result['pvalue']:.6f}\")\n",
    "                print(f\"  Significant: {'Yes' if result['pvalue'] < 0.05 else 'No'} (α=0.05)\")\n",
    "\n",
    "# Per-subject comparisons (Wilcoxon signed-rank test)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Per-Subject Model Comparisons (Wilcoxon Signed-Rank Test)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Collect per-subject accuracies\n",
    "subject_accuracies = {}\n",
    "\n",
    "for model_name, model_data in model_outputs.items():\n",
    "    if model_data.get(\"per_subject_metrics\") is not None:\n",
    "        df = model_data[\"per_subject_metrics\"]\n",
    "        if \"subject_id\" in df.columns and \"accuracy\" in df.columns:\n",
    "            subject_accuracies[model_name] = df.set_index(\"subject_id\")[\"accuracy\"].to_dict()\n",
    "\n",
    "if len(subject_accuracies) >= 2:\n",
    "    model_names = list(subject_accuracies.keys())\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            name_a, name_b = model_names[i], model_names[j]\n",
    "            \n",
    "            # Get common subjects\n",
    "            subjects_a = set(subject_accuracies[name_a].keys())\n",
    "            subjects_b = set(subject_accuracies[name_b].keys())\n",
    "            common_subjects = sorted(subjects_a & subjects_b)\n",
    "            \n",
    "            if len(common_subjects) >= 3:\n",
    "                acc_a = np.array([subject_accuracies[name_a][s] for s in common_subjects])\n",
    "                acc_b = np.array([subject_accuracies[name_b][s] for s in common_subjects])\n",
    "                \n",
    "                print(f\"\\n{name_a} vs {name_b}:\")\n",
    "                print(f\"  Common subjects: {len(common_subjects)}\")\n",
    "                print(f\"  Mean accuracy A: {np.mean(acc_a):.4f}\")\n",
    "                print(f\"  Mean accuracy B: {np.mean(acc_b):.4f}\")\n",
    "                \n",
    "                result = run_wilcoxon_test(acc_a, acc_b, name_a, name_b)\n",
    "                if result:\n",
    "                    print(f\"  Wilcoxon statistic: {result['statistic']:.4f}\")\n",
    "                    print(f\"  p-value: {result['pvalue']:.6f}\")\n",
    "                    print(f\"  Significant: {'Yes' if result['pvalue'] < 0.05 else 'No'} (α=0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Statistical Tests Complete\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Statisticla test of the different models compared to each other\n",
    "# Does is make sense to do it for all of them or just for the onse trained on the same dataset? (pairwise model comparisons) Bonferroni correction?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific subjects analysis\n",
    "# Analyze test accuracies for subjects across OLD models.\n",
    "\n",
    "SPEC_SUBJECTS = [10135, 10171, 10193, 10203, 10204]\n",
    "SPECIFIC_USE = 'fooof_only'  # 'fooof_only' or 'both'\n",
    "USE_PSD = SPECIFIC_USE not in {'fooof_only','fooof'}\n",
    "\n",
    "old_fooof = _get_model_by_prefix('old_dataset__fooof')\n",
    "old_psd = _get_model_by_prefix('old_dataset__no_fooof') if USE_PSD else None\n",
    "\n",
    "if old_fooof is None:\n",
    "    print('Missing old FOOOF model outputs.')\n",
    "elif USE_PSD and old_psd is None:\n",
    "    print('Missing old PSD model outputs (set SPECIFIC_USE=fooof_only to skip).')\n",
    "else:\n",
    "    df_fooof = attach_subject_mapping(old_fooof, index_map_old)\n",
    "    df_fooof = df_fooof[df_fooof['subject_id'].isin(SPEC_SUBJECTS)].copy()\n",
    "\n",
    "    df_psd = pd.DataFrame()\n",
    "    if old_psd is not None:\n",
    "        df_psd = attach_subject_mapping(old_psd, index_map_old)\n",
    "        df_psd = df_psd[df_psd['subject_id'].isin(SPEC_SUBJECTS)].copy()\n",
    "\n",
    "    print('Rows (fooof):', len(df_fooof), '| Rows (psd):', len(df_psd))\n",
    "\n",
    "    def _summ(df: pd.DataFrame, name: str):\n",
    "        if df is None or df.empty:\n",
    "            print(name, ': no rows')\n",
    "            return None\n",
    "        y_true = df['y_true_model'].to_numpy(dtype=int)\n",
    "        prob = df['prob_ec'].to_numpy(dtype=float)\n",
    "        y_pred = (prob >= 0.5).astype(int)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(name)\n",
    "        print(\"=\"*60)\n",
    "        print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "        print('Confusion:\\n', confusion_matrix(y_true, y_pred))\n",
    "        try:\n",
    "            print('AUC:', roc_auc_score(y_true, prob))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        per_subj = df.groupby('subject_id').apply(lambda g: accuracy_score(g['y_true_model'], (g['prob_ec']>=0.5).astype(int))).reset_index(name='accuracy')\n",
    "        print('\\nPer-subject accuracy:')\n",
    "        print(per_subj.sort_values('subject_id').to_string(index=False))\n",
    "        return per_subj\n",
    "\n",
    "    per_fooof = _summ(df_fooof, 'old_fooof (specific subjects)')\n",
    "    per_psd = _summ(df_psd, 'old_psd (specific subjects)') if USE_PSD else None\n",
    "\n",
    "    # ROC curves\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    if df_fooof is not None and not df_fooof.empty:\n",
    "        fpr, tpr, _ = roc_curve(df_fooof['y_true_model'].to_numpy(dtype=int), df_fooof['prob_ec'].to_numpy(dtype=float))\n",
    "        ax.plot(fpr, tpr, label='old_fooof')\n",
    "    if USE_PSD and df_psd is not None and not df_psd.empty:\n",
    "        fpr, tpr, _ = roc_curve(df_psd['y_true_model'].to_numpy(dtype=int), df_psd['prob_ec'].to_numpy(dtype=float))\n",
    "        ax.plot(fpr, tpr, label='old_psd')\n",
    "    ax.plot([0,1],[0,1],'k--',alpha=0.4)\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('ROC (specific subjects, OLD dataset)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Confusion matrices\n",
    "    if USE_PSD:\n",
    "        fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
    "        for ax, df, label in [(axes[0], df_fooof, 'old_fooof'), (axes[1], df_psd, 'old_psd')]:\n",
    "            if df is None or df.empty:\n",
    "                ax.axis('off'); continue\n",
    "            y_true = df['y_true_model'].to_numpy(dtype=int)\n",
    "            y_pred = (df['prob_ec'].to_numpy(dtype=float) >= 0.5).astype(int)\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=['EO','EC'], yticklabels=['EO','EC'])\n",
    "            ax.set_title(label)\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('True')\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Accuracy per subject plot\n",
    "    if per_fooof is not None and (not USE_PSD or per_psd is not None):\n",
    "        if USE_PSD:\n",
    "            merged = per_fooof.merge(per_psd, on='subject_id', how='outer', suffixes=('_fooof','_psd')).sort_values('subject_id')\n",
    "            x = np.arange(len(merged))\n",
    "            width = 0.35\n",
    "            plt.figure(figsize=(10,4))\n",
    "            plt.bar(x - width/2, merged['accuracy_fooof'], width, label='old_fooof')\n",
    "            plt.bar(x + width/2, merged['accuracy_psd'], width, label='old_psd')\n",
    "            plt.xticks(x, [str(int(s)) for s in merged['subject_id']], rotation=45, ha='right')\n",
    "            plt.ylim(0, 1.02)\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.title('Per-subject accuracy (specific subjects)')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.legend(); plt.tight_layout(); plt.show()\n",
    "        else:\n",
    "            merged = per_fooof.sort_values('subject_id')\n",
    "            x = np.arange(len(merged))\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.bar(x, merged['accuracy'], label='old_fooof')\n",
    "            plt.xticks(x, [str(int(s)) for s in merged['subject_id']], rotation=45, ha='right')\n",
    "            plt.ylim(0, 1.02)\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.title('Per-subject accuracy (specific subjects, fooof only)')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
