{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d76d38",
   "metadata": {},
   "source": [
    "# EC vs EO Classification Pipeline\n",
    "\n",
    "This notebook builds an **EC (eyes closed)** vs **EO (eyes open)** classifier from *clinical* EEG data.\n",
    "\n",
    "**High-level flow**\n",
    "1. Import dependencies.\n",
    "2. Set global toggles (channel selection, feature mode, CV settings, temporal smoothing).\n",
    "3. Resolve input files:\n",
    "   - **OLD dataset**: EC/EO marked EEGLAB `.set` files.\n",
    "   - **NEW dataset (optional)**: preprocessed MNE epoch `.fif` files (doctor a/b).\n",
    "4. Extract features per epoch:\n",
    "   - **PSD features** (optionally frequency-binned), or\n",
    "   - **FOOOF/specparam features** (optionally `ONE_MAIN_FOOOF` subject-level alpha template).\n",
    "5. Train + evaluate **logistic regression** with subject-wise CV and optional temporal smoothing.\n",
    "6. Generate diagnostics/plots and save artifacts under `outputs/`.\n",
    "\n",
    "**Core globals created downstream**\n",
    "- Feature/label arrays: `X_combined`, `y_combined`, `subject_ids`\n",
    "- Spectral data (for plots): `psd_cube`, `psd_freqs`, `feature_channels`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ.setdefault(\"NUMBA_DISABLE_CACHE\", \"1\")\n",
    "os.environ.setdefault(\"NUMBA_CACHE_DIR\", str((Path.cwd() / \".numba_cache\").resolve()))\n",
    "Path(os.environ[\"NUMBA_CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import stft\n",
    "import joblib\n",
    "import mne\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    log_loss,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd6aff",
   "metadata": {},
   "source": [
    "## Data Locations & Global Toggles\n",
    "\n",
    "This section controls *what data is loaded* and *how the pipeline behaves*.\n",
    "\n",
    "**Data locations (clinical)**\n",
    "- OLD `.set` data can be pointed to via environment variables:\n",
    "  - `EC_EO_OPEN_DIR` (EO)\n",
    "  - `EC_EO_CLOSED_DIR` (EC)\n",
    "- NEW preprocessed epoch `.fif` data can be pointed to via:\n",
    "  - `NEW_EEG_PROCESSED_DIR`\n",
    "\n",
    "**Most important toggles**\n",
    "- `NEW_DATA`: use NEW `.fif` epochs instead of OLD `.set` files.\n",
    "- `USE_BOTH_DATASETS` / `TEST_ON_OTHER_DATASET`: cross-dataset evaluation modes.\n",
    "- `CHANNEL_SELECTION`: list of channels to use, or `'all'`.\n",
    "- `USE_FOOOF`: feature type (`False` = PSD, `True` = FOOOF/specparam).\n",
    "- `ONE_MAIN_FOOOF`: subject-level alpha template mode (FOOOF only).\n",
    "- `CV_LEVEL` and related knobs: define the subject-wise evaluation scheme.\n",
    "- `USE_TIME_ADJUSTMENT`: enables run-length smoothing of predicted labels (uses `TIME_AXIS_MODE`).\n",
    "\n",
    "Set these once up front, then run the next cell(s) top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d308652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import platform\n",
    "\n",
    "# -----------------\n",
    "# High-level toggles\n",
    "# -----------------\n",
    "COMBINE_ADJACENT_EPOCHS = False  # Pair consecutive clean epochs into 2-second windows\n",
    "NEW_DATA = False                 # True -> use NEW preprocessed FIF dataset (doctor a/b union); False -> use old EC/EO .set files\n",
    "\n",
    "# Dataset evaluation / mixing toggles\n",
    "TEST_ON_OTHER_DATASET = False   # If True: train on selected dataset (NEW_DATA), test on the other dataset\n",
    "USE_BOTH_DATASETS = False       # If True: load NEW+OLD together and run subject-wise CV over the combined pool\n",
    "DATASET_SUBJECT_OFFSET = 1000000  # Offset used to avoid subject_id collisions across datasets\n",
    "CROSS_DATASET_TEST = bool(TEST_ON_OTHER_DATASET and (not USE_BOTH_DATASETS))\n",
    "\n",
    "# How to build the per-epoch \"time_idx\" used in timeline plots\n",
    "# - \"append_files\": old behavior (EO file(s) then EC file(s))\n",
    "# - \"align_conditions\": use the epoch number as the timeline (best when EO/EC files are two label-views of the same 1800 epochs)\n",
    "# - \"interleave_conditions\": treat EO epoch k then EC epoch k as consecutive time steps (t=2k, t=2k+1)\n",
    "TIME_AXIS_MODE = \"align_conditions\"\n",
    "CHANNEL_SELECTION = [   \n",
    "                     #     'O1', 'O2'\n",
    "                     #   , 'P3', 'P4', 'P7', 'P8', 'Pz'\n",
    "                     #   , 'F3', 'F4', 'C3', 'C4', 'F7', 'F8', 'T7', 'T8', 'Fz', 'Cz'\n",
    "                     #   , 'Fp1', 'Fp2'\n",
    "                        'all'\n",
    "                     #   \"Fp1\",\"Fp2\",\"F3\",\"F4\",\"C3\",\"C4\",\"P3\",\"P4\",\"O1\",\"O2\",\"F7\",\"F8\",\"T7\",\"T8\",\"P7\",\"P8\",\"T9\", \"T10\", \"Fz\"\n",
    "                     ]  # \"O1\", \"O2\", \"P3\", \"P4\", \"P7\", \"P8\"  or \"all\" to use every EEG channel available\n",
    "USE_FOOOF = False             # True -> FOOOF (specparam) features, False -> PSD features\n",
    "USE_SAVED_FOOOF = True        # If True and available: reuse precomputed ONE_MAIN_FOOOF features (only applies for ONE_MAIN_FOOOF)\n",
    "ONE_MAIN_FOOOF = True  # Use subject-level alpha template FOOOF mode\n",
    "MAIN_FOOOF_USE_ALL_EPOCHS = True  # If True, build SUBJECT_ALPHA_PROFILE from all finite epochs (ignores per-epoch labels / rejmanual)\n",
    "\n",
    "# Logistic regression penalty tuning\n",
    "TUNE_LOGREG_PENALTY = False   # If True, compare L2 vs L1 during inner-loop tuning\n",
    "LOGREG_PENALTY_OPTIONS = [\"l2\", \"l1\"]\n",
    "LOGREG_PENALTY_FIXED = \"l2\"   # Used when TUNE_LOGREG_PENALTY=False\n",
    "LOGREG_MAX_ITER = 2000        # Used for all logistic regression fits\n",
    "ALPHA_PROFILE_RANGE = (4.0, 16.0)  # Hz range to find subject alpha peak\n",
    "ALPHA_PROFILE_ROI = ['O1', 'O2', 'P3', 'P4', 'P7', 'P8', 'Pz']  # channels used to build alpha profile\n",
    "\n",
    "# -----------------\n",
    "# Channel sanity checks (especially for NEW .fif dataset)\n",
    "# -----------------\n",
    "# The NEW dataset is intended to be 19ch 10-20. If your processed files contain extra channels\n",
    "# or inconsistent naming across subjects, the pipeline will build a UNION, increasing feature dims.\n",
    "STANDARD_19_CHANNELS = [\n",
    "    \"Fp1\", \"Fp2\", \"F3\", \"F4\", \"C3\", \"C4\", \"P3\", \"P4\", \"O1\", \"O2\",\n",
    "    \"F7\", \"F8\", \"T7\", \"T8\", \"P7\", \"P8\", \"Fz\", \"Cz\", \"Pz\",\n",
    "]\n",
    "EXPECTED_NEW_CHANNEL_COUNT = 19\n",
    "FAIL_IF_NEW_CHANNEL_MISMATCH = True\n",
    "# If True, force NEW dataset training to exactly STANDARD_19_CHANNELS (skip subjects missing any).\n",
    "FORCE_STANDARD_19_FOR_NEW = False\n",
    "USE_CLASS_WEIGHTS = True    # Toggle to enable class_weight='balanced' in classifiers\n",
    "CLASS_WEIGHT = \"balanced\" if USE_CLASS_WEIGHTS else None\n",
    "USE_FREQ_BINNING = True      # Only relevant when USE_FOOOF is False\n",
    "FREQ_BIN_OPTIONS = [5, 10, 15, 20, 25, 30]\n",
    "C_GRID = [0.01, 0.1, 0.2, 0.5, 1.0]\n",
    "\n",
    "# FOOOF feature selection (applies when USE_FOOOF is True)\n",
    "# Choose a subset of [\"offset\", \"exponent\", \"alpha_cf\", \"alpha_amp\", \"alpha_bw\"]\n",
    "FOOOF_SELECTED_FEATURES = [\n",
    "    \"offset\", \"exponent\",  # Aperiodic features\n",
    "    #\"alpha_cf\", \"alpha_bw\", # center frequency and bandwidth\n",
    "    \"alpha_amp\" # amplitude\n",
    "    ]\n",
    "\n",
    "# PSD feature selection (applies when USE_FOOOF is False)\n",
    "# Set PSD_FEATURE_RANGE to a tuple like (8.0, 12.0) to restrict\n",
    "# classifier features to that frequency band; use None for full range.\n",
    "PSD_FEATURE_RANGE = None #(8.0, 12.0) # or None \n",
    "\n",
    "CV_LEVEL = 2                  # 1 = single hold-out, 2 = covering CV, 3 = repeated covering CV, 4 = fixed test split\n",
    "CV_TEST_SUBJECTS_PER_SPLIT = 5  # Number of subjects assigned to each held-out pane\n",
    "CV_REPEAT_COUNT = 3           # Only applies when CV_LEVEL==3\n",
    "CV_RANDOM_SEED = 13           # Controls subject shuffling for reproducibility\n",
    "FIXED_TEST_SUBJECTS_LEVEL4 = [10135, 10171, 10193, 10203, 10204]\n",
    "\n",
    "# ---- Component analysis (optional PCA/ICA) ----\n",
    "USE_COMPONENT_ANALYSIS = False    # True to enable PCA/ICA before logistic regression\n",
    "COMPONENT_METHOD = \"pca\"          # \"pca\" or \"ica\"\n",
    "COMPONENT_N_COMPONENTS = 10     # int or None; if None a default is used # this is max components in training\n",
    "ELBOW_MAX_COMPONENTS = 50         # max components shown in elbow plot\n",
    "\n",
    "# Temporal smoothing of predicted labels\n",
    "USE_TIME_ADJUSTMENT = True      # Toggle run-length smoothing of predictions\n",
    "LENGTH_TUNING = True            # If True, tune run-length on out-of-fold predictions\n",
    "LENGTH_GRID = [1, 3, 5, 7, 9, 11, 15, 21]  # Candidate run lengths (in epochs)\n",
    "LENGTH_TUNING_METRIC = \"balanced_accuracy\"  # \"balanced_accuracy\" or \"accuracy\"\n",
    "MIN_RUN_LENGTH = 5              # Minimum interior run length to keep as-is\n",
    "USE_EDGE_SMOOTHING = True       # Also smooth short runs at the edges\n",
    "MIN_RUN_LENGTH_EDGE = 5         # Edge-window span (in original epochs) for majority vote\n",
    "\n",
    "\n",
    "# PSD and FOOOF parameters\n",
    "PSD_KWARGS = dict(method=\"welch\", fmin=1.0, fmax=45.0)\n",
    "ALPHA_BAND = (8.0, 12.0)\n",
    "ALPHA_PRIMARY_RANGE = (8.0, 14.0)\n",
    "ALPHA_EXPANDED_RANGE = (4.0, 17.0)\n",
    "FOOOF_SETTINGS = dict(\n",
    "    aperiodic_mode=\"fixed\",\n",
    "    peak_width_limits=(0.5, 12.0),\n",
    "    max_n_peaks=6,\n",
    "    min_peak_height=0.05,\n",
    "    peak_threshold=2.0,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2362de",
   "metadata": {},
   "source": [
    "## Paths, Data Discovery, and Output Folders\n",
    "\n",
    "This cell sets up **where data comes from** and **where results are written**.\n",
    "\n",
    "**What it does**\n",
    "- Detects a plausible project root.\n",
    "- Defines helper functions for locating directories and collecting `.set` files.\n",
    "- Reads optional environment overrides (`EC_EO_OPEN_DIR`, `EC_EO_CLOSED_DIR`, `NEW_EEG_PROCESSED_DIR`).\n",
    "- Creates an `outputs/` folder and helper `outpath(...)` for saving artifacts.\n",
    "- Resolves input files:\n",
    "  - OLD `.set` files → `eyes_open_files`, `eyes_closed_files`, `set_files`, `set_labels`.\n",
    "  - NEW `.fif` epoch files (if enabled) → `new_subject_pairs`.\n",
    "\n",
    "Run it after configuring toggles in the previous cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbac29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------\n",
    "# Helper utilities for locating data\n",
    "# -----------------\n",
    "def guess_project_root() -> Path:\n",
    "    \"\"\"Walk up the directory tree to find a plausible project root.\"\"\"\n",
    "    p = Path.cwd().resolve()\n",
    "    for _ in range(6):\n",
    "        if (p / \".git\").exists() or (p / \"data\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "def first_existing(candidates):\n",
    "    \"\"\"Return the first existing Path from a list of candidates.\"\"\"\n",
    "    for candidate in candidates:\n",
    "        if candidate is None:\n",
    "            continue\n",
    "        path_obj = Path(candidate).expanduser()\n",
    "        try:\n",
    "            if path_obj.exists():\n",
    "                return path_obj.resolve()\n",
    "        except OSError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def collect_set_files(directory: Path, recursive: bool = True):\n",
    "    \"\"\"Collect .set files (case-insensitive) from a directory.\"\"\"\n",
    "    if directory is None or not directory.exists():\n",
    "        return []\n",
    "    patterns = (\"*.set\", \"*.SET\")\n",
    "    if recursive:\n",
    "        files = [p.resolve() for pat in patterns for p in directory.rglob(pat)]\n",
    "    else:\n",
    "        files = [p.resolve() for pat in patterns for p in directory.glob(pat)]\n",
    "    return sorted({str(f) for f in files})\n",
    "\n",
    "def is_wsl() -> bool:\n",
    "    \"\"\"Detect Windows Subsystem for Linux.\"\"\"\n",
    "    try:\n",
    "        return \"microsoft\" in platform.uname().release.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "project_root = guess_project_root()\n",
    "system = platform.system()\n",
    "wsl = is_wsl()\n",
    "\n",
    "ENV = {\n",
    "    \"EC_EO_CLOSED_DIR\": os.getenv(\"EC_EO_CLOSED_DIR\"),\n",
    "    \"EC_EO_OPEN_DIR\": os.getenv(\"EC_EO_OPEN_DIR\"),\n",
    "    \"NEW_EEG_PROCESSED_DIR\": os.getenv(\"NEW_EEG_PROCESSED_DIR\"),\n",
    "}\n",
    "\n",
    "WIN_CLOSED = [\n",
    "    r\"E:\\Saxe_sandkasse\\30EOEC_filer\\Closed_marked\",\n",
    "    r\"E:/Saxe_sandkasse/30EOEC_filer/Closed_marked\",\n",
    "]\n",
    "WIN_OPEN = [\n",
    "    r\"E:\\Saxe_sandkasse\\30EOEC_filer\\Open_marked\",\n",
    "    r\"E:/Saxe_sandkasse/30EOEC_filer/Open_marked\",\n",
    "]\n",
    "WSL_CLOSED = [r\"/mnt/e/Saxe_sandkasse/30EOEC_filer/Closed_marked\"]\n",
    "WSL_OPEN = [r\"/mnt/e/Saxe_sandkasse/30EOEC_filer/Open_marked\"]\n",
    "\n",
    "REL_CLINICAL_CLOSED = [\n",
    "    r\"E:\\Saxe_sandkasse\\30EOEC_filer\\Closed_marked\",\n",
    "    project_root / \"data/30EOEC_filer/Closed_marked\",\n",
    "]\n",
    "REL_CLINICAL_OPEN = [\n",
    "    r\"E:\\Saxe_sandkasse\\30EOEC_filer\\Open_marked\",\n",
    "    project_root / \"data/30EOEC_filer/Open_marked\",\n",
    "]\n",
    "\n",
    "# NEW data locations (preprocessed FIF epoch files)\n",
    "NEW_MAC_PROCESSED = [\n",
    "    r\"/Users/Saxe/Desktop/GitHub/EEG-classifiers/data/NEW_processed.nosync\",\n",
    "    project_root / \"data/NEW_processed.nosync\",\n",
    "]\n",
    "NEW_WIN_PROCESSED = [\n",
    "    r\"G:\\ChristianMusaeus\\New_EEG\\Processed\",\n",
    "]\n",
    "NEW_WSL_PROCESSED = [r\"/mnt/g/ChristianMusaeus/New_EEG/Processed\"]\n",
    "\n",
    "print(\"System:\", system, \"(WSL:\", wsl, \")\")\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"NEW_DATA:\", NEW_DATA)\n",
    "print(\"Channel selection:\", CHANNEL_SELECTION)\n",
    "print(\"Using FOOOF features:\" if USE_FOOOF else \"Using PSD features only:\", USE_FOOOF)\n",
    "print(\"Class weights:\", CLASS_WEIGHT)\n",
    "print(\"Combine adjacent epochs:\", COMBINE_ADJACENT_EPOCHS)\n",
    "print(\"Time axis mode:\", TIME_AXIS_MODE)\n",
    "\n",
    "# -------------------- Output folder setup --------------------\n",
    "# Save all generated artifacts under: <notebook_dir>/outputs/<config_tag>/...\n",
    "from typing import Optional\n",
    "def _detect_notebook_path() -> Optional[Path]:\n",
    "    # VS Code notebooks sometimes inject this.\n",
    "    try:\n",
    "        vsc = globals().get('__vsc_ipynb_file__', None)\n",
    "        if vsc:\n",
    "            p = Path(str(vsc)).expanduser()\n",
    "            if p.suffix.lower() == '.ipynb' and p.exists():\n",
    "                return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Env override (optional)\n",
    "    for key in (\"NOTEBOOK_PATH\", \"IPYNB_PATH\"):\n",
    "        v = os.getenv(key)\n",
    "        if v:\n",
    "            try:\n",
    "                p = Path(v).expanduser()\n",
    "                if p.suffix.lower() == '.ipynb' and p.exists():\n",
    "                    return p.resolve()\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Repo-local fallback (this notebook lives at New_EEG/EC_EO_Classifier.ipynb)\n",
    "    try:\n",
    "        guess = Path(project_root) / 'New_EEG' / 'EC_EO_Classifier.ipynb'\n",
    "        if guess.exists():\n",
    "            return guess.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Search fallback\n",
    "    try:\n",
    "        for p in Path(project_root).rglob('EC_EO_Classifier.ipynb'):\n",
    "            return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "NOTEBOOK_PATH = _detect_notebook_path()\n",
    "NOTEBOOK_DIR = NOTEBOOK_PATH.parent if NOTEBOOK_PATH is not None else Path.cwd().resolve()\n",
    "OUTPUTS_ROOT = NOTEBOOK_DIR / 'outputs'\n",
    "OUTPUTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SAVED_FOOOF_ROOT = OUTPUTS_ROOT / 'saved_fooof'\n",
    "\n",
    "def _safe_tag(s: str) -> str:\n",
    "    s = re.sub(r'[^a-zA-Z0-9._-]+', '_', str(s))\n",
    "    s = re.sub(r'_+', '_', s).strip('_')\n",
    "    return s[:120] if len(s) > 120 else s\n",
    "\n",
    "def _channel_tag() -> str:\n",
    "    try:\n",
    "        sel = list(CHANNEL_SELECTION)\n",
    "    except Exception:\n",
    "        sel = []\n",
    "    sel_norm = [str(x).strip() for x in sel if x is not None]\n",
    "    if any(x.lower() == 'all' for x in sel_norm):\n",
    "        return 'allch'\n",
    "    if not sel_norm:\n",
    "        return 'ch_unknown'\n",
    "    joined = '-'.join(_safe_tag(x.upper()) for x in sel_norm)\n",
    "    return f\"ch_{joined}\"[:80]\n",
    "\n",
    "def _dataset_tag() -> str:\n",
    "    if 'USE_BOTH_DATASETS' in globals() and USE_BOTH_DATASETS:\n",
    "        return 'combined_datasets'\n",
    "    if 'CROSS_DATASET_TEST' in globals() and CROSS_DATASET_TEST:\n",
    "        return 'train_new_test_old' if NEW_DATA else 'train_old_test_new'\n",
    "    return 'new_dataset' if NEW_DATA else 'old_dataset'\n",
    "\n",
    "def _fooof_tag() -> str:\n",
    "    return 'fooof' if USE_FOOOF else 'no_fooof'\n",
    "\n",
    "def _config_subdir() -> str:\n",
    "    parts = [_dataset_tag(), _fooof_tag(), _channel_tag(), f\"cv{CV_LEVEL}\"]\n",
    "    try:\n",
    "        parts.append(f\"time_{_safe_tag(TIME_AXIS_MODE)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    if USE_FOOOF and ONE_MAIN_FOOOF:\n",
    "        parts.append('one_main_fooof')\n",
    "        if 'MAIN_FOOOF_USE_ALL_EPOCHS' in globals() and MAIN_FOOOF_USE_ALL_EPOCHS:\n",
    "            parts.append('mainfooof_all_epochs')\n",
    "    if 'USE_COMPONENT_ANALYSIS' in globals() and USE_COMPONENT_ANALYSIS:\n",
    "        parts.append(f\"comp_{_safe_tag(COMPONENT_METHOD)}{COMPONENT_N_COMPONENTS}\")\n",
    "    if 'TUNE_LOGREG_PENALTY' in globals() and TUNE_LOGREG_PENALTY:\n",
    "        parts.append('tune_penalty')\n",
    "    else:\n",
    "        try:\n",
    "            parts.append(f\"pen_{_safe_tag(LOGREG_PENALTY_FIXED)}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return '__'.join([p for p in parts if p])\n",
    "\n",
    "def get_output_dir() -> Path:\n",
    "    out_dir = OUTPUTS_ROOT / _config_subdir()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "def outpath(filename: str) -> Path:\n",
    "    return get_output_dir() / str(filename)\n",
    "\n",
    "print('Notebook dir:', NOTEBOOK_DIR)\n",
    "print('Outputs root:', OUTPUTS_ROOT)\n",
    "print('Saved-FOOOF root:', SAVED_FOOOF_ROOT)\n",
    "print('Output dir:', get_output_dir())\n",
    "\n",
    "# -------------------- Resolve input files --------------------\n",
    "# Depending on TEST_ON_OTHER_DATASET / USE_BOTH_DATASETS, we may need to resolve BOTH datasets.\n",
    "if USE_BOTH_DATASETS and TEST_ON_OTHER_DATASET:\n",
    "    warnings.warn('USE_BOTH_DATASETS=True overrides TEST_ON_OTHER_DATASET; running combined CV instead of cross-test.')\n",
    "need_new = bool(NEW_DATA or CROSS_DATASET_TEST or USE_BOTH_DATASETS)\n",
    "need_old = bool((not NEW_DATA) or CROSS_DATASET_TEST or USE_BOTH_DATASETS)\n",
    "mode = \"combined\" if USE_BOTH_DATASETS else (\"cross-test\" if CROSS_DATASET_TEST else \"single\")\n",
    "print(\"Dataset mode:\", mode)\n",
    "\n",
    "new_processed_dir = None\n",
    "new_subject_pairs = []  # [(subject_id, file_a, file_b), ...] for NEW_DATA\n",
    "new_epoch_files = []\n",
    "\n",
    "eyes_closed_files, eyes_open_files = [], []\n",
    "set_files, set_labels = [], np.array([], dtype=int)\n",
    "\n",
    "if need_new:\n",
    "    tried_new = [ENV[\"NEW_EEG_PROCESSED_DIR\"]] + NEW_WIN_PROCESSED + NEW_WSL_PROCESSED + NEW_MAC_PROCESSED\n",
    "\n",
    "    new_processed_dir = first_existing(tried_new)\n",
    "    print(\"Tried NEW processed paths (in order):\")\n",
    "    for path in tried_new:\n",
    "        print(\"  -\", path)\n",
    "    print(\"Resolved NEW processed dir:\", new_processed_dir)\n",
    "\n",
    "    if new_processed_dir is None:\n",
    "        warnings.warn(\"Could not resolve NEW processed directory.\")\n",
    "        new_epoch_files = []\n",
    "    else:\n",
    "        new_epoch_files = [p.resolve() for p in Path(new_processed_dir).glob(\"*_epo.fif\")] + [p.resolve() for p in Path(new_processed_dir).glob(\"*_epo.FIF\")]\n",
    "    print(f\"Total NEW epoch FIF files collected: {len(new_epoch_files)}\")\n",
    "\n",
    "    # Collect doctor a/b per subject.\n",
    "    # If a subject only has a single rater file (only 'a' OR only 'b'), keep it as a valid NEW subject entry.\n",
    "    pairs = {}\n",
    "    for p in new_epoch_files:\n",
    "        m = re.search(r\"sub(\\d+)([ab])\", p.stem, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            continue\n",
    "        sid = int(m.group(1))\n",
    "        rater = m.group(2).lower()\n",
    "        pairs.setdefault(sid, {})[rater] = str(p)\n",
    "\n",
    "    new_subject_pairs = []  # [(subject_id, file_primary, file_secondary_or_None), ...]\n",
    "    paired_count = 0\n",
    "    single_count = 0\n",
    "    for sid in sorted(pairs):\n",
    "        entry = pairs[sid]\n",
    "        if ('a' in entry) and ('b' in entry):\n",
    "            new_subject_pairs.append((sid, entry['a'], entry['b']))\n",
    "            paired_count += 1\n",
    "        elif 'a' in entry:\n",
    "            new_subject_pairs.append((sid, entry['a'], None))\n",
    "            single_count += 1\n",
    "        elif 'b' in entry:\n",
    "            new_subject_pairs.append((sid, entry['b'], None))\n",
    "            single_count += 1\n",
    "\n",
    "    print(f\"NEW subjects found: total={len(new_subject_pairs)} (paired={paired_count}, single={single_count})\")\n",
    "    if new_subject_pairs:\n",
    "        print(\"Example NEW subject entry:\", new_subject_pairs[0])\n",
    "else:\n",
    "    print(\"Skipping NEW dataset resolution (need_new=False).\")\n",
    "\n",
    "if need_old:\n",
    "    tried_closed = [ENV[\"EC_EO_CLOSED_DIR\"]] + WIN_CLOSED + WSL_CLOSED + REL_CLINICAL_CLOSED\n",
    "    tried_open = [ENV[\"EC_EO_OPEN_DIR\"]] + WIN_OPEN + WSL_OPEN + REL_CLINICAL_OPEN\n",
    "\n",
    "    eyes_closed_dir = first_existing(tried_closed)\n",
    "    eyes_open_dir = first_existing(tried_open)\n",
    "\n",
    "    print(\"Tried eyes-closed paths (in order):\")\n",
    "    for path in tried_closed:\n",
    "        print(\"  -\", path)\n",
    "    print(\"Tried eyes-open paths (in order):\")\n",
    "    for path in tried_open:\n",
    "        print(\"  -\", path)\n",
    "\n",
    "    eyes_closed_files = collect_set_files(eyes_closed_dir, recursive=True)\n",
    "    eyes_open_files = collect_set_files(eyes_open_dir, recursive=True)\n",
    "\n",
    "    if not eyes_closed_files or not eyes_open_files:\n",
    "        warnings.warn(\n",
    "            \"No .set files found for the configured clinical EC/EO paths.\"\n",
    "            f\"Resolved eyes_closed_dir: {eyes_closed_dir}\"\n",
    "            f\"Resolved eyes_open_dir  : {eyes_open_dir}\"\n",
    "            \"Tips:\"\n",
    "            \"  • Verify the folders contain .set files.\"\n",
    "            \"  • Override with env vars if the defaults differ on your machine.\"\n",
    "            \"  • If on WSL, ensure /mnt/... paths are mounted.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Clinical data: {len(eyes_closed_files)} closed files, \"\n",
    "            f\"{len(eyes_open_files)} open files.\"\n",
    "        )\n",
    "\n",
    "    set_files = eyes_open_files + eyes_closed_files\n",
    "    set_labels = np.array([0] * len(eyes_open_files) + [1] * len(eyes_closed_files))\n",
    "    print(f\"Total .set files collected: {len(set_files)}\")\n",
    "else:\n",
    "    print(\"Skipping OLD dataset resolution (need_old=False).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220614c",
   "metadata": {},
   "source": [
    "### Sanity check: confirm that files were found\n",
    "\n",
    "This cell prints one example EO and EC file path from the discovered lists.\n",
    "\n",
    "If either list is empty, go back and fix the input paths (env vars or the default Windows/WSL locations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_open = eyes_open_files[0] if eyes_open_files else \"None\"\n",
    "example_closed = eyes_closed_files[0] if eyes_closed_files else \"None\"\n",
    "print(f\"Example eyes-open file: {example_open}\")\n",
    "print(f\"Example eyes-closed file: {example_closed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456c6ac",
   "metadata": {},
   "source": [
    "## Feature Extraction & Label Preparation\n",
    "\n",
    "This stage turns raw epochs into a model-ready feature matrix.\n",
    "\n",
    "**What happens**\n",
    "- Loads epochs for EO/EC and assigns labels (`EO=0`, `EC=1`).\n",
    "- Drops rejected or invalid epochs (e.g., non-finite values or `rejmanual` where available).\n",
    "- Selects channels (`CHANNEL_SELECTION`) and aligns channels across subjects.\n",
    "- Computes PSDs for each epoch and channel (`psd_cube`, `psd_freqs`).\n",
    "- Builds features:\n",
    "  - **PSD mode** (`USE_FOOOF=False`): flattens PSD (optionally frequency-binned).\n",
    "  - **FOOOF mode** (`USE_FOOOF=True`): fits aperiodic + peaks and produces per-channel features.\n",
    "  - **ONE_MAIN_FOOOF** (`ONE_MAIN_FOOOF=True`): derives a subject-level alpha profile (`SUBJECT_ALPHA_PROFILE`) and uses it to compute a fixed alpha template amplitude per epoch.\n",
    "\n",
    "**Outputs used by training**\n",
    "- `X_combined`: shape `(n_epochs_total, n_features)`\n",
    "- `y_combined`: shape `(n_epochs_total,)`\n",
    "- `subject_ids`: shape `(n_epochs_total,)`\n",
    "\n",
    "Run this once after file discovery succeeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a072dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Feature extraction pipeline (drop-in replacement; supports \"all\" or [\"ALL\"]) ----\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "from collections import Counter, OrderedDict\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "os.environ.setdefault(\"NUMBA_DISABLE_CACHE\", \"1\")\n",
    "os.environ.setdefault(\"NUMBA_CACHE_DIR\", str((Path.cwd() / \".numba_cache\").resolve()))\n",
    "Path(os.environ[\"NUMBA_CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "import mne\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Prefer specparam; fallback to legacy fooof if needed\n",
    "try:\n",
    "    from specparam import SpectralModel\n",
    "    # If specparam FitError isn't available in your env, we create a fallback\n",
    "    try:\n",
    "        from specparam.core.errors import FitError  # type: ignore\n",
    "    except Exception:\n",
    "        class FitError(Exception):\n",
    "            pass\n",
    "    FOOOF_BACKEND = \"specparam\"\n",
    "except Exception:\n",
    "    try:\n",
    "        from fooof import FOOOF as SpectralModel\n",
    "        from fooof.core.errors import FitError\n",
    "        FOOOF_BACKEND = \"fooof\"\n",
    "    except Exception:\n",
    "        SpectralModel = None\n",
    "        class FitError(Exception):\n",
    "            pass\n",
    "        FOOOF_BACKEND = None\n",
    "        warnings.warn(\"FOOOF/specparam not available. Set USE_FOOOF=False to proceed.\")\n",
    "\n",
    "# ---------- Helper: normalize CHANNEL_SELECTION so \"ALL\" or [\"ALL\"] means use all channels ----------\n",
    "def _is_all_channels(sel):\n",
    "    if isinstance(sel, str):\n",
    "        return sel.strip().lower() == \"all\"\n",
    "    if isinstance(sel, (list, tuple, set)) and len(sel) == 1:\n",
    "        only = next(iter(sel))\n",
    "        return isinstance(only, str) and only.strip().lower() == \"all\"\n",
    "    return False\n",
    "\n",
    "ALL_CHANNELS = _is_all_channels(CHANNEL_SELECTION)\n",
    "\n",
    "ALPHA_FREQ_RANGE = (3.0, 40.0)\n",
    "\n",
    "# -------------------- Saved ONE_MAIN_FOOOF cache (precomputed) --------------------\n",
    "def _canonical_channel_name(ch_name: str) -> str:\n",
    "    name = str(ch_name).strip()\n",
    "    name = re.sub(r\"^EEG\\s+\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"-REF$\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \"\", name)\n",
    "    return name\n",
    "\n",
    "def _normalize_channel_selection(sel) -> str:\n",
    "    try:\n",
    "        items = list(sel)\n",
    "    except Exception:\n",
    "        items = []\n",
    "    items = [str(x).strip() for x in items if x is not None]\n",
    "    if any(x.lower() == 'all' for x in items):\n",
    "        return 'all'\n",
    "    return ','.join([_canonical_channel_name(x).upper() for x in items])\n",
    "\n",
    "def _saved_fooof_dataset_tag_from_file(path_str: str) -> str:\n",
    "    s = str(path_str).lower()\n",
    "    if 'open_marked' in s or 'closed_marked' in s:\n",
    "        return 'old_dataset_open_closed_marked'\n",
    "    if 'preprocessed_setfiles' in s:\n",
    "        return 'old_dataset_preprocessed_setfiles'\n",
    "    if 'new_eeg' in s and 'processed' in s:\n",
    "        return 'new_dataset_processed'\n",
    "    return 'old_dataset_preprocessed_setfiles'\n",
    "\n",
    "def _saved_fooof_expected_config_tag(dataset_tag: str) -> str:\n",
    "    parts = [\n",
    "        str(dataset_tag),\n",
    "        'fooof',\n",
    "        'one_main_fooof',\n",
    "        'cache_b',\n",
    "        _channel_tag(),\n",
    "        f\"psd_{PSD_KWARGS.get('fmin', 1.0)}-{PSD_KWARGS.get('fmax', 45.0)}Hz\",\n",
    "        f\"fit_{ALPHA_FREQ_RANGE[0]}-{ALPHA_FREQ_RANGE[1]}Hz\",\n",
    "    ]\n",
    "    if 'FOOOF_SELECTED_FEATURES' in globals() and FOOOF_SELECTED_FEATURES:\n",
    "        parts.append('feat_' + _safe_tag('-'.join(list(FOOOF_SELECTED_FEATURES))))\n",
    "    return '__'.join([p for p in parts if p])\n",
    "\n",
    "def _saved_fooof_config_ok(folder: Path) -> bool:\n",
    "    cfg_path = folder / 'config.json'\n",
    "    if not cfg_path.exists():\n",
    "        return True\n",
    "    try:\n",
    "        cfg = json.loads(cfg_path.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        return False\n",
    "    # Channel selection\n",
    "    if 'CHANNEL_SELECTION' in cfg:\n",
    "        if _normalize_channel_selection(cfg.get('CHANNEL_SELECTION')) != _normalize_channel_selection(CHANNEL_SELECTION):\n",
    "            return False\n",
    "    # PSD band\n",
    "    psd_cfg = cfg.get('PSD_KWARGS') or {}\n",
    "    if float(psd_cfg.get('fmin', PSD_KWARGS.get('fmin', 1.0))) != float(PSD_KWARGS.get('fmin', 1.0)):\n",
    "        return False\n",
    "    if float(psd_cfg.get('fmax', PSD_KWARGS.get('fmax', 45.0))) != float(PSD_KWARGS.get('fmax', 45.0)):\n",
    "        return False\n",
    "    # Must be ONE_MAIN_FOOOF cache mode B\n",
    "    if str(cfg.get('CACHE_MODE', '')).upper() != 'B':\n",
    "        return False\n",
    "    if bool(cfg.get('COMBINE_ADJACENT_EPOCHS', False)):\n",
    "        return False\n",
    "    # Fit range\n",
    "    afr = cfg.get('ALPHA_FREQ_RANGE', None)\n",
    "    if afr is not None:\n",
    "        try:\n",
    "            afr = tuple(float(x) for x in afr)\n",
    "            if tuple(float(x) for x in ALPHA_FREQ_RANGE) != afr:\n",
    "                return False\n",
    "        except Exception:\n",
    "            return False\n",
    "    # Selected features\n",
    "    if 'FOOOF_SELECTED_FEATURES' in cfg and 'FOOOF_SELECTED_FEATURES' in globals():\n",
    "        if list(cfg.get('FOOOF_SELECTED_FEATURES') or []) != list(FOOOF_SELECTED_FEATURES or []):\n",
    "            return False\n",
    "    # FOOOF settings (match on important keys)\n",
    "    want = dict(FOOOF_SETTINGS) if 'FOOOF_SETTINGS' in globals() else {}\n",
    "    have = cfg.get('FOOOF_SETTINGS') or {}\n",
    "    for k in ('aperiodic_mode', 'peak_width_limits', 'max_n_peaks', 'min_peak_height', 'peak_threshold'):\n",
    "        if k in want and k in have and want.get(k) != have.get(k):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _find_saved_fooof_npz(file_path: str, subject_raw: int) -> Optional[Path]:\n",
    "    try:\n",
    "        root = SAVED_FOOOF_ROOT\n",
    "    except Exception:\n",
    "        return None\n",
    "    if root is None or not Path(root).exists():\n",
    "        return None\n",
    "    file_tag = _safe_tag(Path(str(file_path)).stem)\n",
    "    ds_tag = _saved_fooof_dataset_tag_from_file(str(file_path))\n",
    "    cfg_tag = _saved_fooof_expected_config_tag(ds_tag)\n",
    "    cfg_dir = Path(root) / cfg_tag\n",
    "    cand = cfg_dir / f\"features_subject_{int(subject_raw)}__{file_tag}.npz\"\n",
    "    if cand.exists() and _saved_fooof_config_ok(cfg_dir):\n",
    "        return cand\n",
    "    # Fallback: search anywhere under saved_fooof\n",
    "    pattern = f\"features_subject_{int(subject_raw)}__{file_tag}.npz\"\n",
    "    matches = [p for p in Path(root).rglob(pattern) if p.is_file()]\n",
    "    if len(matches) == 1:\n",
    "        folder = matches[0].parent\n",
    "        if _saved_fooof_config_ok(folder):\n",
    "            return matches[0]\n",
    "    elif len(matches) > 1:\n",
    "        for p in matches:\n",
    "            if _saved_fooof_config_ok(p.parent):\n",
    "                return p\n",
    "    return None\n",
    "closed_set = set(map(str, eyes_closed_files))\n",
    "open_set = set(map(str, eyes_open_files))\n",
    "\n",
    "def infer_class_from_name(filepath: str) -> Optional[int]:\n",
    "    \"\"\"Return 1 for EC, 0 for EO based on filename or folder membership.\"\"\"\n",
    "    name = Path(filepath).name.lower()\n",
    "    if \"epoched_60epochsmarked\" in name or \"closed_marked\" in name:\n",
    "        return 1\n",
    "    if \"eyesopen_marked\" in name or \"open_marked\" in name:\n",
    "        return 0\n",
    "    if filepath in closed_set:\n",
    "        return 1\n",
    "    if filepath in open_set:\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "def parse_subject_id(filepath: str) -> int:\n",
    "    \"\"\"Extract a subject identifier from the filename.\"\"\"\n",
    "    path = Path(filepath)\n",
    "    match = re.search(r\"(\\\\|/)(\\d{5})_\", filepath)\n",
    "    if match:\n",
    "        return int(match.group(2))\n",
    "    match = re.search(r\"label[01]_(\\d+)\", path.name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    match = re.search(r\"(\\d+)\", path.stem)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return abs(hash(path.stem)) % (10 ** 7)\n",
    "\n",
    "def _load_rejmanual_vector(mat_path: str, n_epochs_expected: int) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load EEGLAB reject.rejmanual (1=reject, 0=keep).\"\"\"\n",
    "    try:\n",
    "        mat = loadmat(mat_path, struct_as_record=False, squeeze_me=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "    rej = None\n",
    "    block = mat.get(\"reject\", None)\n",
    "    if block is not None and hasattr(block, \"rejmanual\"):\n",
    "        rej = np.array(block.rejmanual)\n",
    "    if rej is None and \"EEG\" in mat:\n",
    "        EEG = mat[\"EEG\"]\n",
    "        try:\n",
    "            reject_section = getattr(EEG, \"reject\", None)\n",
    "        except Exception:\n",
    "            reject_section = None\n",
    "        if reject_section is not None:\n",
    "            if hasattr(reject_section, \"rejmanual\"):\n",
    "                rej = np.array(reject_section.rejmanual)\n",
    "            elif isinstance(reject_section, dict) and \"rejmanual\" in reject_section:\n",
    "                rej = np.array(reject_section[\"rejmanual\"])\n",
    "    if rej is None:\n",
    "        return None\n",
    "    rej = np.asarray(rej).ravel().astype(int)\n",
    "    if rej.size != n_epochs_expected:\n",
    "        warnings.warn(f\"{Path(mat_path).name}: rejmanual length {rej.size} != n_epochs {n_epochs_expected}.\")\n",
    "        return None\n",
    "    return (rej != 0).astype(int)\n",
    "\n",
    "def _psd_array_welch_clean(data: np.ndarray, sfreq: float, fmin=1.0, fmax=45.0, target_secs=2.0):\n",
    "    \"\"\"Compute PSDs with Welch while handling NaNs and short epochs.\"\"\"\n",
    "    n_epochs, _, n_times = data.shape\n",
    "    n_per_seg = max(8, min(n_times, int(round(target_secs * sfreq))))\n",
    "    n_overlap = n_per_seg // 2 if n_per_seg >= 16 else 0\n",
    "    psds, freqs = mne.time_frequency.psd_array_welch(\n",
    "        data,\n",
    "        sfreq=sfreq,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        n_per_seg=n_per_seg,\n",
    "        n_overlap=n_overlap,\n",
    "        window=\"hann\",\n",
    "        average=\"mean\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    return psds, freqs\n",
    "\n",
    "def reduce_freq_resolution(psd_cube: np.ndarray, n_bins: int) -> np.ndarray:\n",
    "    \"\"\"Mean-bin the frequency axis to reduce dimensionality.\"\"\"\n",
    "    n_samples, n_channels, n_freqs = psd_cube.shape\n",
    "    bin_size = n_freqs // n_bins\n",
    "    if bin_size == 0:\n",
    "        raise ValueError(f\"n_bins={n_bins} is too high for n_freqs={n_freqs}\")\n",
    "    trimmed = psd_cube[:, :, : bin_size * n_bins]\n",
    "    reshaped = trimmed.reshape(n_samples, n_channels, n_bins, bin_size)\n",
    "    binned = reshaped.mean(axis=3)\n",
    "    return binned\n",
    "\n",
    "\n",
    "def _select_alpha_peak(peaks: np.ndarray):\n",
    "    \"\"\"Select alpha peak in a two-stage window.\n",
    "    First search 8–14 Hz, then 4–17 Hz.\n",
    "    Returns a (CF, Amp, BW) row or None.\n",
    "    \"\"\"\n",
    "    peaks_arr = np.asarray(peaks, float)\n",
    "    if peaks_arr.size == 0:\n",
    "        return None\n",
    "    if peaks_arr.ndim == 1:\n",
    "        peaks_arr = peaks_arr.reshape(1, -1)\n",
    "    try:\n",
    "        primary_lo, primary_hi = (ALPHA_PRIMARY_RANGE if 'ALPHA_PRIMARY_RANGE' in globals() else (8.0, 14.0))\n",
    "        exp_lo, exp_hi = (ALPHA_EXPANDED_RANGE if 'ALPHA_EXPANDED_RANGE' in globals() else (4.0, 17.0))\n",
    "    except Exception:\n",
    "        primary_lo, primary_hi = 8.0, 14.0\n",
    "        exp_lo, exp_hi = 4.0, 17.0\n",
    "\n",
    "    chosen = None\n",
    "    mask = (peaks_arr[:, 0] >= primary_lo) & (peaks_arr[:, 0] <= primary_hi)\n",
    "    if np.any(mask):\n",
    "        subset = peaks_arr[mask]\n",
    "        chosen = subset[np.argmax(subset[:, 1])]\n",
    "    if chosen is None:\n",
    "        mask = (peaks_arr[:, 0] >= exp_lo) & (peaks_arr[:, 0] <= exp_hi)\n",
    "        if np.any(mask):\n",
    "            subset = peaks_arr[mask]\n",
    "            chosen = subset[np.argmax(subset[:, 1])]\n",
    "    return chosen\n",
    "\n",
    "def compute_fooof_features(freqs: np.ndarray, psd_cube: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute FOOOF/specparam features per epoch/channel.\n",
    "    Uses a two-stage alpha peak search (8–14 Hz, then 4–17 Hz).\n",
    "    Peaks outside this band are ignored; if none are found, alpha features stay at 0.\n",
    "    NaN spectra -> zeros via exception path.\n",
    "    \"\"\"\n",
    "    if SpectralModel is None:\n",
    "        raise RuntimeError('FOOOF backend unavailable; set USE_FOOOF=False.')\n",
    "    features = []\n",
    "    for epoch_psd in psd_cube:\n",
    "        epoch_feats = []\n",
    "        for spectrum in epoch_psd:\n",
    "            try:\n",
    "                if not np.all(np.isfinite(spectrum)):\n",
    "                    raise ValueError('Non-finite in spectrum')\n",
    "                model = SpectralModel(**FOOOF_SETTINGS)\n",
    "                model.fit(freqs, spectrum, freq_range=ALPHA_FREQ_RANGE)\n",
    "                offset, exponent = 0.0, 0.0\n",
    "                if hasattr(model, 'aperiodic_params_'):\n",
    "                    params = np.asarray(model.aperiodic_params_)\n",
    "                    if params.size > 0:\n",
    "                        offset = float(params[0])\n",
    "                    if params.size > 1:\n",
    "                        exponent = float(params[1])\n",
    "                alpha_cf = alpha_amp = alpha_bw = 0.0\n",
    "                peaks = np.asarray(getattr(model, 'peak_params_', []))\n",
    "                if peaks.size:\n",
    "                    chosen = _select_alpha_peak(peaks)\n",
    "                    if chosen is not None:\n",
    "                        alpha_cf, alpha_amp, alpha_bw = map(float, chosen[:3])\n",
    "                epoch_feats.extend([offset, exponent, alpha_cf, alpha_amp, alpha_bw])\n",
    "            except (FitError, RuntimeError, ValueError, np.linalg.LinAlgError):\n",
    "                epoch_feats.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        features.append(epoch_feats)\n",
    "    return np.asarray(features, dtype=float)\n",
    "\n",
    "\n",
    "def compute_one_main_fooof_features(freqs: np.ndarray, psd_cube: np.ndarray, subject_id: int, alpha_profile_map, include_aperiodic: bool = True) -> np.ndarray:\n",
    "    \"\"\"Compute features using subject-level alpha profile and per-epoch aperiodic fits.\n",
    "    For each subject, alpha center frequency and bandwidth are fixed from SUBJECT_ALPHA_PROFILE.\n",
    "    For each epoch/channel, we fit only the aperiodic component (max_n_peaks=0) and then\n",
    "    fit the amplitude of a Gaussian template at the subject alpha params to the residual.\n",
    "    Feature layout per channel: [offset, exponent, alpha_cf, alpha_amp, alpha_bw].\n",
    "    \"\"\"\n",
    "    if SpectralModel is None:\n",
    "        raise RuntimeError(\"FOOOF backend unavailable; set USE_FOOOF=False.\")\n",
    "    subj = int(subject_id)\n",
    "    profile = alpha_profile_map.get(subj) if alpha_profile_map is not None else None\n",
    "    # If no profile is available, fall back to zero alpha amplitude (still allow aperiodic if requested).\n",
    "    has_profile = profile is not None and len(profile) == 2\n",
    "    if has_profile:\n",
    "        alpha_cf, alpha_bw = map(float, profile)\n",
    "    else:\n",
    "        alpha_cf, alpha_bw = 0.0, 0.0\n",
    "    freqs_arr = np.asarray(freqs, float)\n",
    "    # Gaussian template (unit amplitude) using fooof/specparam definition: exp(-(f-cf)^2 / (2*sigma^2)), where\n",
    "    # bw ~ 2*sqrt(2*ln(2))*sigma. We invert that to get sigma from bw.\n",
    "    import math\n",
    "    if has_profile and alpha_bw > 0:\n",
    "        sigma = float(alpha_bw) / (2.0 * math.sqrt(2.0 * math.log(2.0)))\n",
    "        gauss = np.exp(-0.5 * ((freqs_arr - alpha_cf) / sigma) ** 2)\n",
    "    else:\n",
    "        gauss = np.zeros_like(freqs_arr)\n",
    "    denom = float(np.sum(gauss ** 2)) if gauss.size else 0.0\n",
    "    features = []\n",
    "    # Prepare FOOOF settings for aperiodic-only fits\n",
    "    ap_settings = dict(FOOOF_SETTINGS)\n",
    "    try:\n",
    "        ap_settings[\"max_n_peaks\"] = 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    for epoch_psd in psd_cube:\n",
    "        epoch_feats = []\n",
    "        for spectrum in epoch_psd:\n",
    "            try:\n",
    "                if not np.all(np.isfinite(spectrum)):\n",
    "                    raise ValueError(\"Non-finite in spectrum\")\n",
    "                offset, exponent = 0.0, 0.0\n",
    "                alpha_amp = 0.0\n",
    "                if include_aperiodic or has_profile:\n",
    "                    model = SpectralModel(**ap_settings)\n",
    "                    model.fit(freqs_arr, spectrum, freq_range=ALPHA_FREQ_RANGE)\n",
    "                    if hasattr(model, \"aperiodic_params_\"):\n",
    "                        params = np.asarray(model.aperiodic_params_)\n",
    "                        if params.size > 0:\n",
    "                            offset = float(params[0])\n",
    "                        if params.size > 1:\n",
    "                            exponent = float(params[1])\n",
    "                    # reconstruct aperiodic-only spectrum (log10 power)\n",
    "                    try:\n",
    "                        # specparam offers get_model_spectrum with no peaks if max_n_peaks=0\n",
    "                        ap_fit = None\n",
    "                        get_fun = getattr(model, \"get_model_spectrum\", None)\n",
    "                        if callable(get_fun):\n",
    "                            ap_fit = np.asarray(get_fun(freqs_arr))\n",
    "                    except Exception:\n",
    "                        ap_fit = None\n",
    "                    if ap_fit is None:\n",
    "                        # fallback: use modeled spectrum attribute if available\n",
    "                        for name in (\"fooofed_spectrum_\", \"modeled_spectrum_\", \"model_spectrum_\", \"model_spectrum__\"):\n",
    "                            if hasattr(model, name):\n",
    "                                ap_fit = np.asarray(getattr(model, name))\n",
    "                                break\n",
    "                    if ap_fit is None or ap_fit.shape != spectrum.shape:\n",
    "                        ap_fit = np.zeros_like(spectrum)\n",
    "                    if has_profile and denom > 0.0:\n",
    "                        # Assume spectrum & ap_fit are in the same (log10) scale; fit template amplitude to residual\n",
    "                        residual = spectrum - ap_fit\n",
    "                        num = float(np.sum(gauss * residual))\n",
    "                        alpha_amp = max(num / denom, 0.0)\n",
    "                epoch_feats.extend([offset, exponent, alpha_cf if has_profile else 0.0, alpha_amp, alpha_bw if has_profile else 0.0])\n",
    "            except (FitError, RuntimeError, ValueError, np.linalg.LinAlgError):\n",
    "                epoch_feats.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        features.append(epoch_feats)\n",
    "    return np.asarray(features, dtype=float)\n",
    "\n",
    "def _canonical_eeg_channel_name(ch_name: str) -> str:\n",
    "    name = str(ch_name).strip()\n",
    "    name = re.sub(r\"^EEG\\s+\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"-REF$\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \"\", name)\n",
    "    return name\n",
    "\n",
    "def _rename_epochs_channels_canonical(epochs):\n",
    "    new_names = [_canonical_eeg_channel_name(ch) for ch in epochs.ch_names]\n",
    "    if len(set(new_names)) != len(new_names):\n",
    "        warnings.warn(\"Canonical channel renaming would create duplicate names; keeping original channel names.\")\n",
    "        return epochs\n",
    "    mapping = {old: new for old, new in zip(epochs.ch_names, new_names) if old != new}\n",
    "    if mapping:\n",
    "        epochs.rename_channels(mapping)\n",
    "    return epochs\n",
    "\n",
    "def _labels_from_epochs_events(epochs) -> np.ndarray:\n",
    "    code_to_name = {int(v): str(k).upper() for k, v in epochs.event_id.items()}\n",
    "    labels = np.full(len(epochs), -1, dtype=int)\n",
    "    for i, code in enumerate(epochs.events[:, 2].astype(int)):\n",
    "        name = code_to_name.get(int(code), \"\")\n",
    "        if name.startswith(\"EO\"):\n",
    "            labels[i] = 0\n",
    "        elif name.startswith(\"EC\"):\n",
    "            labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "# -------------------- MAIN EXTRACTION LOOP --------------------\n",
    "records = []\n",
    "psd_aligned = []\n",
    "class_epoch_found = Counter()\n",
    "class_epoch_kept = Counter()\n",
    "dropped_due_to_nonfinite = 0\n",
    "paired_epochs_created = 0\n",
    "paired_epochs_dropped = 0\n",
    "freq_reference = None\n",
    "# Track per-subject original epoch positions across files\n",
    "subject_epoch_cursor = {}\n",
    "\n",
    "conflict_rows = []\n",
    "conflict_log_path = None\n",
    "if 'need_new' in globals() and need_new:\n",
    "    try:\n",
    "        conflict_dir = Path(new_processed_dir) if 'new_processed_dir' in globals() and new_processed_dir else None\n",
    "    except Exception:\n",
    "        conflict_dir = None\n",
    "    if conflict_dir is None:\n",
    "        conflict_dir = Path.cwd()\n",
    "    conflict_log_path = outpath('doctor_label_conflicts.csv')\n",
    "\n",
    "items = []\n",
    "use_offset = bool(CROSS_DATASET_TEST or USE_BOTH_DATASETS)\n",
    "if 'need_new' in globals() and need_new:\n",
    "    items.extend([(\"new\", sid, fa, fb) for (sid, fa, fb) in new_subject_pairs])\n",
    "    if not new_subject_pairs:\n",
    "        warnings.warn('need_new=True but no NEW subject files were found.')\n",
    "if 'need_old' in globals() and need_old:\n",
    "    items.extend([(\"old\", None, f, None) for f in set_files])\n",
    "    if not set_files:\n",
    "        warnings.warn('need_old=True but no OLD .set files were found.')\n",
    "if not items:\n",
    "    raise RuntimeError('No input files found for the selected dataset mode. Check paths and toggles.')\n",
    "\n",
    "for dataset_tag, subj_hint, file_a, file_b in items:\n",
    "    is_new = (dataset_tag == 'new')\n",
    "    path = Path(file_a)\n",
    "    if not path.exists():\n",
    "        warnings.warn(f'Missing file: {path}')\n",
    "        continue\n",
    "\n",
    "    file_level_label = None\n",
    "    path_b = None\n",
    "    subject_id_raw = None\n",
    "    if is_new:\n",
    "        try:\n",
    "            epochs = mne.read_epochs(str(path), preload=False, verbose='ERROR')\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f'Failed to read NEW FIF epochs for subject {subj_hint}: {exc}')\n",
    "            continue\n",
    "        epochs = _rename_epochs_channels_canonical(epochs)\n",
    "        labels_a = _labels_from_epochs_events(epochs)\n",
    "        # Optional second rater file (doctor b). If missing, proceed with labels from the single file.\n",
    "        path_b = Path(file_b) if file_b else None\n",
    "        labels_all = labels_a\n",
    "        if path_b is not None:\n",
    "            if not path_b.exists():\n",
    "                warnings.warn(f'Missing paired file (b): {path_b}. Proceeding with single NEW file: {path.name}.')\n",
    "                path_b = None\n",
    "            else:\n",
    "                try:\n",
    "                    epochs_b = mne.read_epochs(str(path_b), preload=False, verbose='ERROR')\n",
    "                    labels_b = _labels_from_epochs_events(epochs_b)\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f'Failed to read NEW paired FIF epochs for subject {subj_hint}: {path_b.name}: {exc}. Proceeding with single file.')\n",
    "                    labels_b = None\n",
    "                    path_b = None\n",
    "                if labels_b is not None:\n",
    "                    if labels_a.shape != labels_b.shape:\n",
    "                        warnings.warn(\n",
    "                            f'Label length mismatch for subject {subj_hint}: {path.name} ({labels_a.size}) vs {path_b.name} ({labels_b.size}). Skipping.'\n",
    "                        )\n",
    "                        continue\n",
    "                    union_labels = labels_a.copy()\n",
    "                    take_from_b = (union_labels < 0)\n",
    "                    union_labels[take_from_b] = labels_b[take_from_b]\n",
    "                    conflict_mask = (labels_a >= 0) & (labels_b >= 0) & (labels_a != labels_b)\n",
    "                    if np.any(conflict_mask):\n",
    "                        sid = int(subj_hint) if subj_hint is not None else int(parse_subject_id(str(path)))\n",
    "                        for ep_idx in np.flatnonzero(conflict_mask):\n",
    "                            conflict_rows.append({\n",
    "                                'subject_id': int(sid),\n",
    "                                'epoch_index': int(ep_idx),\n",
    "                                'label_a': int(labels_a[ep_idx]),\n",
    "                                'label_b': int(labels_b[ep_idx]),\n",
    "                                'file_a': str(path),\n",
    "                                'file_b': str(path_b),\n",
    "                            })\n",
    "                    union_labels[conflict_mask] = -1\n",
    "                    labels_all = union_labels\n",
    "\n",
    "        keep_mask_labels = (labels_all >= 0)\n",
    "        subject_id_raw = int(subj_hint) if subj_hint is not None else int(parse_subject_id(str(path)))\n",
    "    else:\n",
    "        label = infer_class_from_name(str(path))\n",
    "        if label is None:\n",
    "            warnings.warn(f'Cannot infer EC/EO label for {path.name}. Skipping.')\n",
    "            continue\n",
    "        file_level_label = int(label)\n",
    "        try:\n",
    "            epochs = mne.io.read_epochs_eeglab(str(path), verbose='ERROR')\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f'Failed to read {path.name}: {exc}')\n",
    "            continue\n",
    "        labels_all = np.full(len(epochs), int(label), dtype=int)\n",
    "        rej = _load_rejmanual_vector(str(path), n_epochs_expected=len(epochs))\n",
    "        if rej is None:\n",
    "            warnings.warn(f'{path.name}: no valid rejmanual vector. Skipping.')\n",
    "            continue\n",
    "        keep_mask_labels = (rej == 0)\n",
    "        if not np.any(keep_mask_labels):\n",
    "            warnings.warn(f'{path.name}: no epochs marked as keep (rejmanual==0). Skipping.')\n",
    "            continue\n",
    "        subject_id_raw = int(parse_subject_id(str(path)))\n",
    "\n",
    "    # Use a disambiguated subject_id when mixing datasets (prevents collisions across NEW vs OLD).\n",
    "    subject_id = int(subject_id_raw)\n",
    "    if use_offset and (not is_new):\n",
    "        subject_id = int(subject_id_raw) + int(DATASET_SUBJECT_OFFSET)\n",
    "\n",
    "    channels_info = epochs.info['chs']\n",
    "\n",
    "    if ALL_CHANNELS:\n",
    "        picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude='bads')\n",
    "        if len(picks_all) == 0:\n",
    "            picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=[])\n",
    "        available_channels = [epochs.ch_names[idx] for idx in picks_all]\n",
    "        picks = picks_all\n",
    "    else:\n",
    "        requested = CHANNEL_SELECTION\n",
    "        requested_upper = [ch.upper() for ch in requested]\n",
    "        name_lookup = {ch_info['ch_name'].upper(): ch_info['ch_name'] for ch_info in channels_info}\n",
    "        missing = [ch for ch in requested_upper if ch not in name_lookup]\n",
    "        if missing:\n",
    "            print(f'{path.name}: missing requested channels {missing}. Available channel names:')\n",
    "            for ch in channels_info:\n",
    "                print(f\"  - {ch['ch_name']}\")\n",
    "            continue\n",
    "        available_channels = [name_lookup[ch] for ch in requested_upper]\n",
    "        picks = [epochs.ch_names.index(ch) for ch in available_channels]\n",
    "\n",
    "    if not available_channels:\n",
    "        warnings.warn(f'{path.name}: no EEG channels matched the selection. Skipping.')\n",
    "        continue\n",
    "\n",
    "    data = epochs.get_data(picks=picks)\n",
    "    n_epochs, _, _ = data.shape\n",
    "    sfreq = float(epochs.info['sfreq'])\n",
    "    if labels_all.shape[0] != n_epochs or keep_mask_labels.shape[0] != n_epochs:\n",
    "        warnings.warn(f'{path.name}: label/mask length mismatch (n_epochs={n_epochs}). Skipping.')\n",
    "        continue\n",
    "\n",
    "    for cls in (0, 1):\n",
    "        class_epoch_found[cls] += int(np.sum(labels_all == cls))\n",
    "\n",
    "    finite_mask = np.all(np.isfinite(data), axis=(1, 2))\n",
    "    if not np.all(finite_mask):\n",
    "        dropped = int(np.sum(~finite_mask))\n",
    "        dropped_due_to_nonfinite += dropped\n",
    "        warnings.warn(f'{path.name}: dropping {dropped} epochs containing NaN/Inf.')\n",
    "    keep_mask = keep_mask_labels & finite_mask\n",
    "    if not np.any(keep_mask):\n",
    "        warnings.warn(f'{path.name}: no usable epochs after keep-mask and NaN/Inf filtering. Skipping.')\n",
    "        continue\n",
    "\n",
    "    kept_indices = np.flatnonzero(keep_mask)\n",
    "    kept_labels = labels_all[keep_mask].astype(int)\n",
    "    if COMBINE_ADJACENT_EPOCHS:\n",
    "        grouped_pairs = []\n",
    "        pair_labels = []\n",
    "        cursor = 0\n",
    "        while cursor < len(kept_indices):\n",
    "            run_end = cursor + 1\n",
    "            while (\n",
    "                run_end < len(kept_indices)\n",
    "                and kept_indices[run_end] == kept_indices[run_end - 1] + 1\n",
    "                and kept_labels[run_end] == kept_labels[run_end - 1]\n",
    "            ):\n",
    "                run_end += 1\n",
    "            run = kept_indices[cursor:run_end]\n",
    "            run_label = int(kept_labels[cursor])\n",
    "            n_pairs = len(run) // 2\n",
    "            for pair_idx in range(n_pairs):\n",
    "                grouped_pairs.append((run[2 * pair_idx], run[2 * pair_idx + 1]))\n",
    "                pair_labels.append(run_label)\n",
    "            if len(run) % 2 == 1:\n",
    "                paired_epochs_dropped += 1\n",
    "            cursor = run_end\n",
    "        if not grouped_pairs:\n",
    "            warnings.warn(f'{path.name}: no adjacent same-label kept epochs available for pairing. Skipping file.')\n",
    "            continue\n",
    "        combined = [np.concatenate([data[i0], data[i1]], axis=1) for i0, i1 in grouped_pairs]\n",
    "        data_kept = np.stack(combined, axis=0)\n",
    "        epoch_local_indices = np.array([min(i0, i1) for i0, i1 in grouped_pairs], dtype=int)\n",
    "        labels_kept = np.asarray(pair_labels, dtype=int)\n",
    "        paired_epochs_created += len(grouped_pairs)\n",
    "    else:\n",
    "        data_kept = data[keep_mask]\n",
    "        epoch_local_indices = kept_indices.astype(int)\n",
    "        labels_kept = kept_labels.astype(int)\n",
    "\n",
    "    for cls in (0, 1):\n",
    "        class_epoch_kept[cls] += int(np.sum(labels_kept == cls))\n",
    "\n",
    "    target_secs = 1.0 if data_kept.shape[-1] < sfreq * 3 else 2.0\n",
    "    try:\n",
    "        psd_data, freqs = _psd_array_welch_clean(\n",
    "            data_kept,\n",
    "            sfreq=sfreq,\n",
    "            fmin=PSD_KWARGS.get('fmin', 1.0),\n",
    "            fmax=PSD_KWARGS.get('fmax', 45.0),\n",
    "            target_secs=target_secs,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f'PSD computation failed for {path.name}: {exc}')\n",
    "        continue\n",
    "\n",
    "    if freq_reference is None:\n",
    "        freq_reference = freqs\n",
    "    else:\n",
    "        if freqs.shape != freq_reference.shape or not np.allclose(freqs, freq_reference):\n",
    "            raise RuntimeError(f'Frequency axis mismatch in {path.name} compared to earlier files.')\n",
    "\n",
    "    # For ONE_MAIN_FOOOF, optionally build the per-subject alpha profile from ALL epochs (to mimic unseen data).\n",
    "    # This does not change which epochs are used for supervised training (labels_kept still comes from keep_mask).\n",
    "    psd_profile = psd_data\n",
    "    if USE_FOOOF and ONE_MAIN_FOOOF and MAIN_FOOOF_USE_ALL_EPOCHS and SpectralModel is not None:\n",
    "        profile_mask = finite_mask  # ignore keep_mask_labels (labels/rejmanual) for alpha-profile PSD\n",
    "        if not np.any(profile_mask):\n",
    "            warnings.warn(f'{path.name}: no finite epochs available for MAIN_FOOOF_USE_ALL_EPOCHS; using labeled/kept epochs instead.')\n",
    "        else:\n",
    "            data_profile = None\n",
    "            if COMBINE_ADJACENT_EPOCHS:\n",
    "                # Pair consecutive finite epochs regardless of label (labels may be missing/unknown).\n",
    "                profile_indices = np.flatnonzero(profile_mask)\n",
    "                profile_pairs = []\n",
    "                cursor = 0\n",
    "                while cursor < len(profile_indices) - 1:\n",
    "                    i0 = int(profile_indices[cursor])\n",
    "                    i1 = int(profile_indices[cursor + 1])\n",
    "                    if i1 == i0 + 1:\n",
    "                        profile_pairs.append((i0, i1))\n",
    "                        cursor += 2\n",
    "                    else:\n",
    "                        cursor += 1\n",
    "                if profile_pairs:\n",
    "                    combined_profile = [np.concatenate([data[i0], data[i1]], axis=1) for i0, i1 in profile_pairs]\n",
    "                    data_profile = np.stack(combined_profile, axis=0)\n",
    "                else:\n",
    "                    warnings.warn(f'{path.name}: no adjacent finite epochs available for pairing; using labeled/kept epochs for main-FOOOF profile.')\n",
    "            else:\n",
    "                data_profile = data[profile_mask]\n",
    "\n",
    "            if data_profile is not None and data_profile.shape[0] > 0:\n",
    "                try:\n",
    "                    psd_profile, freqs_profile = _psd_array_welch_clean(\n",
    "                        data_profile,\n",
    "                        sfreq=sfreq,\n",
    "                        fmin=PSD_KWARGS.get('fmin', 1.0),\n",
    "                        fmax=PSD_KWARGS.get('fmax', 45.0),\n",
    "                        target_secs=target_secs,\n",
    "                    )\n",
    "                    if freqs_profile.shape != freqs.shape or not np.allclose(freqs_profile, freqs):\n",
    "                        raise RuntimeError('Frequency axis mismatch for main-FOOOF profile PSD.')\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f'Main-FOOOF profile PSD computation failed for {path.name}: {exc}; using labeled/kept epochs instead.')\n",
    "                    psd_profile = psd_data\n",
    "\n",
    "    # Optional: use precomputed ONE_MAIN_FOOOF features (skip per-epoch SpectralModel.fit)\n",
    "    saved_fooof_npz = None\n",
    "    if (\n",
    "        'USE_SAVED_FOOOF' in globals() and USE_SAVED_FOOOF\n",
    "        and USE_FOOOF and ONE_MAIN_FOOOF\n",
    "        and (not COMBINE_ADJACENT_EPOCHS)\n",
    "    ):\n",
    "        try:\n",
    "            cand = _find_saved_fooof_npz(str(path), subject_raw=int(subject_id_raw))\n",
    "            if cand is not None:\n",
    "                saved_fooof_npz = str(cand)\n",
    "        except Exception:\n",
    "            saved_fooof_npz = None\n",
    "\n",
    "    if TIME_AXIS_MODE == 'append_files':\n",
    "        offset = subject_epoch_cursor.get(subject_id, 0)\n",
    "        time_indices = offset + epoch_local_indices\n",
    "        subject_epoch_cursor[subject_id] = offset + n_epochs\n",
    "    elif TIME_AXIS_MODE == 'align_conditions':\n",
    "        time_indices = epoch_local_indices\n",
    "    elif TIME_AXIS_MODE == 'interleave_conditions':\n",
    "        time_indices = 2 * epoch_local_indices + labels_kept\n",
    "    else:\n",
    "        raise ValueError(f'Unknown TIME_AXIS_MODE: {TIME_AXIS_MODE!r}')\n",
    "\n",
    "    records.append(dict(\n",
    "        file=str(path),\n",
    "        file_b=str(path_b) if (is_new and path_b is not None) else None,\n",
    "        label=file_level_label,\n",
    "        labels=labels_kept.tolist(),\n",
    "        dataset=str(dataset_tag),\n",
    "        subject_raw=int(subject_id_raw),\n",
    "        subject=subject_id,\n",
    "        finite_mask=np.asarray(finite_mask, dtype=bool).tolist(),\n",
    "        keep_mask_labels=np.asarray(keep_mask_labels, dtype=bool).tolist(),\n",
    "        saved_fooof_npz=saved_fooof_npz,\n",
    "        channels=available_channels,\n",
    "        psd=psd_data,\n",
    "        psd_profile=psd_profile,\n",
    "        freqs=freqs,\n",
    "        total_epochs=n_epochs,\n",
    "        kept_epochs=data_kept.shape[0],\n",
    "        epoch_time_indices=np.asarray(time_indices, dtype=int).tolist(),\n",
    "    ))\n",
    "\n",
    "    if is_new:\n",
    "        cnt_eo = int(np.sum(labels_kept == 0))\n",
    "        cnt_ec = int(np.sum(labels_kept == 1))\n",
    "        print(f'[OK] {path.name}: kept {data_kept.shape[0]}/{n_epochs} epochs (EO={cnt_eo}, EC={cnt_ec})')\n",
    "    else:\n",
    "        status = 'EC' if file_level_label == 1 else 'EO'\n",
    "        print(f'[OK] {path.name}: kept {data_kept.shape[0]}/{n_epochs} epochs (class={status})')\n",
    "\n",
    "if ('need_new' in globals() and need_new) and conflict_log_path is not None and conflict_rows:\n",
    "    fieldnames = ['subject_id', 'epoch_index', 'label_a', 'label_b', 'file_a', 'file_b']\n",
    "    try:\n",
    "        with conflict_log_path.open('w', newline='') as f:\n",
    "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            w.writeheader()\n",
    "            w.writerows(conflict_rows)\n",
    "        print(f'Wrote conflict log: {conflict_log_path} ({len(conflict_rows)} conflicts)')\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f'Failed to write conflict log to {conflict_log_path}: {exc}')\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\"No feature vectors extracted. Check channel selection, FOOOF availability, or data quality.\")\n",
    "\n",
    "# -------------------- CHANNEL RECONCILIATION (UNION IF ALL) --------------------\n",
    "if ALL_CHANNELS:\n",
    "    # Build UNION of channels across all files, preserving first-seen order\n",
    "    union_upper_to_name = OrderedDict()\n",
    "    for rec in records:\n",
    "        for ch in rec['channels']:\n",
    "            key = ch.upper()\n",
    "            if key not in union_upper_to_name:\n",
    "                union_upper_to_name[key] = ch\n",
    "    feature_channels = list(union_upper_to_name.values())\n",
    "else:\n",
    "    requested_upper = [ch.upper() for ch in CHANNEL_SELECTION]\n",
    "    feature_channels = []\n",
    "    missing_global = []\n",
    "    for req in requested_upper:\n",
    "        actual = None\n",
    "        for rec in records:\n",
    "            mapping = {ch.upper(): ch for ch in rec['channels']}\n",
    "            if req in mapping:\n",
    "                actual = mapping[req]\n",
    "                break\n",
    "        if actual is None:\n",
    "            missing_global.append(req)\n",
    "        else:\n",
    "            feature_channels.append(actual)\n",
    "    if missing_global:\n",
    "        raise RuntimeError(f\"Requested channels not found in the available data: {missing_global}\")\n",
    "\n",
    "if not feature_channels:\n",
    "    raise RuntimeError(\"Resolved feature channel list is empty. Adjust CHANNEL_SELECTION.\")\n",
    "\n",
    "# -------------------- Channel sanity summary --------------------\n",
    "def _union_channels_for_dataset(ds: str) -> list:\n",
    "    union = OrderedDict()\n",
    "    for rec in records:\n",
    "        if str(rec.get(\"dataset\")) != str(ds):\n",
    "            continue\n",
    "        for ch in rec.get(\"channels\", []):\n",
    "            key = _canonical_channel_name(ch).upper()\n",
    "            if key not in union:\n",
    "                union[key] = ch\n",
    "    return list(union.values())\n",
    "\n",
    "new_feature_channels = _union_channels_for_dataset(\"new\")\n",
    "old_feature_channels = _union_channels_for_dataset(\"old\")\n",
    "if new_feature_channels:\n",
    "    print(f\"NEW dataset channel summary: {len(new_feature_channels)} channels -> {new_feature_channels}\")\n",
    "    std_upper = [c.upper() for c in STANDARD_19_CHANNELS] if \"STANDARD_19_CHANNELS\" in globals() else []\n",
    "    new_upper = [str(c).upper() for c in new_feature_channels]\n",
    "    missing_std = [c for c in (STANDARD_19_CHANNELS if \"STANDARD_19_CHANNELS\" in globals() else []) if c.upper() not in new_upper]\n",
    "    extra = [c for c in new_feature_channels if c.upper() not in std_upper] if std_upper else []\n",
    "    if missing_std:\n",
    "        print(\"NEW dataset missing STANDARD_19_CHANNELS:\", missing_std)\n",
    "    if extra:\n",
    "        print(\"NEW dataset extra channels vs STANDARD_19_CHANNELS:\", extra)\n",
    "    if \"FAIL_IF_NEW_CHANNEL_MISMATCH\" in globals() and FAIL_IF_NEW_CHANNEL_MISMATCH:\n",
    "        if \"EXPECTED_NEW_CHANNEL_COUNT\" in globals() and len(new_feature_channels) != int(EXPECTED_NEW_CHANNEL_COUNT):\n",
    "            warnings.warn(f\"NEW dataset channel count is {len(new_feature_channels)} (expected {EXPECTED_NEW_CHANNEL_COUNT}). This will change feature dimensionality.\")\n",
    "\n",
    "    if \"FORCE_STANDARD_19_FOR_NEW\" in globals() and FORCE_STANDARD_19_FOR_NEW:\n",
    "        # Only safe when training NEW dataset alone; forces a stable feature axis.\n",
    "        if missing_std:\n",
    "            raise RuntimeError(f\"FORCE_STANDARD_19_FOR_NEW=True but missing channels: {missing_std}\")\n",
    "        # Force global feature_channels to the standard order (NEW dataset channels assumed canonical already).\n",
    "        feature_channels = list(STANDARD_19_CHANNELS)\n",
    "        print(\"FORCE_STANDARD_19_FOR_NEW applied: feature_channels forced to STANDARD_19_CHANNELS\")\n",
    "\n",
    "if old_feature_channels:\n",
    "    print(f\"OLD dataset channel summary: {len(old_feature_channels)} channels -> {old_feature_channels}\")\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# -------------------- ALIGN TO FEATURE CHANNELS + BUILD FEATURES --------------------\n",
    "psd_freqs = freq_reference\n",
    "psd_aligned = []          # features: aligned PSDs for labeled/kept epochs\n",
    "psd_aligned_profile = []  # profile: aligned PSDs used to build SUBJECT_ALPHA_PROFILE\n",
    "\n",
    "missing_report = []\n",
    "for rec in records:\n",
    "    name_to_idx = {ch: idx for idx, ch in enumerate(rec['channels'])}\n",
    "    pick_idx = [name_to_idx.get(ch, None) for ch in feature_channels]\n",
    "\n",
    "    psd_feat = rec['psd']\n",
    "    psd_prof = rec.get('psd_profile', psd_feat)\n",
    "\n",
    "    n_epochs_here = psd_feat.shape[0]\n",
    "    n_freqs_here = psd_feat.shape[2]\n",
    "\n",
    "    aligned = np.full((n_epochs_here, len(feature_channels), n_freqs_here), np.nan, dtype=float)\n",
    "\n",
    "    n_epochs_prof = psd_prof.shape[0]\n",
    "    aligned_prof = np.full((n_epochs_prof, len(feature_channels), n_freqs_here), np.nan, dtype=float)\n",
    "\n",
    "    have_pairs = [(out_i, in_i) for out_i, in_i in enumerate(pick_idx) if in_i is not None]\n",
    "    if have_pairs:\n",
    "        out_idx, in_idx = zip(*have_pairs)\n",
    "        aligned[:, list(out_idx), :] = psd_feat[:, list(in_idx), :]\n",
    "        aligned_prof[:, list(out_idx), :] = psd_prof[:, list(in_idx), :]\n",
    "\n",
    "    n_missing = sum(1 for x in pick_idx if x is None)\n",
    "    if n_missing:\n",
    "        missing_report.append((Path(rec['file']).name, n_missing))\n",
    "\n",
    "    psd_aligned.append(aligned)\n",
    "    psd_aligned_profile.append(aligned_prof)\n",
    "\n",
    "# -------------------- SUBJECT ALPHA PROFILE ESTIMATION (per subject, from EC epochs and posterior ROI) --------------------\n",
    "SUBJECT_ALPHA_PROFILE = {}\n",
    "if USE_FOOOF and ONE_MAIN_FOOOF and SpectralModel is not None:\n",
    "    # Build subject-level alpha profiles by averaging ROI spectra and running FOOOF once per subject.\n",
    "    # We reuse feature_channels (already aligned with psd_aligned).\n",
    "    roi_names_profile = [ch for ch in ALPHA_PROFILE_ROI if ch in feature_channels]\n",
    "    roi_idx_profile = [feature_channels.index(ch) for ch in roi_names_profile]\n",
    "    if roi_idx_profile:\n",
    "        per_subject_psds = {}\n",
    "        for rec, aligned in zip(records, psd_aligned_profile):\n",
    "            subj = int(rec['subject'])\n",
    "            # aligned: (n_epochs, n_channels, n_freqs); take ROI channels\n",
    "            roi_cube = aligned[:, roi_idx_profile, :]\n",
    "            lst = per_subject_psds.setdefault(subj, [])\n",
    "            lst.append(roi_cube)\n",
    "        for subj, cubes in per_subject_psds.items():\n",
    "            try:\n",
    "                data = np.concatenate(cubes, axis=0)  # (epochs_total, n_roi, n_freqs)\n",
    "                mean_spectrum = data.mean(axis=(0, 1))\n",
    "                if not np.any(np.isfinite(mean_spectrum)):\n",
    "                    continue\n",
    "                # FOOOF model for alpha profile\n",
    "                model = SpectralModel(**FOOOF_SETTINGS)\n",
    "                lo, hi = ALPHA_PROFILE_RANGE\n",
    "                model.fit(psd_freqs, mean_spectrum, freq_range=(lo, hi))\n",
    "                peaks = np.asarray(getattr(model, 'peak_params_', []))\n",
    "                if peaks.size:\n",
    "                    # pick strongest peak in ALPHA_PROFILE_RANGE\n",
    "                    mask = (peaks[:, 0] >= lo) & (peaks[:, 0] <= hi)\n",
    "                    if np.any(mask):\n",
    "                        subset = peaks[mask]\n",
    "                        best = subset[np.argmax(subset[:, 1])]\n",
    "                        cf, amp, bw = map(float, best[:3])\n",
    "                        SUBJECT_ALPHA_PROFILE[subj] = (cf, bw)\n",
    "            except Exception:\n",
    "                continue\n",
    "    print('Built alpha profiles for', len(SUBJECT_ALPHA_PROFILE), 'subject(s); ROI channels:', (roi_names_profile if roi_idx_profile else 'None found'))\n",
    "else:\n",
    "    SUBJECT_ALPHA_PROFILE = {}\n",
    "\n",
    "def _load_saved_one_main_fooof_features(rec: dict, feature_channels: list, psd_freqs: np.ndarray) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load cached ONE_MAIN_FOOOF features and expand to full 5-feature layout.\n",
    "\n",
    "    Returns an array shaped (n_epochs, len(feature_channels) * 5) or None.\n",
    "    \"\"\"\n",
    "    if not ('USE_SAVED_FOOOF' in globals() and USE_SAVED_FOOOF):\n",
    "        return None\n",
    "    if not (USE_FOOOF and ONE_MAIN_FOOOF):\n",
    "        return None\n",
    "    if COMBINE_ADJACENT_EPOCHS:\n",
    "        # Precompute notebook pairs finite epochs; classifier pairs kept same-label runs.\n",
    "        return None\n",
    "    npz_path = rec.get('saved_fooof_npz', None)\n",
    "    if not npz_path:\n",
    "        return None\n",
    "    npz_path = Path(str(npz_path))\n",
    "    if not npz_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        d = np.load(npz_path, allow_pickle=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if 'X' not in d or 'feature_names' not in d or 'freqs' not in d:\n",
    "        return None\n",
    "    freqs_saved = np.asarray(d['freqs'], float).ravel()\n",
    "    freqs_here = np.asarray(psd_freqs, float).ravel()\n",
    "    if freqs_saved.shape != freqs_here.shape or not np.allclose(freqs_saved, freqs_here):\n",
    "        return None\n",
    "    X_saved = np.asarray(d['X'])\n",
    "    feat_names = [str(x) for x in np.asarray(d['feature_names']).ravel().tolist()]\n",
    "\n",
    "    # Cache is computed on finite epochs only; subset to the same kept epochs used here.\n",
    "    finite_mask = np.asarray(rec.get('finite_mask', []), dtype=bool)\n",
    "    keep_mask_labels = np.asarray(rec.get('keep_mask_labels', []), dtype=bool)\n",
    "    if finite_mask.size and keep_mask_labels.size and finite_mask.shape == keep_mask_labels.shape:\n",
    "        keep_finite = keep_mask_labels[finite_mask]\n",
    "        if keep_finite.shape[0] != X_saved.shape[0]:\n",
    "            return None\n",
    "        X_saved = X_saved[keep_finite]\n",
    "\n",
    "    # Ensure epoch count matches labels\n",
    "    n_labels = len(rec.get('labels', []))\n",
    "    if n_labels and X_saved.shape[0] != n_labels:\n",
    "        return None\n",
    "\n",
    "    base_order = ['offset', 'exponent', 'alpha_cf', 'alpha_amp', 'alpha_bw']\n",
    "    try:\n",
    "        needed = [f for f in list(FOOOF_SELECTED_FEATURES) if f in base_order]\n",
    "    except Exception:\n",
    "        needed = base_order\n",
    "    if not needed:\n",
    "        needed = base_order\n",
    "    available_feats = set()\n",
    "    mapping = {}\n",
    "    for idx, name in enumerate(feat_names):\n",
    "        try:\n",
    "            ch_part, feat = str(name).rsplit('_', 1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        feat = str(feat)\n",
    "        if feat not in base_order:\n",
    "            continue\n",
    "        ch_key = _canonical_channel_name(ch_part).upper()\n",
    "        mapping[(ch_key, feat)] = int(idx)\n",
    "        available_feats.add(feat)\n",
    "    if not set(needed).issubset(available_feats):\n",
    "        return None\n",
    "\n",
    "    # Expand into the full 5-feature-per-channel layout expected by the classifier notebook.\n",
    "    out = np.zeros((int(X_saved.shape[0]), int(len(feature_channels) * len(base_order))), dtype=float)\n",
    "    for ch_i, ch in enumerate(feature_channels):\n",
    "        ch_key = _canonical_channel_name(ch).upper()\n",
    "        base_col = ch_i * len(base_order)\n",
    "        for off, feat in enumerate(base_order):\n",
    "            src = mapping.get((ch_key, feat), None)\n",
    "            if src is None:\n",
    "                continue\n",
    "            out[:, base_col + off] = X_saved[:, src]\n",
    "    return np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Build features (FOOOF or flattened PSD)\n",
    "X_list, y_list, subject_ids, epoch_time_list = [], [], [], []\n",
    "for rec, aligned in zip(records, psd_aligned):\n",
    "    feats = None\n",
    "    if USE_FOOOF and ONE_MAIN_FOOOF:\n",
    "        feats = _load_saved_one_main_fooof_features(rec, feature_channels=feature_channels, psd_freqs=psd_freqs)\n",
    "        if feats is not None:\n",
    "            try:\n",
    "                print(f\"[SAVED_FOOOF] {Path(str(rec.get('file',''))).name} -> {Path(str(rec.get('saved_fooof_npz'))).name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    if feats is None:\n",
    "        if USE_FOOOF:\n",
    "            if ONE_MAIN_FOOOF:\n",
    "                feats = compute_one_main_fooof_features(psd_freqs, aligned, subject_id=int(rec['subject']), alpha_profile_map=SUBJECT_ALPHA_PROFILE)\n",
    "            else:\n",
    "                feats = compute_fooof_features(psd_freqs, aligned)\n",
    "        else:\n",
    "            flat = aligned.reshape(aligned.shape[0], -1)\n",
    "            feats = np.nan_to_num(flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_list.append(feats)\n",
    "    labels = np.asarray(rec.get('labels', []), dtype=int)\n",
    "    if labels.shape[0] != feats.shape[0]:\n",
    "        labels = np.full(feats.shape[0], int(rec.get('label', -1)), dtype=int)\n",
    "    y_list.append(labels)\n",
    "    subject_ids.extend([rec['subject']] * feats.shape[0])\n",
    "    epoch_time_list.append(np.asarray(rec['epoch_time_indices'], dtype=int))\n",
    "\n",
    "psd_cube = np.concatenate(psd_aligned, axis=0)\n",
    "X_combined = np.vstack(X_list).astype(float)\n",
    "y_combined = np.concatenate(y_list).astype(int)\n",
    "subject_ids = np.array(subject_ids, dtype=int)\n",
    "epoch_time_indices = np.concatenate(epoch_time_list).astype(int)\n",
    "\n",
    "# -------------------- REPORT --------------------\n",
    "print(f\"Features collected: {X_combined.shape[0]} epochs × {X_combined.shape[1]} features.\")\n",
    "class_names = {0: \"EO\", 1: \"EC\"}\n",
    "counts = {class_names.get(int(cls), str(cls)): int(cnt) for cls, cnt in zip(*np.unique(y_combined, return_counts=True))}\n",
    "print(\"Class counts:\", counts)\n",
    "print(\"FOOOF backend:\", FOOOF_BACKEND)\n",
    "print(f\"Final feature channel list ({len(feature_channels)} channels): {feature_channels}\")\n",
    "if dropped_due_to_nonfinite:\n",
    "    print(f\"Dropped epochs due to NaN/Inf: {dropped_due_to_nonfinite}\")\n",
    "if COMBINE_ADJACENT_EPOCHS:\n",
    "    print(f\"Combined {paired_epochs_created} pairs of adjacent epochs; dropped {paired_epochs_dropped} singles that could not be paired.\")\n",
    "print(\"Epoch statistics (found → used):\")\n",
    "for cls in sorted(class_epoch_found):\n",
    "    label_name = class_names.get(cls, str(cls))\n",
    "    found = class_epoch_found[cls]\n",
    "    kept = class_epoch_kept[cls]\n",
    "    print(f\"  {label_name}: {found} found, {kept} used\")\n",
    "\n",
    "# Optional QC: channels missing per file (when using UNION)\n",
    "if missing_report:\n",
    "    for fname, n_miss in missing_report:\n",
    "        print(f\"{fname}: {n_miss} / {len(feature_channels)} channels missing in this recording\")\n",
    "\n",
    "# Derive TARGET_CHANNELS for plotting or later use\n",
    "if ALL_CHANNELS:\n",
    "    preferred = [ch for ch in (\"O1\", \"O2\"\n",
    "                               , \"P3\", \"P4\", \"P7\", \"P8\", \"Pz\"\n",
    "                            #   , \"F3\", \"F4\", \"C3\", \"C4\", \"F7\", \"F8\", \"T7\", \"T8\", \"Fz\", \"Cz\"\n",
    "                            #   , \"Fp1\", \"Fp2\"\n",
    "                               ) if ch in feature_channels]\n",
    "    TARGET_CHANNELS = preferred if preferred else feature_channels[:2]\n",
    "else:\n",
    "    TARGET_CHANNELS = feature_channels\n",
    "\n",
    "PSD_META = dict(n_channels=len(feature_channels), n_freqs=psd_cube.shape[-1], channels=feature_channels, freqs=psd_freqs)\n",
    "\n",
    "# Persist feature-axis metadata for downstream labeling notebooks\n",
    "try:\n",
    "    np.save(outpath('feature_channels.npy'), np.asarray(feature_channels, dtype=object))\n",
    "    np.save(outpath('psd_freqs.npy'), np.asarray(psd_freqs, dtype=float))\n",
    "    if 'USE_FOOOF' in globals() and USE_FOOOF:\n",
    "        np.save(outpath('fooof_selected_features.npy'), np.asarray(list(FOOOF_SELECTED_FEATURES), dtype=object))\n",
    "    print('Saved feature metadata:', outpath('feature_channels.npy').name, outpath('psd_freqs.npy').name)\n",
    "except Exception as _exc:\n",
    "    warnings.warn(f'Could not save feature metadata: {_exc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995cc60",
   "metadata": {},
   "source": [
    "### Diagnostic: subject-level ROI PSD and main alpha peak (ONE_MAIN_FOOOF)\n",
    "\n",
    "This cell is a *visual check* of the **subject-level alpha profile** used in `ONE_MAIN_FOOOF`.\n",
    "\n",
    "It:\n",
    "- Picks a subject that has an entry in `SUBJECT_ALPHA_PROFILE`.\n",
    "- Averages PSDs across the ROI channels (`ALPHA_PROFILE_ROI`).\n",
    "- Fits FOOOF/specparam to the averaged spectrum.\n",
    "- Plots the empirical PSD, the reconstructed aperiodic fit, and the strongest alpha-range peak.\n",
    "\n",
    "Run it only after feature extraction when `USE_FOOOF=True` and `ONE_MAIN_FOOOF=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadab2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot shows: original PSD, aperiodic fit, and the Gaussian alpha peak component.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "if not (\"USE_FOOOF\" in globals() and USE_FOOOF and \"ONE_MAIN_FOOOF\" in globals() and ONE_MAIN_FOOOF):\n",
    "    print(\"This plot is intended for ONE_MAIN_FOOOF mode after feature extraction.\")\n",
    "else:\n",
    "    if not SUBJECT_ALPHA_PROFILE:\n",
    "        print(\"No SUBJECT_ALPHA_PROFILE available – run the feature extraction cell first.\")\n",
    "    else:\n",
    "        # Pick the first subject with an alpha profile\n",
    "        subj_example = sorted(SUBJECT_ALPHA_PROFILE.keys())[0]\n",
    "        alpha_cf, alpha_bw = SUBJECT_ALPHA_PROFILE[subj_example]\n",
    "\n",
    "        # Determine ROI channels used for the profile\n",
    "        roi_names = [ch for ch in ALPHA_PROFILE_ROI if ch in feature_channels]\n",
    "        if not roi_names:\n",
    "            roi_names = feature_channels\n",
    "        roi_idx = [feature_channels.index(ch) for ch in roi_names]\n",
    "\n",
    "        # Gather all PSDs for this subject and ROI channels\n",
    "        subj_mask = (subject_ids == subj_example)\n",
    "        if not np.any(subj_mask):\n",
    "            print(\"No epochs found in psd_cube for subject\", subj_example)\n",
    "        else:\n",
    "            data = psd_cube[subj_mask][:, roi_idx, :]  # (n_epochs, n_roi, n_freqs)\n",
    "            mean_spectrum = data.mean(axis=(0, 1))\n",
    "            if not np.any(np.isfinite(mean_spectrum)):\n",
    "                print(\"Mean spectrum for subject\", subj_example, \"is non-finite.\")\n",
    "            else:\n",
    "                freqs_arr = np.asarray(psd_freqs, float)\n",
    "\n",
    "                # ---- choose FOOOF fit range (broad, like the report) ----\n",
    "                if isinstance(FOOOF_SETTINGS, dict) and \"freq_range\" in FOOOF_SETTINGS and FOOOF_SETTINGS[\"freq_range\"] is not None:\n",
    "                    fit_lo, fit_hi = FOOOF_SETTINGS[\"freq_range\"]\n",
    "                else:\n",
    "                    fit_lo, fit_hi = freqs_arr[0], freqs_arr[-1]\n",
    "\n",
    "                fit_mask = (freqs_arr >= fit_lo) & (freqs_arr <= fit_hi)\n",
    "                freqs_fit = freqs_arr[fit_mask]\n",
    "                spec_fit = mean_spectrum[fit_mask]\n",
    "\n",
    "                # ---- Fit FOOOF/specparam on the averaged spectrum ----\n",
    "                model = SpectralModel(**FOOOF_SETTINGS)\n",
    "                model.fit(freqs_fit, spec_fit)\n",
    "\n",
    "                # Frequency axis and original spectrum as the model sees them\n",
    "                freqs_plot = np.asarray(getattr(model, \"freqs\", freqs_fit))\n",
    "                psd_plot   = np.asarray(getattr(model, \"power_spectrum\", spec_fit))\n",
    "\n",
    "                # ---- Reconstruct aperiodic fit from aperiodic_params_ ----\n",
    "                ap_params = np.asarray(getattr(model, \"aperiodic_params_\", []), float)\n",
    "                if ap_params.size == 0:\n",
    "                    ap_fit = np.zeros_like(freqs_plot, dtype=float)\n",
    "                else:\n",
    "                    if ap_params.size == 2:\n",
    "                        # fixed mode: [offset, exponent]\n",
    "                        offset, exponent = ap_params\n",
    "                        ap_fit = offset - exponent * np.log10(freqs_plot)\n",
    "                    elif ap_params.size == 3:\n",
    "                        # knee mode: [offset, knee, exponent] – approximate formula\n",
    "                        offset, knee, exponent = ap_params\n",
    "                        ap_fit = offset - np.log10(knee + freqs_plot**exponent)\n",
    "                    else:\n",
    "                        # fallback: flat\n",
    "                        ap_fit = np.zeros_like(freqs_plot, dtype=float)\n",
    "\n",
    "                # ---- Build Gaussian for the main alpha peak ----\n",
    "                gauss_main = None\n",
    "                peaks = np.asarray(getattr(model, \"peak_params_\", []), float)\n",
    "                if peaks.size:\n",
    "                    lo_alpha, hi_alpha = ALPHA_PROFILE_RANGE\n",
    "                    # pick strongest peak inside alpha range\n",
    "                    mask_peaks = (peaks[:, 0] >= lo_alpha) & (peaks[:, 0] <= hi_alpha)\n",
    "                    if np.any(mask_peaks):\n",
    "                        subset = peaks[mask_peaks]\n",
    "                        best = subset[np.argmax(subset[:, 1])]\n",
    "                        cf, amp, bw = map(float, best[:3])\n",
    "                        if bw > 0:\n",
    "                            sigma = bw / (2.0 * math.sqrt(2.0 * math.log(2.0)))\n",
    "                            gauss_main = amp * np.exp(-0.5 * ((freqs_plot - cf) / sigma) ** 2)\n",
    "\n",
    "                # ---- Prepare plot ----\n",
    "                plt.figure(figsize=(7, 4))\n",
    "\n",
    "                # Original averaged PSD (as FOOOF sees it)\n",
    "                plt.plot(freqs_plot, psd_plot, label=\"Averaged PSD\", color=\"#1f77b4\")\n",
    "\n",
    "                # Aperiodic fit\n",
    "                plt.plot(freqs_plot, ap_fit, label=\"Aperiodic fit\", color=\"#ff7f0e\", linestyle=\"--\")\n",
    "\n",
    "                # Aperiodic + main alpha peak\n",
    "                if gauss_main is not None:\n",
    "                    plt.plot(freqs_plot, ap_fit + gauss_main,\n",
    "                             label=\"Aperiodic + main alpha\", color=\"#2ca02c\")\n",
    "\n",
    "                plt.xlabel(\"Frequency (Hz)\")\n",
    "                plt.ylabel(\"Power (log10 or model units)\")\n",
    "                plt.title(f\"Subject {subj_example}: averaged ROI PSD and main FOOOF fit\")\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859482b",
   "metadata": {},
   "source": [
    "### Verification: FOOOF/specparam fitting behavior\n",
    "\n",
    "These plots help verify that the FOOOF/specparam model is behaving sensibly.\n",
    "\n",
    "Typical checks:\n",
    "- The modeled spectrum tracks the empirical PSD over the fit range.\n",
    "- Detected peaks fall in plausible frequency ranges.\n",
    "- The selected alpha-related peak/feature is consistent across epochs.\n",
    "\n",
    "Use this when debugging feature behavior or comparing settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd18efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- FOOOF VERIFICATION PLOTS --------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional deps that might be used here\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "# ---- safety & defaults ----\n",
    "# Gracefully handle FitError not being imported\n",
    "try:\n",
    "    _FitErrorBase = FitError  # noqa: F821\n",
    "except Exception:\n",
    "    class _FitErrorBase(Exception):\n",
    "        pass\n",
    "\n",
    "# Ensure an alpha range exists for model fitting\n",
    "if 'ALPHA_FREQ_RANGE' not in globals() or ALPHA_FREQ_RANGE is None:\n",
    "    if 'ALPHA_BAND' in globals() and ALPHA_BAND is not None:\n",
    "        ALPHA_FREQ_RANGE = ALPHA_BAND\n",
    "    else:\n",
    "        ALPHA_FREQ_RANGE = (7.0, 14.0)  # sensible default\n",
    "\n",
    "# Helper to extract the modeled spectrum across fooof/specparam versions\n",
    "def _get_modeled_spectrum(model):\n",
    "    # Known attr variants across versions\n",
    "    for name in ('fooofed_spectrum_', 'modeled_spectrum_', 'model_spectrum_', 'model_spectrum__'):\n",
    "        if hasattr(model, name):\n",
    "            return getattr(model, name)\n",
    "    # API method on some versions\n",
    "    get_fun = getattr(model, 'get_model_spectrum', None)\n",
    "    if callable(get_fun):\n",
    "        try:\n",
    "            return get_fun()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# Helper to extract the frequency vector used by the model\n",
    "def _get_model_freqs(model, psd_freqs, modeled, alpha_range):\n",
    "    for attr in (\"freqs\", \"freqs_\", \"model_freqs_\", \"model_freqs\"):\n",
    "        if hasattr(model, attr):\n",
    "            freqs = np.asarray(getattr(model, attr))\n",
    "            if freqs is not None and freqs.size == np.asarray(modeled).size:\n",
    "                return freqs\n",
    "    # Fallback: try to map to the input freqs via the fit range\n",
    "    lo, hi = alpha_range\n",
    "    mask = (psd_freqs >= lo) & (psd_freqs <= hi)\n",
    "    if modeled is not None and mask.sum() == np.asarray(modeled).size:\n",
    "        return psd_freqs[mask]\n",
    "    # Last resort: evenly space within the range with the correct length\n",
    "    return np.linspace(lo, hi, np.asarray(modeled).size)\n",
    "\n",
    "# ---- main guard ----\n",
    "if not USE_FOOOF or SpectralModel is None:\n",
    "    print(\"FOOOF verification skipped because USE_FOOOF=False or no compatible backend is available.\")\n",
    "else:\n",
    "    rng = np.random.default_rng(42)\n",
    "    class_lookup = {0: \"Eyes Open\", 1: \"Eyes Closed\"}\n",
    "\n",
    "    # Pick channels present in PSD_META; fall back to the first channel if none match\n",
    "    channels_for_plot = [ch for ch in TARGET_CHANNELS if ch in PSD_META['channels']]\n",
    "    if not channels_for_plot:\n",
    "        channels_for_plot = [PSD_META['channels'][0]]\n",
    "    channel_indices = [PSD_META['channels'].index(ch) for ch in channels_for_plot]\n",
    "\n",
    "    # Sample a few epochs per class\n",
    "    samples_per_class = 2\n",
    "    sample_records = []\n",
    "    for label in (0, 1):\n",
    "        idx_pool = np.where(y_combined == label)[0]\n",
    "        if idx_pool.size == 0:\n",
    "            continue\n",
    "        take = min(samples_per_class, idx_pool.size)\n",
    "        chosen = rng.choice(idx_pool, size=take, replace=False)\n",
    "        for epoch_idx in chosen:\n",
    "            sample_records.append({\n",
    "                \"epoch_index\": int(epoch_idx),\n",
    "                \"label\": label,\n",
    "                \"label_name\": class_lookup.get(label, str(label)),\n",
    "                \"subject_id\": int(subject_ids[epoch_idx]),\n",
    "            })\n",
    "    if not sample_records:\n",
    "        raise RuntimeError(\"Could not locate any epochs for FOOOF verification plots.\")\n",
    "\n",
    "    # Fit per sample/channel and collect outputs\n",
    "    plot_payloads = []\n",
    "    for rec in sample_records:\n",
    "        channel_payload = {}\n",
    "        for ch_name, ch_idx in zip(channels_for_plot, channel_indices):\n",
    "            spectrum = psd_cube[rec[\"epoch_index\"], ch_idx, :]\n",
    "            if spectrum.ndim != 1 or not np.any(np.isfinite(spectrum)):\n",
    "                continue\n",
    "            try:\n",
    "                model = SpectralModel(**FOOOF_SETTINGS)\n",
    "                # Most implementations expect linear power; keep as-is\n",
    "                model.fit(psd_freqs, spectrum, freq_range=ALPHA_FREQ_RANGE)\n",
    "\n",
    "                # Peaks: (CF, Amp, BW) rows; empty if none\n",
    "                peaks = np.asarray(getattr(model, \"peak_params_\", []))\n",
    "\n",
    "                # Modeled spectrum (log or linear depending on backend)\n",
    "                modeled = _get_modeled_spectrum(model)\n",
    "\n",
    "                # Frequencies the model actually used (length must match modeled)\n",
    "                fit_freqs = _get_model_freqs(model, psd_freqs, modeled, ALPHA_FREQ_RANGE)\n",
    "            except (_FitErrorBase, RuntimeError, ValueError, np.linalg.LinAlgError, FloatingPointError):\n",
    "                peaks, modeled, fit_freqs = np.empty((0, 3)), None, None\n",
    "\n",
    "            channel_payload[ch_name] = {\n",
    "                \"spectrum\": spectrum,\n",
    "                \"modeled\": modeled,\n",
    "                \"fit_freqs\": fit_freqs,\n",
    "                \"peaks\": peaks,\n",
    "            }\n",
    "        plot_payloads.append({**rec, \"channels\": channel_payload})\n",
    "\n",
    "    # Plot PSDs and fits with alpha markers\n",
    "    n_cols = len(plot_payloads)\n",
    "    n_rows = len(channels_for_plot)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3.2 * n_rows), sharex=True, sharey=False)\n",
    "    if n_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    if n_cols == 1:\n",
    "        axes = np.expand_dims(axes, axis=1)\n",
    "\n",
    "    # Use ALPHA_BAND for shading if available; else ALPHA_FREQ_RANGE\n",
    "    _alpha_lo, _alpha_hi = (ALPHA_BAND if 'ALPHA_BAND' in globals() and ALPHA_BAND is not None\n",
    "                            else ALPHA_FREQ_RANGE)\n",
    "\n",
    "    for col, payload in enumerate(plot_payloads):\n",
    "        for row, ch_name in enumerate(channels_for_plot):\n",
    "            ax = axes[row, col]\n",
    "            channel_info = payload[\"channels\"].get(ch_name)\n",
    "            if not channel_info:\n",
    "                ax.text(0.5, 0.5, f\"{ch_name} missing\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "\n",
    "            # Empirical PSD (log10 for readability)\n",
    "            y_emp = np.log10(np.maximum(channel_info[\"spectrum\"], 1e-30))\n",
    "            ax.plot(psd_freqs, y_emp, label=\"PSD\", linewidth=1.5, color=\"#1f77b4\")\n",
    "\n",
    "            # Modeled fit (use model's own frequency vector)\n",
    "            if channel_info[\"modeled\"] is not None:\n",
    "                modeled = np.asarray(channel_info[\"modeled\"]).squeeze()\n",
    "                if modeled.ndim != 1:\n",
    "                    modeled = modeled.reshape(-1)\n",
    "                x_mod = channel_info.get(\"fit_freqs\")\n",
    "                if x_mod is None or np.asarray(x_mod).shape[0] != modeled.shape[0]:\n",
    "                    lo, hi = (_alpha_lo, _alpha_hi)\n",
    "                    x_mod = np.linspace(lo, hi, modeled.shape[0])\n",
    "\n",
    "                # Keep y-scale consistent with y_emp (log10)\n",
    "                # Heuristic: fooofed_spectrum_ is already log10-scale\n",
    "                m_is_log = (np.nanmax(modeled) < 5.0)  # typical log10 PSD range\n",
    "                y_mod = modeled if m_is_log else np.log10(np.maximum(modeled, 1e-30))\n",
    "\n",
    "                ax.plot(np.asarray(x_mod), y_mod, linestyle=\"--\", linewidth=1.2, color=\"#ff7f0e\", label=\"FOOOF fit\")\n",
    "\n",
    "            # Mark peaks within the shaded alpha band (for context)\n",
    "            peaks = channel_info[\"peaks\"]\n",
    "            mask = (peaks[:, 0] >= _alpha_lo) & (peaks[:, 0] <= _alpha_hi) if peaks.size else np.zeros(0, dtype=bool)\n",
    "            if mask.any():\n",
    "                ax.vlines(peaks[mask][:, 0], ymin=np.min(y_emp), ymax=np.max(y_emp),\n",
    "                          colors=\"#d62728\", linestyles=\":\", label=\"Alpha peaks (in band)\")\n",
    "            # Highlight the exact peak used as the feature (may be outside 8–12 Hz)\n",
    "            sel_cf = None\n",
    "            if peaks.size:\n",
    "                # Selection logic mirrors compute_fooof_features via _select_alpha_peak\n",
    "                try:\n",
    "                    if \"_select_alpha_peak\" in globals():\n",
    "                        chosen = _select_alpha_peak(peaks)\n",
    "                    else:\n",
    "                        chosen = None\n",
    "                    if chosen is not None:\n",
    "                        sel_cf = float(chosen[0])\n",
    "                except Exception:\n",
    "                    sel_cf = None\n",
    "            if sel_cf is not None:\n",
    "                ax.vlines([sel_cf], ymin=np.min(y_emp), ymax=np.max(y_emp), colors=\"#e41a1c\", linestyles=\"-\", linewidth=2.0, label=\"Selected feature peak\")\n",
    "\n",
    "            # Shade alpha band\n",
    "            ax.axvspan(_alpha_lo, _alpha_hi, color=\"#ffbf00\", alpha=0.15)\n",
    "\n",
    "            if row == 0:\n",
    "                ax.set_title(f\"{payload['label_name']} — subj {payload['subject_id']} — epoch #{payload['epoch_index']}\")\n",
    "            ax.set_ylabel(f\"{ch_name}: log10 power\")\n",
    "            ax.set_xlabel(\"Frequency (Hz)\")\n",
    "            ax.grid(alpha=0.2)\n",
    "\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        # Avoid duplicate legend entries\n",
    "        uniq = dict(zip(labels, handles))\n",
    "        fig.legend(uniq.values(), uniq.keys(), loc=\"upper right\")\n",
    "    fig.suptitle(\"FOOOF verification: PSDs with detected peaks\", fontsize=14)\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.97))\n",
    "    plt.show()\n",
    "\n",
    "    # ----- Distribution of detected alpha-center features from your feature matrix -----\n",
    "    feature_stride = 5  # [offset, exponent, center, amp, bw] → center is index +2\n",
    "    channel_to_feat = {ch: idx for idx, ch in enumerate(feature_channels)}\n",
    "    peak_records = []\n",
    "\n",
    "    for ch_name, feat_idx in channel_to_feat.items():\n",
    "        center_col = feat_idx * feature_stride + 2\n",
    "        if center_col >= X_combined.shape[1]:\n",
    "            continue\n",
    "        center_values = X_combined[:, center_col]\n",
    "        mask = np.isfinite(center_values) & (center_values > 0)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        for freq, label in zip(center_values[mask], y_combined[mask]):\n",
    "            peak_records.append({\n",
    "                \"channel\": ch_name,\n",
    "                \"center_freq\": float(freq),\n",
    "                \"label\": class_lookup.get(int(label), str(label)),\n",
    "            })\n",
    "\n",
    "    if peak_records and sns is not None:\n",
    "        alpha_df = pd.DataFrame(peak_records)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        lo_hist, hi_hist = ALPHA_FREQ_RANGE\n",
    "        sns.histplot(alpha_df, x=\"center_freq\", hue=\"label\", multiple=\"stack\",\n",
    "                     bins=np.linspace(lo_hist, hi_hist, 25))\n",
    "        plt.xlabel(\"Detected alpha peak center frequency (Hz)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Distribution of detected alpha peaks (all epochs)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif not peak_records:\n",
    "        print(\"Alpha peak distribution skipped – the feature matrix did not contain valid center frequencies.\")\n",
    "    else:\n",
    "        print(\"Seaborn not available; skipping alpha peak distribution plot.\")\n",
    "# ---- end verification ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd283f",
   "metadata": {},
   "source": [
    "### FOOOF Plot\n",
    "\n",
    "This plot is a **sanity check** for FOOOF/specparam-based features.\n",
    "\n",
    "It loads one subject/epoch (EO and EC), then:\n",
    "- Overlays the empirical PSD with the fitted aperiodic/background component.\n",
    "- Shows detected peaks and the selected alpha-related peak/features.\n",
    "\n",
    "Use it when `USE_FOOOF=True` (especially with `ONE_MAIN_FOOOF=True`) to confirm that the fitting range, channel selection, and peak behavior look reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33535c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Drop-in: PSD + FOOOF/specparam overlays for selected channels (EC vs EO) ----\n",
    "# Implements:\n",
    "# • Plot PSD for selected channels.\n",
    "# • If FOOOF/specparam is enabled, also overlay the FULL model fit and the APERIODIC fit.\n",
    "# • Toggle y-axis between log10 power and linear power via FOOOF_VIS_LOG_POWER.\n",
    "# • Two side-by-side columns: Eyes Closed (EC) and Eyes Open (EO).\n",
    "#\n",
    "# Assumes the following exist in your notebook:\n",
    "#   USE_FOOOF, SpectralModel, FOOOF_SETTINGS, TARGET_CHANNELS,\n",
    "#   eyes_closed_files, eyes_open_files, parse_subject_id, PSD_KWARGS\n",
    "# and imports: numpy as np, matplotlib.pyplot as plt, warnings, mne, loadmat, Path\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "FOOOF_VIS_SUBJECT = int(subject_ids[0])\n",
    "FOOOF_VIS_EPOCH_RANK = {\"EC\": 0, \"EO\": 0}\n",
    "FOOOF_VIS_FREQ_RANGE = (3.0, 40.0)\n",
    "FOOOF_VIS_LOG_POWER = True  # Toggle: True = log10(y), False = linear(y)\n",
    "\n",
    "# -------------------- UTILS ---------------------\n",
    "# Some environments don't have FitError imported; use a safe fallback\n",
    "try:\n",
    "    _FitErrorBase = FitError  # noqa: F821\n",
    "except Exception:\n",
    "    class _FitErrorBase(Exception):\n",
    "        pass\n",
    "\n",
    "def _locate_subject_file(file_list, subj_id):\n",
    "    for candidate in file_list:\n",
    "        try:\n",
    "            if parse_subject_id(candidate) == subj_id:\n",
    "                return candidate\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _get_modeled_spectrum(model):\n",
    "    \"\"\"Return the model's full fit spectrum (may be log10 or linear depending on backend).\"\"\"\n",
    "    for name in ('fooofed_spectrum_', 'modeled_spectrum_', 'model_spectrum_', 'model_spectrum__'):\n",
    "        if hasattr(model, name):\n",
    "            return getattr(model, name)\n",
    "    fn = getattr(model, 'get_model_spectrum', None)\n",
    "    if callable(fn):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _get_model_freqs(model, freqs_input, modeled, fit_range):\n",
    "    \"\"\"Return the frequency vector that corresponds to the modeled array length.\"\"\"\n",
    "    modeled = np.asarray(modeled) if modeled is not None else None\n",
    "    for attr in (\"freqs\", \"freqs_\", \"model_freqs_\", \"model_freqs\"):\n",
    "        if hasattr(model, attr):\n",
    "            f = np.asarray(getattr(model, attr))\n",
    "            if modeled is None or f.size == modeled.size:\n",
    "                return f\n",
    "    # Fallbacks:\n",
    "    lo, hi = fit_range\n",
    "    mask = (freqs_input >= lo) & (freqs_input <= hi)\n",
    "    if modeled is not None and mask.sum() == modeled.size:\n",
    "        return freqs_input[mask]\n",
    "    if modeled is not None:\n",
    "        return np.linspace(lo, hi, modeled.size)\n",
    "    return freqs_input[(freqs_input >= lo) & (freqs_input <= hi)]\n",
    "\n",
    "def _compute_aperiodic_curve(model, freqs_for_curve):\n",
    "    \"\"\"\n",
    "    Compute aperiodic-only curve on freqs_for_curve.\n",
    "    Works for both fixed and knee modes if params are exposed.\n",
    "    Returns array on LOG10 scale (to match fooof's internal representation).\n",
    "    \"\"\"\n",
    "    params = getattr(model, \"aperiodic_params_\", None)\n",
    "    mode = getattr(model, \"aperiodic_mode_\", getattr(model, \"aperiodic_mode\", \"fixed\"))\n",
    "    if params is None:\n",
    "        # Some versions have a direct method:\n",
    "        fn = getattr(model, \"get_aperiodic\", None)\n",
    "        if callable(fn):\n",
    "            try:\n",
    "                ap = fn()\n",
    "                return np.asarray(ap)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    p = np.asarray(params).ravel()\n",
    "    f = np.asarray(freqs_for_curve, dtype=float)\n",
    "    f = np.clip(f, 1e-6, None)  # avoid log10(0)\n",
    "\n",
    "    try:\n",
    "        if (isinstance(mode, str) and \"knee\" in mode.lower()) or p.size >= 3:\n",
    "            # Knee model: log10 P = offset - log10(knee + f**exponent)\n",
    "            offset = p[0]; knee = p[1]; exponent = p[2]\n",
    "            log10_ap = offset - np.log10(knee + np.power(f, exponent))\n",
    "        else:\n",
    "            # Fixed model: log10 P = offset - exponent*log10(f)\n",
    "            offset = p[0]; exponent = p[1]\n",
    "            log10_ap = offset - exponent * np.log10(f)\n",
    "        return log10_ap\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_log10(arr):\n",
    "    return np.log10(np.maximum(np.asarray(arr, dtype=float), 1e-30))\n",
    "\n",
    "def _load_epoch_payload(file_path, rank_within_marked, label_hint):\n",
    "    if file_path is None:\n",
    "        raise RuntimeError(f\"No {label_hint} file matched subject {FOOOF_VIS_SUBJECT}.\")\n",
    "    path_obj = Path(file_path).resolve()\n",
    "    if path_obj.suffix.lower() == '.fif':\n",
    "        epochs = mne.read_epochs(str(path_obj), preload=False, verbose='ERROR')\n",
    "        if '_rename_epochs_channels_canonical' in globals():\n",
    "            epochs = _rename_epochs_channels_canonical(epochs)\n",
    "    else:\n",
    "        epochs = mne.io.read_epochs_eeglab(str(path_obj), verbose='ERROR')\n",
    "\n",
    "    available_channels = [ch for ch in TARGET_CHANNELS if ch in epochs.ch_names]\n",
    "    if not available_channels:\n",
    "        raise RuntimeError(f\"{path_obj.name} does not contain the requested channels {TARGET_CHANNELS}.\")\n",
    "\n",
    "    if path_obj.suffix.lower() == '.fif':\n",
    "        want_label = 1 if ('Closed' in str(label_hint)) else 0\n",
    "        if '_labels_from_epochs_events' in globals():\n",
    "            labels_a = _labels_from_epochs_events(epochs)\n",
    "        else:\n",
    "            labels_a = np.full(len(epochs), -1, dtype=int)\n",
    "        union_labels = labels_a\n",
    "        try:\n",
    "            b_stem = re.sub(r'(sub\\\\d+)a', r'\\\\1b', path_obj.stem, flags=re.IGNORECASE)\n",
    "            b_path = path_obj.with_name(b_stem + path_obj.suffix)\n",
    "        except Exception:\n",
    "            b_path = None\n",
    "        if b_path is not None and b_path.exists() and '_labels_from_epochs_events' in globals():\n",
    "            try:\n",
    "                epochs_b = mne.read_epochs(str(b_path), preload=False, verbose='ERROR')\n",
    "                labels_b = _labels_from_epochs_events(epochs_b)\n",
    "                if labels_b.shape == labels_a.shape:\n",
    "                    union_labels = labels_a.copy()\n",
    "                    take_from_b = (union_labels < 0)\n",
    "                    union_labels[take_from_b] = labels_b[take_from_b]\n",
    "                    conflict_mask = (labels_a >= 0) & (labels_b >= 0) & (labels_a != labels_b)\n",
    "                    union_labels[conflict_mask] = -1\n",
    "            except Exception:\n",
    "                pass\n",
    "        keep_indices = np.where(union_labels == want_label)[0]\n",
    "    else:\n",
    "        # Read EEGLAB 'reject' marks if present\n",
    "        try:\n",
    "            mat = loadmat(str(path_obj), struct_as_record=False, squeeze_me=True)\n",
    "            reject_block = mat.get(\"reject\", None)\n",
    "            if reject_block is not None and hasattr(reject_block, \"rejmanual\"):\n",
    "                labels = np.array(reject_block.rejmanual, dtype=int).ravel()\n",
    "            elif reject_block is not None:\n",
    "                labels = np.array(reject_block, dtype=int).ravel()\n",
    "            else:\n",
    "                labels = np.zeros(len(epochs), dtype=int)\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f\"Falling back to unlabelled epochs for {path_obj.name}: {exc}\")\n",
    "            labels = np.zeros(len(epochs), dtype=int)\n",
    "\n",
    "        keep_indices = np.where(labels == 0)[0]\n",
    "    if keep_indices.size == 0:\n",
    "        keep_indices = np.arange(len(epochs))\n",
    "    if rank_within_marked >= keep_indices.size:\n",
    "        raise IndexError(\n",
    "            f\"Requested epoch rank {rank_within_marked} exceeds the available marked epochs (n={keep_indices.size}).\"\n",
    "        )\n",
    "\n",
    "    target_epoch = int(keep_indices[rank_within_marked])\n",
    "    single_epoch = epochs[target_epoch:target_epoch + 1]\n",
    "    psd = single_epoch.compute_psd(**PSD_KWARGS)\n",
    "    freqs = psd.freqs\n",
    "    spectra = psd.get_data()[0]\n",
    "\n",
    "    fooof_enabled = bool(USE_FOOOF and SpectralModel is not None)\n",
    "\n",
    "    fooof_full, fooof_aper, fooof_freqs = {}, {}, {}\n",
    "    for ch in available_channels:\n",
    "        idx = epochs.ch_names.index(ch)\n",
    "        fooof_full[ch] = None\n",
    "        fooof_aper[ch] = None\n",
    "        fooof_freqs[ch] = None\n",
    "        if not fooof_enabled:\n",
    "            continue\n",
    "\n",
    "        spectrum = spectra[idx]\n",
    "        try:\n",
    "            model = SpectralModel(**FOOOF_SETTINGS)\n",
    "            model.fit(freqs, spectrum, freq_range=FOOOF_VIS_FREQ_RANGE)\n",
    "\n",
    "            modeled = _get_modeled_spectrum(model)          # may be log10 or linear\n",
    "            fit_freqs = _get_model_freqs(model, freqs, modeled, FOOOF_VIS_FREQ_RANGE)\n",
    "\n",
    "            # Ensure 1D & same length for frequency & modeled\n",
    "            if modeled is not None:\n",
    "                modeled = np.asarray(modeled).reshape(-1)\n",
    "                if fit_freqs.shape[0] != modeled.shape[0]:\n",
    "                    # Last-resort alignment: interpolate modeled to fit_freqs length if needed\n",
    "                    # (but usually _get_model_freqs ensures equality)\n",
    "                    x_tmp = np.linspace(fit_freqs.min(), fit_freqs.max(), modeled.shape[0])\n",
    "                    modeled = np.interp(fit_freqs, x_tmp, modeled)\n",
    "\n",
    "            # Aperiodic curve (log10 scale)\n",
    "            log10_ap = _compute_aperiodic_curve(model, fit_freqs)\n",
    "\n",
    "            fooof_full[ch] = modeled\n",
    "            fooof_aper[ch] = log10_ap  # store LOG10 curve; we'll convert later if needed\n",
    "            fooof_freqs[ch] = fit_freqs\n",
    "\n",
    "        except (_FitErrorBase, RuntimeError, ValueError, np.linalg.LinAlgError, FloatingPointError) as exc:\n",
    "            warnings.warn(f\"FOOOF failed for {path_obj.name} channel {ch}: {exc}\")\n",
    "\n",
    "    payload = {\n",
    "        \"file\": str(path_obj),\n",
    "        \"epoch_index\": target_epoch,\n",
    "        \"freqs\": freqs,\n",
    "        \"spectra\": {ch: spectra[epochs.ch_names.index(ch)] for ch in available_channels},\n",
    "        \"fooof_full\": fooof_full,\n",
    "        \"fooof_aper\": fooof_aper,     # LOG10 curve if present\n",
    "        \"fooof_freqs\": fooof_freqs,\n",
    "        \"available_channels\": available_channels,\n",
    "        \"sfreq\": float(epochs.info['sfreq']),\n",
    "        \"time_series\": single_epoch.get_data()[0][[epochs.ch_names.index(ch) for ch in available_channels], :],\n",
    "        \"label\": label_hint,\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "# -------------------- LOAD EC/EO PAYLOADS ---------------------\n",
    "# Prefer locating files via `records` (works even when NEW+OLD are both loaded).\n",
    "closed_file = None\n",
    "open_file = None\n",
    "try:\n",
    "    _sid = int(FOOOF_VIS_SUBJECT)\n",
    "except Exception:\n",
    "    _sid = None\n",
    "if _sid is not None and 'records' in globals():\n",
    "    recs_subj = [r for r in records if int(r.get('subject', -1)) == _sid]\n",
    "    if recs_subj:\n",
    "        if any(str(r.get('dataset', '')) == 'old' for r in recs_subj):\n",
    "            rec_ec = next((r for r in recs_subj if int(r.get('label', -1)) == 1), None)\n",
    "            rec_eo = next((r for r in recs_subj if int(r.get('label', -1)) == 0), None)\n",
    "            closed_file = rec_ec['file'] if rec_ec is not None else recs_subj[0]['file']\n",
    "            open_file = rec_eo['file'] if rec_eo is not None else recs_subj[0]['file']\n",
    "        else:\n",
    "            # NEW dataset: one file contains both EC/EO labeled epochs\n",
    "            closed_file = recs_subj[0]['file']\n",
    "            open_file = recs_subj[0]['file']\n",
    "\n",
    "# Fallback to legacy file-location logic if needed\n",
    "if closed_file is None or open_file is None:\n",
    "    if NEW_DATA:\n",
    "        subj_file = None\n",
    "        for sid, fa, fb in new_subject_pairs:\n",
    "            if int(sid) == int(FOOOF_VIS_SUBJECT):\n",
    "                subj_file = fa\n",
    "                break\n",
    "        closed_file = subj_file\n",
    "        open_file = subj_file\n",
    "    else:\n",
    "        closed_file = _locate_subject_file(eyes_closed_files, FOOOF_VIS_SUBJECT)\n",
    "        open_file   = _locate_subject_file(eyes_open_files,  FOOOF_VIS_SUBJECT)\n",
    "\n",
    "closed_payload = _load_epoch_payload(closed_file, FOOOF_VIS_EPOCH_RANK['EC'], 'Eyes Closed')\n",
    "open_payload   = _load_epoch_payload(open_file,  FOOOF_VIS_EPOCH_RANK['EO'], 'Eyes Open')\n",
    "\n",
    "shared_channels = [ch for ch in TARGET_CHANNELS if (ch in closed_payload['spectra'] or ch in open_payload['spectra'])]\n",
    "if not shared_channels:\n",
    "    shared_channels = list(closed_payload['spectra'].keys()) or list(open_payload['spectra'].keys())\n",
    "\n",
    "conditions = [(\"Eyes Closed\", closed_payload), (\"Eyes Open\", open_payload)]\n",
    "\n",
    "# -------------------- PLOTTING ---------------------\n",
    "fig, axes = plt.subplots(len(shared_channels), len(conditions), figsize=(12, 4 * len(shared_channels)), sharex=False)\n",
    "if len(shared_channels) == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for row, ch in enumerate(shared_channels):\n",
    "    for col, (label, payload) in enumerate(conditions):\n",
    "        ax = axes[row, col]\n",
    "        spectrum = payload['spectra'].get(ch)\n",
    "        if spectrum is None:\n",
    "            ax.text(0.5, 0.5, f\"{ch} missing\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.set_axis_off()\n",
    "            continue\n",
    "\n",
    "        freqs = payload['freqs']\n",
    "        # PSD (empirical)\n",
    "        if FOOOF_VIS_LOG_POWER:\n",
    "            y_emp = _safe_log10(spectrum)\n",
    "            y_label = \"log10 Power\"\n",
    "        else:\n",
    "            y_emp = np.asarray(spectrum)\n",
    "            y_label = \"Power (AU)\"\n",
    "        mask = (freqs >= FOOOF_VIS_FREQ_RANGE[0]) & (freqs <= FOOOF_VIS_FREQ_RANGE[1])\n",
    "        ax.plot(freqs[mask], y_emp[mask], label=f\"PSD ({ch})\", linewidth=1.5)\n",
    "\n",
    "        # Full model overlay (if available)\n",
    "        model_freqs = payload['fooof_freqs'].get(ch)\n",
    "        full_curve  = payload['fooof_full'].get(ch)\n",
    "\n",
    "        if model_freqs is not None and full_curve is not None:\n",
    "            model_freqs = np.asarray(model_freqs).reshape(-1)\n",
    "            full_curve  = np.asarray(full_curve).reshape(-1)\n",
    "            # If lengths still mismatch, resample y to x via interpolation\n",
    "            if model_freqs.shape[0] != full_curve.shape[0]:\n",
    "                x_tmp = np.linspace(model_freqs.min(), model_freqs.max(), full_curve.shape[0])\n",
    "                full_curve = np.interp(model_freqs, x_tmp, full_curve)\n",
    "\n",
    "            # Determine whether model curve is already log10 (FOOOFed) or linear\n",
    "            m_is_log = (np.nanmax(full_curve) < 5.0)  # typical log10 PSD range\n",
    "            y_full = full_curve if m_is_log else _safe_log10(full_curve)\n",
    "            if not FOOOF_VIS_LOG_POWER and m_is_log:\n",
    "                # convert LOG10 back to linear if requested\n",
    "                y_full = np.power(10.0, y_full)\n",
    "\n",
    "            ax.plot(model_freqs, y_full, linestyle='--', linewidth=1.2, label='Model fit')\n",
    "\n",
    "        # Aperiodic overlay (if available)\n",
    "        ap_log10 = payload['fooof_aper'].get(ch)  # stored as LOG10 if computed\n",
    "        if model_freqs is not None and ap_log10 is not None:\n",
    "            ap_log10 = np.asarray(ap_log10).reshape(-1)\n",
    "\n",
    "            # If lengths mismatch against model_freqs, interpolate\n",
    "            if ap_log10.shape[0] != np.asarray(model_freqs).shape[0]:\n",
    "                x_tmp = np.linspace(model_freqs.min(), model_freqs.max(), ap_log10.shape[0])\n",
    "                ap_log10 = np.interp(model_freqs, x_tmp, ap_log10)\n",
    "\n",
    "            y_ap = ap_log10\n",
    "            if not FOOOF_VIS_LOG_POWER:\n",
    "                y_ap = np.power(10.0, y_ap)\n",
    "\n",
    "            ax.plot(model_freqs, y_ap, linestyle=':', linewidth=1.2, label='Aperiodic fit')\n",
    "\n",
    "        ax.set_title(f\"{label} — {Path(payload['file']).name} — epoch {payload['epoch_index']} — {ch}\")\n",
    "        ax.set_xlabel(\"Frequency (Hz)\")\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.grid(alpha=0.2)\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "fig.suptitle(f\"Subject {FOOOF_VIS_SUBJECT}: PSD vs. Model & Aperiodic fits\", fontsize=14, y=0.98)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "plt.show()\n",
    "\n",
    "# Optionally keep a small summary dict of what you just plotted\n",
    "selected_epoch_data = {\n",
    "    \"subject_id\": FOOOF_VIS_SUBJECT,\n",
    "    \"sfreq\": float(closed_payload['sfreq']),\n",
    "    \"channels\": shared_channels,\n",
    "    \"eyes_closed\": {\n",
    "        \"avg_signal\": closed_payload['time_series'].mean(axis=0),\n",
    "        \"epoch_index\": closed_payload['epoch_index'],\n",
    "        \"file\": closed_payload['file'],\n",
    "    },\n",
    "    \"eyes_open\": {\n",
    "        \"avg_signal\": open_payload['time_series'].mean(axis=0),\n",
    "        \"epoch_index\": open_payload['epoch_index'],\n",
    "        \"file\": open_payload['file'],\n",
    "    },\n",
    "}\n",
    "# ---- End drop-in ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bac17",
   "metadata": {},
   "source": [
    "### Spectrogram plot\n",
    "\n",
    "This visualization shows **time–frequency structure** for a selected epoch.\n",
    "\n",
    "It is useful to:\n",
    "- Spot obvious artifacts (broadband bursts, line noise harmonics).\n",
    "- Compare EO vs EC structure qualitatively.\n",
    "- Confirm that epoch duration and sampling rate are interpreted correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Drop-in: Spectrogram visualization for selected epoch data ----\n",
    "\n",
    "if 'selected_epoch_data' not in globals():\n",
    "    raise RuntimeError(\"Run the PSD/FOOOF visualization cell first to populate selected_epoch_data.\")\n",
    "\n",
    "sfreq = selected_epoch_data['sfreq']\n",
    "window = 0.5\n",
    "step = 0.05\n",
    "fmax = 45.0\n",
    "\n",
    "\n",
    "def _spectrogram(signal, sfreq, window_sec, step_sec, fmax_hz):\n",
    "    signal = np.asarray(signal, float)\n",
    "    if signal.size == 0:\n",
    "        return None\n",
    "    nperseg = max(32, int(round(window_sec * sfreq)))\n",
    "    hop = max(1, int(round(step_sec * sfreq)))\n",
    "    nperseg = min(nperseg, signal.size)\n",
    "    noverlap = max(0, nperseg - hop)\n",
    "    freqs, times, Zxx = stft(signal - np.mean(signal), fs=sfreq, nperseg=nperseg, noverlap=noverlap, boundary='zeros', padded=True)\n",
    "    if freqs.size == 0 or times.size == 0:\n",
    "        return None\n",
    "    mask = freqs <= fmax_hz\n",
    "    if not np.any(mask):\n",
    "        mask = slice(None)\n",
    "    freqs = freqs[mask]\n",
    "    Zxx = Zxx[mask]\n",
    "    power = 10.0 * np.log10(np.maximum(np.abs(Zxx) ** 2, 1e-30))\n",
    "    return times, freqs, power\n",
    "\n",
    "spectra_payloads = {\n",
    "    'Eyes Closed': selected_epoch_data['eyes_closed'],\n",
    "    'Eyes Open': selected_epoch_data['eyes_open'],\n",
    "}\n",
    "results = {}\n",
    "all_power = []\n",
    "for label, payload in spectra_payloads.items():\n",
    "    spec = _spectrogram(payload['avg_signal'], sfreq, window, step, fmax)\n",
    "    results[label] = spec\n",
    "    if spec is not None:\n",
    "        all_power.append(spec[2].ravel())\n",
    "if not any(results.values()):\n",
    "    raise RuntimeError(\"STFT computation failed for both conditions.\")\n",
    "if all_power:\n",
    "    concatenated = np.concatenate(all_power)\n",
    "    vmin, vmax = np.percentile(concatenated, [5, 95])\n",
    "else:\n",
    "    vmin = vmax = None\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "axes = np.atleast_1d(axes)\n",
    "for ax, (label, payload) in zip(axes, spectra_payloads.items()):\n",
    "    spec = results[label]\n",
    "    if spec is None:\n",
    "        ax.text(0.5, 0.5, 'No spectrogram', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_axis_off()\n",
    "        continue\n",
    "    times, freqs, power = spec\n",
    "    mesh = ax.pcolormesh(times, freqs, power, shading='auto', cmap='magma', vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"{label} — epoch {payload['epoch_index']}\")\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "    fig.colorbar(mesh, ax=ax, pad=0.02, label='Power (dB)')\n",
    "fig.suptitle(\n",
    "    f\"Subject {selected_epoch_data['subject_id']} — spectrogram of averaged {', '.join(selected_epoch_data['channels'])}\",\n",
    "    fontsize=14,\n",
    "    y=0.98,\n",
    ")\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf630d7",
   "metadata": {},
   "source": [
    "## Training the logistic regression model\n",
    "\n",
    "This section trains and evaluates a **subject-wise** EC/EO classifier.\n",
    "\n",
    "Key ideas:\n",
    "- Splits are done by **subject**, not by epoch, to prevent leakage.\n",
    "- An inner loop chooses hyperparameters (e.g., `C`, and PSD binning when enabled).\n",
    "- Optional temporal smoothing (`USE_TIME_ADJUSTMENT`) can be evaluated on out-of-fold predictions.\n",
    "\n",
    "Run this after feature extraction has produced `X_combined`, `y_combined`, and `subject_ids`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa3ff7",
   "metadata": {},
   "source": [
    "The inner loop uses **leave-one-subject-out (LOSO)** validation on the training subjects.\n",
    "\n",
    "**What the next code cell does**\n",
    "- Builds the outer evaluation split(s) based on `CV_LEVEL`.\n",
    "- For each outer split:\n",
    "  - Runs an inner LOSO sweep over `C_GRID` (and `FREQ_BIN_OPTIONS` when PSD+binning is enabled).\n",
    "  - Fits the final logistic regression model with the chosen hyperparameters.\n",
    "  - Stores per-epoch predictions and fold metadata for later plots/exports.\n",
    "- Computes aggregated metrics and saves artifacts (CSV/NPZ/joblib) under `outputs/<config_tag>/`.\n",
    "\n",
    "**Important inputs/toggles**\n",
    "- Feature mode: `USE_FOOOF`, `FOOOF_SELECTED_FEATURES` vs PSD settings.\n",
    "- CV settings: `CV_LEVEL`, `CV_TEST_SUBJECTS_PER_SPLIT`, `CV_REPEAT_COUNT`, `CV_RANDOM_SEED`.\n",
    "- Temporal smoothing: `USE_TIME_ADJUSTMENT`, `MIN_RUN_LENGTH`, `TIME_AXIS_MODE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c502b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cross-validated logistic regression with inner LOSO hyper-parameter tuning ----\n",
    "\n",
    "C_grid = C_GRID\n",
    "n_bins_grid = FREQ_BIN_OPTIONS if (not USE_FOOOF and USE_FREQ_BINNING) else [None]\n",
    "\n",
    "# Logistic regression penalty grid (L1 vs L2)\n",
    "try:\n",
    "    penalty_grid = list(LOGREG_PENALTY_OPTIONS) if TUNE_LOGREG_PENALTY else [str(LOGREG_PENALTY_FIXED)]\n",
    "except Exception:\n",
    "    penalty_grid = [\"l2\"]\n",
    "penalty_grid = [str(p).lower() for p in penalty_grid if p is not None]\n",
    "if not penalty_grid:\n",
    "    penalty_grid = [\"l2\"]\n",
    "\n",
    "def _make_logreg(C_value: float, penalty: str):\n",
    "    pen = str(penalty).lower()\n",
    "    # L1 requires a solver that supports it (saga/liblinear). Use saga so L1 and L2 are comparable.\n",
    "    use_saga = (pen == \"l1\") or (pen == \"elasticnet\") or (pen == \"l2\" and (\"TUNE_LOGREG_PENALTY\" in globals() and TUNE_LOGREG_PENALTY))\n",
    "    solver = \"saga\" if use_saga else \"lbfgs\"\n",
    "    kwargs = dict(C=float(C_value), penalty=pen, solver=solver, max_iter=int(LOGREG_MAX_ITER), class_weight=CLASS_WEIGHT)\n",
    "    if solver == \"saga\":\n",
    "        kwargs[\"random_state\"] = int(CV_RANDOM_SEED)\n",
    "    return LogisticRegression(**kwargs)\n",
    "unique_subjects = np.unique(subject_ids)\n",
    "if unique_subjects.size < 2:\n",
    "    raise ValueError(\"Need at least two unique subjects for cross-validation.\")\n",
    "\n",
    "def _prepare_psd_matrix(psd_block, n_bins):\n",
    "    \"\"\"Reduce PSD to (n_samples, n_features) for a given n_bins.\n",
    "    If PSD_FEATURE_RANGE is set, only keep bins whose frequency span\n",
    "    overlaps the requested range.\n",
    "    \"\"\"\n",
    "    reduced = reduce_freq_resolution(psd_block, n_bins)  # (n_samples, n_channels, n_bins)\n",
    "    n_samples, n_channels, n_bins_eff = reduced.shape\n",
    "    if PSD_FEATURE_RANGE is not None:\n",
    "        fmin_sel, fmax_sel = PSD_FEATURE_RANGE\n",
    "        freqs = np.asarray(PSD_META['freqs'], float)\n",
    "        n_freqs = freqs.size\n",
    "        if n_freqs == 0:\n",
    "            return reduced.reshape(n_samples, n_channels * n_bins_eff)\n",
    "        bin_size = n_freqs // n_bins\n",
    "        if bin_size <= 0:\n",
    "            raise ValueError(f\"n_bins={n_bins} is too high for n_freqs={n_freqs}\")\n",
    "        keep_mask = []\n",
    "        for b in range(n_bins):\n",
    "            start = b * bin_size\n",
    "            end = (b + 1) * bin_size - 1\n",
    "            if start < 0 or end >= n_freqs:\n",
    "                keep_mask.append(False)\n",
    "                continue\n",
    "            f_lo = freqs[start]\n",
    "            f_hi = freqs[end]\n",
    "            # Keep this bin if it overlaps the requested range\n",
    "            keep_mask.append(not (f_hi < fmin_sel or f_lo > fmax_sel))\n",
    "        keep_mask = np.asarray(keep_mask, dtype=bool)\n",
    "        if not np.any(keep_mask):\n",
    "            raise ValueError(\n",
    "                f\"PSD_FEATURE_RANGE={PSD_FEATURE_RANGE} excluded all bins for n_bins={n_bins}\"\n",
    "            )\n",
    "        reduced = reduced[:, :, keep_mask]\n",
    "        n_bins_eff = reduced.shape[2]\n",
    "    return reduced.reshape(n_samples, n_channels * n_bins_eff)\n",
    "\n",
    "def _get_fooof_feature_layout(channels):\n",
    "    \"\"\"Return (indices, names) for the selected FOOOF features.\n",
    "\n",
    "    The base per-channel order is [offset, exponent, alpha_cf, alpha_amp, alpha_bw].\n",
    "    FOOOF_SELECTED_FEATURES controls which of these are kept.\n",
    "    \"\"\"\n",
    "    base_order = [\"offset\", \"exponent\", \"alpha_cf\", \"alpha_amp\", \"alpha_bw\"]\n",
    "    try:\n",
    "        selected = list(FOOOF_SELECTED_FEATURES)\n",
    "    except Exception:\n",
    "        selected = base_order\n",
    "    selected_set = {s for s in selected if s in base_order}\n",
    "    if not selected_set:\n",
    "        selected_set = set(base_order)\n",
    "    indices = []\n",
    "    names = []\n",
    "    stride = len(base_order)\n",
    "    for ch_idx, ch in enumerate(channels):\n",
    "        base_col = ch_idx * stride\n",
    "        for offset_idx, feat_name in enumerate(base_order):\n",
    "            if feat_name not in selected_set:\n",
    "                continue\n",
    "            indices.append(base_col + offset_idx)\n",
    "            names.append(f\"{ch}_{feat_name}\")\n",
    "    return indices, names\n",
    "\n",
    "def _smooth_labels_by_run(\n",
    "    labels: np.ndarray,\n",
    "    positions: np.ndarray,\n",
    "    min_run_interior: int,\n",
    "    min_run_edge: int,\n",
    "    use_edge_smoothing: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Run-length smoothing using a time axis.\n",
    "\n",
    "    Interior runs (surrounded on both sides by the same opposite label)\n",
    "    whose span in *positions* is < min_run_interior are flipped.\n",
    "\n",
    "    When use_edge_smoothing is True, short edge runs (at the start or end)\n",
    "    whose span in *positions* is < min_run_edge are replaced by the\n",
    "    majority label within a time window of size min_run_edge.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    positions = np.asarray(positions, dtype=int)\n",
    "    if labels.size == 0 or positions.size != labels.size:\n",
    "        return labels.copy()\n",
    "    # Identify runs as (start, end, value) with end exclusive\n",
    "    runs = []\n",
    "    start = 0\n",
    "    current = labels[0]\n",
    "    for i in range(1, labels.size):\n",
    "        if labels[i] != current:\n",
    "            runs.append((start, i, current))\n",
    "            start = i\n",
    "            current = labels[i]\n",
    "    runs.append((start, labels.size, current))\n",
    "    out = labels.copy()\n",
    "    if not runs:\n",
    "        return out\n",
    "\n",
    "    # Interior runs\n",
    "    if min_run_interior > 1 and len(runs) >= 3:\n",
    "        for idx in range(1, len(runs) - 1):\n",
    "            s, e, val = runs[idx]\n",
    "            prev_val = runs[idx - 1][2]\n",
    "            next_val = runs[idx + 1][2]\n",
    "            # Span in original time index (inclusive)\n",
    "            span = int(positions[e - 1] - positions[s] + 1)\n",
    "            if prev_val == next_val and prev_val != val and span < min_run_interior:\n",
    "                out[s:e] = prev_val\n",
    "\n",
    "    # Edge runs (first and last) by majority vote in a time window\n",
    "    if use_edge_smoothing and min_run_edge > 1:\n",
    "        # First run\n",
    "        if len(runs) >= 1:\n",
    "            s0, e0, v0 = runs[0]\n",
    "            span0 = int(positions[e0 - 1] - positions[s0] + 1)\n",
    "            if span0 < min_run_edge:\n",
    "                t0 = int(positions[s0])\n",
    "                t_edge = t0 + min_run_edge - 1\n",
    "                win_idx = (positions >= t0) & (positions <= t_edge)\n",
    "                if np.any(win_idx):\n",
    "                    labels_win = out[win_idx]\n",
    "                    vals, counts = np.unique(labels_win, return_counts=True)\n",
    "                    maj = int(vals[np.argmax(counts)])\n",
    "                    if maj != v0:\n",
    "                        out[s0:e0] = maj\n",
    "\n",
    "        # Last run\n",
    "        if len(runs) >= 2:\n",
    "            sn, en, vn = runs[-1]\n",
    "            spann = int(positions[en - 1] - positions[sn] + 1)\n",
    "            if spann < min_run_edge:\n",
    "                t_end = int(positions[en - 1])\n",
    "                t_start = t_end - min_run_edge + 1\n",
    "                win_idx = (positions >= t_start) & (positions <= t_end)\n",
    "                if np.any(win_idx):\n",
    "                    labels_win = out[win_idx]\n",
    "                    vals, counts = np.unique(labels_win, return_counts=True)\n",
    "                    maj = int(vals[np.argmax(counts)])\n",
    "                    if maj != vn:\n",
    "                        out[sn:en] = maj\n",
    "\n",
    "    return out\n",
    "\n",
    "def _get_base_feature_names_for_mode(n_bins=None):\n",
    "    \"\"\"Return feature names for the current feature mode.\n",
    "    When USE_FOOOF is True, names reflect per-channel FOOOF parameters.\n",
    "    When USE_FOOOF is False, names reflect binned PSD features.\n",
    "    \"\"\"\n",
    "    if USE_FOOOF:\n",
    "        _, names = _get_fooof_feature_layout(feature_channels)\n",
    "        return names\n",
    "    else:\n",
    "        if n_bins is None:\n",
    "            n_bins = FREQ_BIN_OPTIONS[0]\n",
    "        freqs = np.asarray(PSD_META['freqs'], float)\n",
    "        n_freqs = freqs.size\n",
    "        if n_freqs == 0:\n",
    "            return []\n",
    "        bin_size = n_freqs // n_bins\n",
    "        if bin_size <= 0:\n",
    "            raise ValueError(f\"n_bins={n_bins} is too high for n_freqs={n_freqs}\")\n",
    "        names = []\n",
    "        for ch in PSD_META['channels']:\n",
    "            for b in range(n_bins):\n",
    "                start = b * bin_size\n",
    "                end = (b + 1) * bin_size - 1\n",
    "                if start < 0 or end >= n_freqs:\n",
    "                    continue\n",
    "                f_lo = freqs[start]\n",
    "                f_hi = freqs[end]\n",
    "                if PSD_FEATURE_RANGE is not None:\n",
    "                    fmin_sel, fmax_sel = PSD_FEATURE_RANGE\n",
    "                    if f_hi < fmin_sel or f_lo > fmax_sel:\n",
    "                        continue\n",
    "                names.append(f\"{ch}_PSD_{f_lo:.2f}-{f_hi:.2f}Hz\")\n",
    "        return names\n",
    "\n",
    "def _build_outer_plan(subjects):\n",
    "    group_size = min(max(1, CV_TEST_SUBJECTS_PER_SPLIT), subjects.size)\n",
    "    repeats = 1 if CV_LEVEL in (1, 2) else max(1, CV_REPEAT_COUNT)\n",
    "    plan = []\n",
    "    rng = np.random.default_rng(CV_RANDOM_SEED)\n",
    "    if CV_LEVEL == 1:\n",
    "        test_subjects = rng.choice(subjects, size=group_size, replace=False)\n",
    "        plan.append({\"repeat\": 0, \"fold\": 1, \"test_subjects\": np.sort(test_subjects)})\n",
    "        return plan\n",
    "    if CV_LEVEL == 4:\n",
    "        # Fixed test split based on configured subject IDs\n",
    "        fixed = np.array(FIXED_TEST_SUBJECTS_LEVEL4, dtype=int)\n",
    "        available = np.intersect1d(subjects, fixed)\n",
    "        if available.size == 0:\n",
    "            raise ValueError(\"CV_LEVEL=4: none of FIXED_TEST_SUBJECTS_LEVEL4 are present in this dataset.\")\n",
    "        plan.append({\"repeat\": 0, \"fold\": 1, \"test_subjects\": np.sort(available)})\n",
    "        return plan\n",
    "    for rep in range(repeats):\n",
    "        perm = subjects.copy()\n",
    "        rng_rep = np.random.default_rng(CV_RANDOM_SEED + rep)\n",
    "        rng_rep.shuffle(perm)\n",
    "        start = 0\n",
    "        fold_idx = 0\n",
    "        while start < perm.size:\n",
    "            fold_idx += 1\n",
    "            group = np.sort(perm[start:start + group_size])\n",
    "            plan.append({\"repeat\": rep, \"fold\": fold_idx, \"test_subjects\": group})\n",
    "            start += group_size\n",
    "    return plan\n",
    "\n",
    "def _run_logreg_fold(plan_entry):\n",
    "    test_subjects_fold = plan_entry['test_subjects']\n",
    "    train_subjects_fold = np.setdiff1d(unique_subjects, test_subjects_fold)\n",
    "    train_idx_fold = np.where(np.isin(subject_ids, train_subjects_fold))[0]\n",
    "    test_idx_fold = np.where(np.isin(subject_ids, test_subjects_fold))[0]\n",
    "    if train_idx_fold.size == 0 or test_idx_fold.size == 0:\n",
    "        raise ValueError(\"Empty train/test split encountered. Adjust CV settings.\")\n",
    "    y_train_fold = y_combined[train_idx_fold]\n",
    "    y_test_fold = y_combined[test_idx_fold]\n",
    "    train_subj_ids = subject_ids[train_idx_fold]\n",
    "    if train_subj_ids.size < 2:\n",
    "        raise ValueError(\"Need at least two training subjects for the inner LOSO loop.\")\n",
    "\n",
    "    if USE_FOOOF:\n",
    "        # Select only the requested FOOOF feature types\n",
    "        fooof_indices, _ = _get_fooof_feature_layout(feature_channels)\n",
    "        if not fooof_indices:\n",
    "            raise RuntimeError(\n",
    "                \"FOOOF_SELECTED_FEATURES produced no valid features; \"\n",
    "                \"check FOOOF_SELECTED_FEATURES at the top of the notebook.\"\n",
    "            )\n",
    "        base_train = X_combined[train_idx_fold]\n",
    "        base_test = X_combined[test_idx_fold]\n",
    "        train_matrix = base_train[:, fooof_indices]\n",
    "        test_matrix = base_test[:, fooof_indices]\n",
    "        train_cache = {None: train_matrix}\n",
    "        test_cache = {None: test_matrix}\n",
    "    else:\n",
    "        train_psd = psd_cube[train_idx_fold]\n",
    "        test_psd = psd_cube[test_idx_fold]\n",
    "        train_cache = {}\n",
    "        test_cache = {}\n",
    "\n",
    "        def _get_cache(cache, data_block, n_bins):\n",
    "            if n_bins not in cache:\n",
    "                cache[n_bins] = _prepare_psd_matrix(data_block, n_bins)\n",
    "            return cache[n_bins]\n",
    "    outer_loo = LeaveOneOut()\n",
    "    inner_subjects = np.unique(train_subj_ids)\n",
    "    best_C_list, best_bins_list, best_penalty_list = [], [], []\n",
    "    val_records = []\n",
    "    for _, (inner_train_idx, val_sub_idx) in enumerate(outer_loo.split(inner_subjects), start=1):\n",
    "        inner_train_subjects = inner_subjects[inner_train_idx]\n",
    "        val_subject = inner_subjects[val_sub_idx[0]]\n",
    "        mask_train = np.where(np.isin(train_subj_ids, inner_train_subjects))[0]\n",
    "        mask_val = np.where(train_subj_ids == val_subject)[0]\n",
    "        best_score = -np.inf\n",
    "        best_params = None\n",
    "        best_val_acc = 0.0\n",
    "        for C in C_grid:\n",
    "            for n_bins in n_bins_grid:\n",
    "                if USE_FOOOF:\n",
    "                    X_inner = train_matrix[mask_train]\n",
    "                    X_val = train_matrix[mask_val]\n",
    "                else:\n",
    "                    n_bins_eval = n_bins if n_bins is not None else FREQ_BIN_OPTIONS[0]\n",
    "                    X_inner = _get_cache(train_cache, train_psd, n_bins_eval)[mask_train]\n",
    "                    X_val = _get_cache(train_cache, train_psd, n_bins_eval)[mask_val]\n",
    "\n",
    "                imputer = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "                scaler = StandardScaler()\n",
    "                X_inner_imp = imputer.fit_transform(X_inner)\n",
    "                X_val_imp = imputer.transform(X_val)\n",
    "                X_inner_scaled = scaler.fit_transform(X_inner_imp)\n",
    "                X_val_scaled = scaler.transform(X_val_imp)\n",
    "\n",
    "                # Optional component analysis (PCA/ICA) in the inner loop\n",
    "                X_inner_proc = X_inner_scaled\n",
    "                X_val_proc = X_val_scaled\n",
    "                if USE_COMPONENT_ANALYSIS:\n",
    "                    method = str(COMPONENT_METHOD).lower()\n",
    "                    n_features_here = X_inner_scaled.shape[1]\n",
    "                    max_default = min(50, n_features_here)\n",
    "                    n_comp = COMPONENT_N_COMPONENTS if COMPONENT_N_COMPONENTS is not None else max_default\n",
    "                    n_comp = max(1, min(int(n_comp), n_features_here))\n",
    "\n",
    "                    if method == \"pca\":\n",
    "                        comp_model_inner = PCA(n_components=n_comp, random_state=CV_RANDOM_SEED)\n",
    "                    elif method == \"ica\":\n",
    "                        comp_model_inner = FastICA(n_components=n_comp, random_state=CV_RANDOM_SEED, max_iter=500)\n",
    "                    else:\n",
    "                        comp_model_inner = None\n",
    "\n",
    "                    if comp_model_inner is not None:\n",
    "                        X_inner_proc = comp_model_inner.fit_transform(X_inner_scaled)\n",
    "                        X_val_proc = comp_model_inner.transform(X_val_scaled)\n",
    "\n",
    "                for penalty in penalty_grid:\n",
    "                    try:\n",
    "                        clf = _make_logreg(C, penalty)\n",
    "                        clf.fit(X_inner_proc, y_train_fold[mask_train])\n",
    "                        probs = clf.predict_proba(X_val_proc)\n",
    "                        score = -log_loss(y_train_fold[mask_val], probs)\n",
    "                        val_acc = accuracy_score(y_train_fold[mask_val], clf.predict(X_val_proc))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\"C\": C, \"n_bins\": n_bins, \"penalty\": penalty}\n",
    "                        best_val_acc = val_acc\n",
    "\n",
    "        if best_params is None:\n",
    "            raise RuntimeError(\"Inner loop failed to find hyper-parameters.\")\n",
    "        best_C_list.append(best_params['C'])\n",
    "        if best_params.get('n_bins', None) is not None:\n",
    "            best_bins_list.append(best_params['n_bins'])\n",
    "        best_penalty_list.append(str(best_params.get('penalty', 'l2')).lower())\n",
    "        val_records.append({\"subject\": int(val_subject), \"accuracy\": float(best_val_acc)})\n",
    "\n",
    "    selected_C_fold = Counter(best_C_list).most_common(1)[0][0]\n",
    "    selected_penalty_fold = Counter(best_penalty_list).most_common(1)[0][0] if best_penalty_list else penalty_grid[0]\n",
    "    if best_bins_list:\n",
    "        selected_bins_fold = Counter(best_bins_list).most_common(1)[0][0]\n",
    "    else:\n",
    "        selected_bins_fold = None\n",
    "    if USE_FOOOF:\n",
    "        X_train_features = train_matrix\n",
    "        X_test_features = test_matrix\n",
    "    else:\n",
    "        n_bins_final = selected_bins_fold if selected_bins_fold is not None else FREQ_BIN_OPTIONS[0]\n",
    "        X_train_features = _get_cache(train_cache, train_psd, n_bins_final)\n",
    "        X_test_features = _get_cache(test_cache, test_psd, n_bins_final)\n",
    "\n",
    "    imputer_final = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "    scaler_final = StandardScaler()\n",
    "    X_train_imp = imputer_final.fit_transform(X_train_features)\n",
    "    X_test_imp = imputer_final.transform(X_test_features)\n",
    "    X_train_scaled = scaler_final.fit_transform(X_train_imp)\n",
    "    X_test_scaled = scaler_final.transform(X_test_imp)\n",
    "\n",
    "    # Diagnostics: feature names before component analysis / PCA\n",
    "    base_feature_names = _get_base_feature_names_for_mode(\n",
    "        selected_bins_fold if (not USE_FOOOF) else None\n",
    "    )\n",
    "    print(\n",
    "        f\"Fold r{plan_entry['repeat']} f{plan_entry['fold']} - input features for component analysis/logistic (n={len(base_feature_names)}):\"\n",
    "    )\n",
    "    for fname in base_feature_names:\n",
    "        print(f\"  {fname}\")\n",
    "\n",
    "    # Optional component analysis for the final model in this fold\n",
    "    X_train_final = X_train_scaled\n",
    "    X_test_final = X_test_scaled\n",
    "    component_model_final = None\n",
    "    component_method_final = None\n",
    "    if USE_COMPONENT_ANALYSIS:\n",
    "        method = str(COMPONENT_METHOD).lower()\n",
    "        component_method_final = method\n",
    "        n_features_here = X_train_scaled.shape[1]\n",
    "        max_default = min(50, n_features_here)\n",
    "        n_comp = COMPONENT_N_COMPONENTS if COMPONENT_N_COMPONENTS is not None else max_default\n",
    "        n_comp = max(1, min(int(n_comp), n_features_here))\n",
    "\n",
    "        if method == \"pca\":\n",
    "            component_model_final = PCA(n_components=n_comp, random_state=CV_RANDOM_SEED)\n",
    "        elif method == \"ica\":\n",
    "            component_model_final = FastICA(n_components=n_comp, random_state=CV_RANDOM_SEED, max_iter=500)\n",
    "\n",
    "        if component_model_final is not None:\n",
    "            X_train_final = component_model_final.fit_transform(X_train_scaled)\n",
    "            X_test_final = component_model_final.transform(X_test_scaled)\n",
    "\n",
    "    # Diagnostics: feature names actually used for logistic regression training\n",
    "    if component_model_final is not None and component_method_final in (\"pca\", \"ica\"):\n",
    "        prefix = \"PC\" if component_method_final == \"pca\" else \"IC\"\n",
    "        training_feature_names = [f\"{prefix}{i+1}\" for i in range(X_train_final.shape[1])]\n",
    "    else:\n",
    "        training_feature_names = base_feature_names\n",
    "    print(\n",
    "        f\"Fold r{plan_entry['repeat']} f{plan_entry['fold']} - features used for logistic regression training (n={len(training_feature_names)}):\"\n",
    "    )\n",
    "    for fname in training_feature_names:\n",
    "        print(f\"  {fname}\")\n",
    "\n",
    "    clf_final = _make_logreg(selected_C_fold, selected_penalty_fold)\n",
    "    clf_final.fit(X_train_final, y_train_fold)\n",
    "    y_pred_fold = clf_final.predict(X_test_final)\n",
    "    y_proba_fold = clf_final.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    fold_result = {\n",
    "        \"repeat\": plan_entry['repeat'],\n",
    "        \"fold\": plan_entry['fold'],\n",
    "        \"test_subjects\": test_subjects_fold,\n",
    "        \"train_subjects\": train_subjects_fold,\n",
    "        \"train_idx\": train_idx_fold,\n",
    "        \"test_idx\": test_idx_fold,\n",
    "        \"y_test\": y_test_fold,\n",
    "        \"y_pred\": y_pred_fold,\n",
    "        \"y_proba\": y_proba_fold,\n",
    "        \"subject_ids\": subject_ids[test_idx_fold],\n",
    "        \"time_idx\": epoch_time_indices[test_idx_fold],\n",
    "        \"acc\": accuracy_score(y_test_fold, y_pred_fold),\n",
    "        \"conf_matrix\": confusion_matrix(y_test_fold, y_pred_fold),\n",
    "        \"selected_C\": selected_C_fold,\n",
    "        \"selected_penalty\": selected_penalty_fold,\n",
    "        \"selected_n_bins\": selected_bins_fold,\n",
    "        \"imputer\": imputer_final,\n",
    "        \"scaler\": scaler_final,\n",
    "        \"model\": clf_final,\n",
    "        \"component_model\": component_model_final,\n",
    "        \"X_train_features\": X_train_features,\n",
    "        \"X_test_features\": X_test_features,\n",
    "        \"val_records\": val_records,\n",
    "    }\n",
    "    return fold_result\n",
    "\n",
    "# Decide how to form the outer test set.\n",
    "# - Default: subject-wise CV on the currently loaded dataset(s)\n",
    "# - TEST_ON_OTHER_DATASET: train on primary (NEW_DATA) and test on the other dataset as a single held-out fold\n",
    "subjects_new = np.array(sorted({int(r['subject']) for r in records if str(r.get('dataset', '')) == 'new'}), dtype=int)\n",
    "subjects_old = np.array(sorted({int(r['subject']) for r in records if str(r.get('dataset', '')) == 'old'}), dtype=int)\n",
    "\n",
    "if (CROSS_DATASET_TEST or USE_BOTH_DATASETS) and CV_LEVEL == 4:\n",
    "    raise ValueError(\n",
    "        \"CV_LEVEL=4 uses a fixed list of subject IDs; when mixing datasets (offset IDs), this is ambiguous. \"\n",
    "        \"Use CV_LEVEL=1/2/3, or update FIXED_TEST_SUBJECTS_LEVEL4 to match the combined subject_ids.\"\n",
    "    )\n",
    "\n",
    "if CROSS_DATASET_TEST:\n",
    "    if NEW_DATA:\n",
    "        test_subjects_external = subjects_old\n",
    "        if test_subjects_external.size == 0:\n",
    "            raise RuntimeError(\"CROSS_DATASET_TEST=True but OLD dataset subjects are empty. Resolve .set paths or disable the toggle.\")\n",
    "    else:\n",
    "        test_subjects_external = subjects_new\n",
    "        if test_subjects_external.size == 0:\n",
    "            raise RuntimeError(\"CROSS_DATASET_TEST=True but NEW dataset subjects are empty. Resolve processed .fif paths or disable the toggle.\")\n",
    "    outer_plan = [{\"repeat\": 0, \"fold\": 1, \"test_subjects\": np.sort(test_subjects_external)}]\n",
    "else:\n",
    "    outer_plan = _build_outer_plan(unique_subjects)\n",
    "print(f\"Running logistic regression with {len(outer_plan)} outer folds (level={CV_LEVEL}).\")\n",
    "logreg_cv_folds = []\n",
    "val_subject_buffer, val_accuracy_buffer = [], []\n",
    "for plan_entry in outer_plan:\n",
    "    fold_result = _run_logreg_fold(plan_entry)\n",
    "    logreg_cv_folds.append(fold_result)\n",
    "    val_subject_buffer.extend([rec['subject'] for rec in fold_result['val_records']])\n",
    "    val_accuracy_buffer.extend([rec['accuracy'] for rec in fold_result['val_records']])\n",
    "    print(f\"  Fold r{plan_entry['repeat']} f{plan_entry['fold']}: test subjects {fold_result['test_subjects']} — acc={fold_result['acc']:.3f}\")\n",
    "if not logreg_cv_folds:\n",
    "    raise RuntimeError(\"Logistic regression did not run any folds. Check CV configuration.\")\n",
    "\n",
    "# ---- Summary of feature and component counts ----\n",
    "base_feature_names = _get_base_feature_names_for_mode(\n",
    "    None if USE_FOOOF else (logreg_cv_folds[0]['selected_n_bins'] or FREQ_BIN_OPTIONS[0])\n",
    ")\n",
    "n_base_features = len(base_feature_names)\n",
    "primary_fold = max(logreg_cv_folds, key=lambda fold: fold['acc'])\n",
    "component_model = primary_fold.get('component_model')\n",
    "if component_model is not None and USE_COMPONENT_ANALYSIS:\n",
    "    if hasattr(component_model, 'components_'):\n",
    "        n_components_fitted = component_model.components_.shape[0]\n",
    "    elif hasattr(component_model, 'n_components_'):\n",
    "        n_components_fitted = int(component_model.n_components_)\n",
    "    else:\n",
    "        n_components_fitted = primary_fold['X_train_features'].shape[1]\n",
    "    # The logistic model sees all columns of X_train_final\n",
    "    n_components_used = primary_fold['model'].coef_.shape[1]\n",
    "else:\n",
    "    n_components_fitted = 0\n",
    "    n_components_used = 0\n",
    "print(\"\\n[Logistic regression summary]\")\n",
    "print(f\"  Total base input features (before PCA/ICA): {n_base_features}\")\n",
    "if USE_COMPONENT_ANALYSIS and component_model is not None:\n",
    "    print(f\"  Components fitted in best fold: {n_components_fitted}\")\n",
    "    print(f\"  Components used by logistic model: {n_components_used}\")\n",
    "else:\n",
    "    print(\"  Component analysis disabled for the final model (using base features directly).\")\n",
    "\n",
    "np.save(outpath(\"val_subject_ids.npy\"), np.array(val_subject_buffer, dtype=int))\n",
    "np.save(outpath(\"val_accuracies.npy\"), np.array(val_accuracy_buffer, dtype=float))\n",
    "max_group = max(len(entry['test_subjects']) for entry in outer_plan)\n",
    "cv_matrix = -np.ones((len(outer_plan), max_group), dtype=int)\n",
    "for row, entry in enumerate(outer_plan):\n",
    "    arr = entry['test_subjects']\n",
    "    cv_matrix[row, :arr.size] = arr\n",
    "np.save(outpath(\"cv_test_subjects.npy\"), cv_matrix)\n",
    "primary_fold = max(logreg_cv_folds, key=lambda fold: fold['acc'])\n",
    "logreg_primary_fold = primary_fold\n",
    "logreg_primary_test_subjects = primary_fold['test_subjects']\n",
    "logreg_covered_subjects = np.unique(np.concatenate([fold['subject_ids'] for fold in logreg_cv_folds]))\n",
    "\n",
    "# Persist model artefacts from the best-performing outer fold\n",
    "final_model = primary_fold['model']\n",
    "final_scaler = primary_fold['scaler']\n",
    "final_imputer = primary_fold['imputer']\n",
    "final_component = primary_fold.get('component_model')\n",
    "final_C = primary_fold['selected_C']\n",
    "final_n_bins = primary_fold['selected_n_bins']\n",
    "np.save(outpath(\"test_subjects.npy\"), primary_fold['test_subjects'])\n",
    "np.save(outpath(\"selected_C_lr.npy\"), np.array([final_C], dtype=float))\n",
    "if final_n_bins is not None:\n",
    "    np.save(outpath(\"final_n_bins_lr.npy\"), np.array([final_n_bins], dtype=int))\n",
    "joblib.dump(final_model, outpath(\"final_model_lr.pkl\"))\n",
    "joblib.dump(final_scaler, outpath(\"final_scaler_lr.pkl\"))\n",
    "joblib.dump(final_imputer, outpath(\"final_imputer_lr.pkl\"))\n",
    "if final_component is not None:\n",
    "    joblib.dump(final_component, outpath(\"final_component_lr.pkl\"))\n",
    "\n",
    "# Assemble aggregated predictions across all folds for downstream diagnostics\n",
    "test_idx = np.concatenate([fold['test_idx'] for fold in logreg_cv_folds])\n",
    "y_test = np.concatenate([fold['y_test'] for fold in logreg_cv_folds])\n",
    "y_pred = np.concatenate([fold['y_pred'] for fold in logreg_cv_folds])\n",
    "y_proba = np.concatenate([fold['y_proba'] for fold in logreg_cv_folds])\n",
    "# X_test features may have different widths across folds (different n_bins).\n",
    "# Try to stack; if shapes differ, keep as a list for QC stats.\n",
    "try:\n",
    "    X_test = np.vstack([fold['X_test_features'] for fold in logreg_cv_folds])\n",
    "except Exception:\n",
    "    X_test = [fold['X_test_features'] for fold in logreg_cv_folds]\n",
    "time_idx_all = np.concatenate([fold['time_idx'] for fold in logreg_cv_folds])\n",
    "logreg_predictions_df = pd.DataFrame({\n",
    "    'fold': [f\"r{fold['repeat']}_f{fold['fold']}\" for fold in logreg_cv_folds for _ in range(fold['y_test'].size)],\n",
    "    'subject_id': np.concatenate([fold['subject_ids'] for fold in logreg_cv_folds]),\n",
    "    'epoch_idx': test_idx,\n",
    "    'time_idx': time_idx_all,\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred,\n",
    "    'prob_ec': y_proba,\n",
    "})\n",
    "train_idx = primary_fold['train_idx']\n",
    "train_subjects = primary_fold['train_subjects']\n",
    "test_subjects = logreg_covered_subjects\n",
    "y_train = y_combined[train_idx]\n",
    "X_train = primary_fold['X_train_features']\n",
    "\n",
    "# Optional: tune run-length smoothing length on out-of-fold predictions\n",
    "RUN_LENGTH_TUNED = False\n",
    "if USE_TIME_ADJUSTMENT and LENGTH_TUNING:\n",
    "    from sklearn.metrics import accuracy_score as _acc_score\n",
    "    from sklearn.metrics import balanced_accuracy_score as _bal_acc_score\n",
    "\n",
    "    def _score_smoothing(y_true_arr, y_pred_arr):\n",
    "        metric = str(LENGTH_TUNING_METRIC).lower().strip()\n",
    "        if metric == \"accuracy\":\n",
    "            return float(_acc_score(y_true_arr, y_pred_arr))\n",
    "        if metric == \"balanced_accuracy\":\n",
    "            return float(_bal_acc_score(y_true_arr, y_pred_arr))\n",
    "        raise ValueError(f\"Unknown LENGTH_TUNING_METRIC: {LENGTH_TUNING_METRIC!r}\")\n",
    "\n",
    "    def _smooth_all_with(L: int) -> np.ndarray:\n",
    "        out = np.empty_like(logreg_predictions_df['y_pred'].to_numpy())\n",
    "        for subj in np.unique(logreg_predictions_df['subject_id']):\n",
    "            mask = logreg_predictions_df['subject_id'] == subj\n",
    "            df_subj = logreg_predictions_df.loc[mask].sort_values('time_idx')\n",
    "            smoothed = _smooth_labels_by_run(\n",
    "                df_subj['y_pred'].to_numpy(),\n",
    "                df_subj['time_idx'].to_numpy(),\n",
    "                int(L),\n",
    "                int(L),\n",
    "                use_edge_smoothing=USE_EDGE_SMOOTHING,\n",
    "            )\n",
    "            out[df_subj.index.to_numpy()] = smoothed\n",
    "        return out\n",
    "\n",
    "    grid = [int(x) for x in (LENGTH_GRID if LENGTH_GRID is not None else [])]\n",
    "    grid = sorted({x for x in grid if x >= 1})\n",
    "    if not grid:\n",
    "        grid = [1]\n",
    "    best_L = None\n",
    "    best_score = -1.0\n",
    "    y_true_all = logreg_predictions_df['y_true'].to_numpy()\n",
    "    for L in grid:\n",
    "        y_sm = _smooth_all_with(L)\n",
    "        s = _score_smoothing(y_true_all, y_sm)\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_L = L\n",
    "    MIN_RUN_LENGTH = int(best_L)\n",
    "    MIN_RUN_LENGTH_EDGE = int(best_L)\n",
    "    RUN_LENGTH_TUNED = True\n",
    "    print(f\"[Temporal smoothing tuning] metric={LENGTH_TUNING_METRIC}, grid={grid} -> selected L={best_L} (score={best_score:.3f})\")\n",
    "else:\n",
    "    if USE_TIME_ADJUSTMENT:\n",
    "        print(f\"[Temporal smoothing] tuning disabled; using MIN_RUN_LENGTH={MIN_RUN_LENGTH}, MIN_RUN_LENGTH_EDGE={MIN_RUN_LENGTH_EDGE}\")\n",
    "\n",
    "# Expose the selected smoothing lengths for later display\n",
    "SELECTED_RUN_LENGTH = int(MIN_RUN_LENGTH) if USE_TIME_ADJUSTMENT else None\n",
    "SELECTED_RUN_LENGTH_EDGE = int(MIN_RUN_LENGTH_EDGE) if USE_TIME_ADJUSTMENT else None\n",
    "\n",
    "# Optional temporal smoothing of predicted labels\n",
    "if USE_TIME_ADJUSTMENT:\n",
    "    smoothed_all = np.empty_like(y_pred)\n",
    "    for subj in np.unique(logreg_predictions_df['subject_id']):\n",
    "        mask = logreg_predictions_df['subject_id'] == subj\n",
    "        df_subj = logreg_predictions_df.loc[mask].sort_values('time_idx')\n",
    "        smoothed = _smooth_labels_by_run(\n",
    "            df_subj['y_pred'].to_numpy(),\n",
    "            df_subj['time_idx'].to_numpy(),\n",
    "            MIN_RUN_LENGTH,\n",
    "            MIN_RUN_LENGTH_EDGE,\n",
    "            use_edge_smoothing=USE_EDGE_SMOOTHING,\n",
    "        )\n",
    "        # map back into the global array using the sorted index positions\n",
    "        smoothed_all[df_subj.index.to_numpy()] = smoothed\n",
    "    logreg_predictions_df['y_pred_smooth'] = smoothed_all\n",
    "else:\n",
    "    logreg_predictions_df['y_pred_smooth'] = logreg_predictions_df['y_pred'].to_numpy()\n",
    "\n",
    "# Choose which predictions to use for evaluation\n",
    "if USE_TIME_ADJUSTMENT:\n",
    "    eval_pred = logreg_predictions_df['y_pred_smooth'].to_numpy()\n",
    "else:\n",
    "    eval_pred = y_pred\n",
    "acc = accuracy_score(y_test, eval_pred)\n",
    "report = classification_report(y_test, eval_pred)\n",
    "conf_matrix = confusion_matrix(y_test, eval_pred)\n",
    "\n",
    "# Save per-epoch probabilities for this configuration\n",
    "config_tag = \"fooof\" if USE_FOOOF else \"psd\"\n",
    "np.save(outpath(f\"{config_tag}_epoch_idx.npy\"), test_idx)\n",
    "np.save(outpath(f\"{config_tag}_time_idx.npy\"), epoch_time_indices[test_idx])\n",
    "np.save(outpath(f\"{config_tag}_y_true.npy\"), y_test)\n",
    "np.save(outpath(f\"{config_tag}_prob_ec.npy\"), y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d885cd",
   "metadata": {},
   "source": [
    "### Temporal smoothing: raw vs smoothed predictions\n",
    "\n",
    "If `USE_TIME_ADJUSTMENT=True`, the training cell can create a *smoothed* prediction per epoch based on run-length rules.\n",
    "\n",
    "This plotting cell:\n",
    "- Visualizes per-subject prediction timelines.\n",
    "- Lets you compare raw predictions vs smoothed predictions.\n",
    "\n",
    "It requires outputs from the logistic regression CV cell (predictions + time indices).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Temporal label plots: raw vs smoothed ----\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'logreg_predictions_df' not in globals():\n",
    "    raise RuntimeError(\"Run the logistic regression CV cell first to populate logreg_predictions_df.\")\n",
    "\n",
    "# Ensure we have the smoothed column available\n",
    "if 'y_pred_smooth' not in logreg_predictions_df.columns:\n",
    "    logreg_predictions_df['y_pred_smooth'] = logreg_predictions_df['y_pred'].to_numpy()\n",
    "\n",
    "# Plot configuration: choose subjects and time window\n",
    "# plot_subjects can be \"all\" or a list like [23, 24]\n",
    "plot_subjects = \"all\" # [10213, 10175, 10139, 10136, 10135, 10002]\n",
    "# time_xlim can be None or a tuple like (0, 1000)\n",
    "time_xlim = None\n",
    "\n",
    "subjects = {}\n",
    "for sid in np.sort(logreg_predictions_df['subject_id'].unique()):\n",
    "    subjects[int(sid)] = None\n",
    "\n",
    "if isinstance(plot_subjects, str) and plot_subjects.strip().lower() == \"all\":\n",
    "    subject_ids_sorted = sorted(subjects.keys())\n",
    "else:\n",
    "    try:\n",
    "        subj_list = list(plot_subjects)\n",
    "    except TypeError:\n",
    "        subj_list = [plot_subjects]\n",
    "    subject_ids_sorted = [int(s) for s in subj_list]\n",
    "\n",
    "colors = {0: \"blue\", 1: \"red\"}\n",
    "time_xlabel = {\n",
    "    \"append_files\": \"Epoch timeline index (files appended)\",\n",
    "    \"align_conditions\": \"Epoch number (EO/EC aligned)\",\n",
    "    \"interleave_conditions\": \"Epoch timeline index (EO→EC interleaved)\",\n",
    "}.get(TIME_AXIS_MODE, \"Epoch timeline index\")\n",
    "\n",
    "# Helper to build a figure for a given column name\n",
    "\n",
    "def _plot_labels_per_subject(column_name: str, title: str):\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    for i, subj_id in enumerate(subject_ids_sorted):\n",
    "        df_subj = logreg_predictions_df[logreg_predictions_df['subject_id'] == subj_id].copy()\n",
    "        df_subj = df_subj.sort_values('time_idx')\n",
    "        x_base = df_subj['time_idx'].to_numpy()\n",
    "        y = np.full_like(x_base, i)\n",
    "        labels = df_subj[column_name].to_numpy()\n",
    "        for label_val in [0, 1]:\n",
    "            idx = labels == label_val\n",
    "            if not np.any(idx):\n",
    "                continue\n",
    "            x = x_base\n",
    "            if TIME_AXIS_MODE == \"align_conditions\":\n",
    "                x = x_base + (-0.15 if label_val == 0 else 0.15)\n",
    "            ax.plot(x[idx], y[idx], 'o', color=colors[label_val], markersize=4)\n",
    "    ax.set_yticks(range(len(subject_ids_sorted)))\n",
    "    ax.set_yticklabels(subject_ids_sorted)\n",
    "    if time_xlim is not None:\n",
    "        ax.set_xlim(time_xlim)\n",
    "    ax.set_xlabel(time_xlabel)\n",
    "    ax.set_ylabel(\"Subject ID\")\n",
    "    ax.set_title(title)\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label='Eyes Open', markerfacecolor='blue', markersize=6),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label='Eyes Closed', markerfacecolor='red', markersize=6),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, title=\"Epoch Labels\", bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot raw predictions\n",
    "_plot_labels_per_subject('y_pred', \"Raw classifier labels per subject\")\n",
    "\n",
    "# Plot smoothed predictions\n",
    "_plot_labels_per_subject('y_pred_smooth', f\"Smoothed classifier labels per subject (min_run_length={MIN_RUN_LENGTH}, USE_TIME_ADJUSTMENT={USE_TIME_ADJUSTMENT})\")\n",
    "\n",
    "# Plot true labels\n",
    "_plot_labels_per_subject('y_true', \"True labels per subject\")\n",
    "\n",
    "# ---- Per-subject zoom plots (raw vs smoothed) ----\n",
    "# Set a subject ID here to inspect in detail\n",
    "zoom_subject_id = subject_ids_sorted[0] if subject_ids_sorted else None\n",
    "if zoom_subject_id is not None:\n",
    "    df_zoom = logreg_predictions_df[logreg_predictions_df['subject_id'] == zoom_subject_id].copy()\n",
    "    df_zoom = df_zoom.sort_values('time_idx')\n",
    "    x_base = df_zoom['time_idx'].to_numpy()\n",
    "    raw = df_zoom['y_pred'].to_numpy()\n",
    "    smooth = df_zoom['y_pred_smooth'].to_numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 4))\n",
    "    for label_val, marker, label_name in [(0, 'o', 'EO raw'), (1, 'o', 'EC raw')]:\n",
    "        idx = raw == label_val\n",
    "        if np.any(idx):\n",
    "            x = x_base\n",
    "            if TIME_AXIS_MODE == \"align_conditions\":\n",
    "                x = x_base + (-0.15 if label_val == 0 else 0.15)\n",
    "            ax.plot(x[idx], raw[idx] + 0.0, marker, color=colors[label_val], markersize=4, linestyle='None', label=label_name)\n",
    "    for label_val, marker, label_name in [(0, 'x', 'EO smooth'), (1, 'x', 'EC smooth')]:\n",
    "        idx = smooth == label_val\n",
    "        if np.any(idx):\n",
    "            x = x_base\n",
    "            if TIME_AXIS_MODE == \"align_conditions\":\n",
    "                x = x_base + (-0.15 if label_val == 0 else 0.15)\n",
    "            ax.plot(x[idx], smooth[idx] + 0.1, marker, color=colors[label_val], markersize=5, linestyle='None', label=label_name)\n",
    "    ax.set_xlabel(time_xlabel)\n",
    "    ax.set_ylabel(\"Label\")\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels(['EO', 'EC'])\n",
    "    ax.set_title(f\"Raw vs smoothed labels for subject {zoom_subject_id}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No subjects available for zoom plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f318df",
   "metadata": {},
   "source": [
    "### Component analysis helper: PCA elbow plot\n",
    "\n",
    "When `USE_COMPONENT_ANALYSIS=True`, features are transformed with PCA/ICA before classification.\n",
    "\n",
    "This cell:\n",
    "- Fits PCA on the (imputed + scaled) **training** features from the primary fold.\n",
    "- Plots cumulative explained variance vs number of components.\n",
    "\n",
    "Use it to choose `COMPONENT_N_COMPONENTS` (e.g., the smallest k that explains ~90% variance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0791dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Elbow plot for component analysis (PCA) ----\n",
    "if not USE_COMPONENT_ANALYSIS:\n",
    "    print(\"Component analysis is disabled (USE_COMPONENT_ANALYSIS=False). Enable it in the config cell to use PCA/ICA.\")\n",
    "else:\n",
    "    if 'primary_fold' not in globals():\n",
    "        raise RuntimeError(\"Run the logistic regression CV cell first.\")\n",
    "\n",
    "    X_train_raw = primary_fold['X_train_features']\n",
    "    imputer = primary_fold['imputer']\n",
    "    scaler = primary_fold['scaler']\n",
    "\n",
    "    X_train_imp = imputer.transform(X_train_raw)\n",
    "    X_train_scaled = scaler.transform(X_train_imp)\n",
    "\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    max_components = min(ELBOW_MAX_COMPONENTS, n_features)\n",
    "\n",
    "    pca_elbow = PCA(n_components=max_components, random_state=CV_RANDOM_SEED)\n",
    "    pca_elbow.fit(X_train_scaled)\n",
    "    cumvar = np.cumsum(pca_elbow.explained_variance_ratio_)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(1, max_components + 1), cumvar, marker='o')\n",
    "    plt.axhline(0.9, color='gray', linestyle='--', label='90% variance')\n",
    "    plt.xlabel('Number of PCA components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.title('Elbow plot for PCA on training features')\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print('Example: choose COMPONENT_N_COMPONENTS to the smallest k where the curve bends or cumulative variance reaches around 0.9.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1963c4",
   "metadata": {},
   "source": [
    "### Export: per-fold CV summary\n",
    "\n",
    "This cell collects fold-level results from `logreg_cv_folds` into a single table.\n",
    "\n",
    "It typically includes:\n",
    "- Fold identifiers (repeat/fold).\n",
    "- Test subject sets.\n",
    "- Accuracy and selected hyperparameters.\n",
    "\n",
    "It saves the summary as a CSV under the current `outputs/<config_tag>/` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the per-fold outer-CV metrics in a table and persist them for later inspection.\n",
    "# Also report how much TIME_ADJUSTMENT (run-length smoothing) changes accuracy.\n",
    "if 'logreg_cv_folds' not in globals():\n",
    "    raise RuntimeError(\"Run the logistic regression cell before requesting a CV summary.\")\n",
    "\n",
    "summary_rows = []\n",
    "for fold in logreg_cv_folds:\n",
    "    summary_rows.append({\n",
    "        'repeat': fold['repeat'],\n",
    "        'fold': fold['fold'],\n",
    "        'test_subjects': ','.join(map(str, np.sort(fold['test_subjects']))),\n",
    "        # Keep the historical column name for backwards compatibility\n",
    "        'accuracy': fold.get('acc', None),\n",
    "        'selected_C': fold.get('selected_C', None),\n",
    "        'selected_n_bins': fold.get('selected_n_bins', None) if fold.get('selected_n_bins', None) is not None else 'FOOOF',\n",
    "        'n_test_epochs': int(fold['y_test'].size),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(['repeat', 'fold']).reset_index(drop=True)\n",
    "\n",
    "# If per-epoch predictions are available, compute fold accuracies with/without smoothing\n",
    "if 'logreg_predictions_df' in globals() and isinstance(logreg_predictions_df, pd.DataFrame):\n",
    "    dfp = logreg_predictions_df.copy()\n",
    "    needed = {'fold', 'y_true', 'y_pred', 'y_pred_smooth'}\n",
    "    if needed.issubset(set(dfp.columns)):\n",
    "        fold_acc = (\n",
    "            dfp.groupby('fold')\n",
    "            .apply(lambda g: pd.Series({\n",
    "                'accuracy_raw': float((g['y_true'].to_numpy() == g['y_pred'].to_numpy()).mean()),\n",
    "                'accuracy_smoothed': float((g['y_true'].to_numpy() == g['y_pred_smooth'].to_numpy()).mean()),\n",
    "                'delta_accuracy': float((g['y_true'].to_numpy() == g['y_pred_smooth'].to_numpy()).mean() - (g['y_true'].to_numpy() == g['y_pred'].to_numpy()).mean()),\n",
    "            }))\n",
    "            .reset_index()\n",
    "        )\n",
    "        # fold id in dfp is like r{repeat}_f{fold}\n",
    "        tmp = summary_df.copy()\n",
    "        tmp['fold_id'] = tmp.apply(lambda r: f\"r{int(r['repeat'])}_f{int(r['fold'])}\", axis=1)\n",
    "        merged = tmp.merge(fold_acc, left_on='fold_id', right_on='fold', how='left', suffixes=('', '_y'))\n",
    "        merged = merged.drop(columns=['fold_id', 'fold_y'])\n",
    "        summary_df = merged\n",
    "\n",
    "        # Overall summary\n",
    "        try:\n",
    "            from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "            y_true_all = dfp['y_true'].to_numpy(dtype=int)\n",
    "            y_pred_raw = dfp['y_pred'].to_numpy(dtype=int)\n",
    "            y_pred_sm = dfp['y_pred_smooth'].to_numpy(dtype=int)\n",
    "\n",
    "            overall = {\n",
    "                'use_time_adjustment': bool(globals().get('USE_TIME_ADJUSTMENT', False)),\n",
    "                'run_length_tuned': bool(globals().get('RUN_LENGTH_TUNED', False)),\n",
    "                'selected_run_length': globals().get('SELECTED_RUN_LENGTH', None),\n",
    "                'selected_run_length_edge': globals().get('SELECTED_RUN_LENGTH_EDGE', None),\n",
    "                'accuracy_raw': float(accuracy_score(y_true_all, y_pred_raw)),\n",
    "                'accuracy_smoothed': float(accuracy_score(y_true_all, y_pred_sm)),\n",
    "                'delta_accuracy': float(accuracy_score(y_true_all, y_pred_sm) - accuracy_score(y_true_all, y_pred_raw)),\n",
    "                'balanced_accuracy_raw': float(balanced_accuracy_score(y_true_all, y_pred_raw)),\n",
    "                'balanced_accuracy_smoothed': float(balanced_accuracy_score(y_true_all, y_pred_sm)),\n",
    "                'delta_balanced_accuracy': float(balanced_accuracy_score(y_true_all, y_pred_sm) - balanced_accuracy_score(y_true_all, y_pred_raw)),\n",
    "                'n_epochs_total': int(len(y_true_all)),\n",
    "                'n_subjects_total': int(dfp['subject_id'].nunique()) if 'subject_id' in dfp.columns else None,\n",
    "            }\n",
    "            overall_df = pd.DataFrame([overall])\n",
    "            overall_path = outpath('time_adjustment_summary.csv')\n",
    "            overall_df.to_csv(overall_path, index=False)\n",
    "            print('Saved TIME_ADJUSTMENT summary to', overall_path.resolve())\n",
    "            display(overall_df)\n",
    "        except Exception as exc:\n",
    "            print('Could not compute TIME_ADJUSTMENT summary:', exc)\n",
    "\n",
    "# Persist\n",
    "display(summary_df)\n",
    "summary_path = outpath('logreg_cv_summary.csv')\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Saved logistic CV summary to {summary_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf59af",
   "metadata": {},
   "source": [
    "### Visualization: scalp map of model weights\n",
    "\n",
    "This cell converts logistic regression coefficients into **per-channel weights** and plots them on a scalp topography.\n",
    "\n",
    "- In PSD mode, coefficients are aggregated across frequency bins per channel.\n",
    "- In FOOOF mode, coefficients are aggregated across the selected per-channel FOOOF features.\n",
    "\n",
    "Use it to interpret which channels drive the decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Scalp map (universal: works for any CV level and USE_FOOOF True/False) ----\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "# -------- helpers --------\n",
    "def _extract_binary_coef(model):\n",
    "    coef = np.asarray(getattr(model, \"coef_\", None))\n",
    "    if coef is None:\n",
    "        raise RuntimeError(\"final_model has no coef_.\")\n",
    "    if coef.ndim == 1:\n",
    "        return coef\n",
    "    if coef.ndim == 2 and coef.shape[0] == 1:\n",
    "        return coef[0]\n",
    "    if coef.ndim == 2 and coef.shape[0] > 1:\n",
    "        # pick class 1 vs others if available; else average\n",
    "        return coef[1] - np.mean(np.vstack([coef[:1], coef[2:]]), axis=0) if coef.shape[0] >= 2 else np.mean(coef, axis=0)\n",
    "    raise ValueError(f\"Unexpected coef_ shape: {coef.shape}\")\n",
    "\n",
    "def _per_channel_weights_fooof(coef, feature_channels, default_stride=5):\n",
    "    if not feature_channels:\n",
    "        raise RuntimeError(\"feature_channels missing while USE_FOOOF=True.\")\n",
    "    n_ch = len(feature_channels)\n",
    "    stride = coef.size // n_ch if coef.size % n_ch == 0 else default_stride\n",
    "    if stride * n_ch > coef.size:\n",
    "        stride = default_stride\n",
    "    w = []\n",
    "    for i in range(n_ch):\n",
    "        s, e = i*stride, min((i+1)*stride, coef.size)\n",
    "        w.append(float(np.sum(coef[s:e])))\n",
    "    return np.asarray(w, float), list(feature_channels)\n",
    "\n",
    "def _per_channel_weights_psd(coef, final_n_bins, PSD_META, TARGET_CHANNELS=None, FREQ_BIN_OPTIONS=None):\n",
    "    # bins\n",
    "    if final_n_bins is not None:\n",
    "        n_bins = int(final_n_bins)\n",
    "    elif FREQ_BIN_OPTIONS:\n",
    "        n_bins = int(FREQ_BIN_OPTIONS[0])\n",
    "    else:\n",
    "        n_bins = None\n",
    "    # channels\n",
    "    psd_ch = list(PSD_META['channels']) if isinstance(PSD_META, dict) and 'channels' in PSD_META else None\n",
    "    if not psd_ch:\n",
    "        raise RuntimeError(\"PSD_META['channels'] missing or empty.\")\n",
    "    cand = list(TARGET_CHANNELS) if TARGET_CHANNELS else psd_ch\n",
    "    if n_bins is not None:\n",
    "        if coef.size % n_bins != 0:\n",
    "            # try infer from candidate channels\n",
    "            n_bins = coef.size // len(cand) if len(cand) and coef.size % len(cand) == 0 else max(1, coef.size // len(psd_ch))\n",
    "        n_ch = coef.size // n_bins\n",
    "    else:\n",
    "        if len(cand) and coef.size % len(cand) == 0:\n",
    "            n_ch = len(cand); n_bins = coef.size // n_ch\n",
    "        elif coef.size % len(psd_ch) == 0:\n",
    "            n_ch = len(psd_ch); n_bins = coef.size // n_ch\n",
    "        else:\n",
    "            raise ValueError(\"Cannot infer n_bins/channels from coef size.\")\n",
    "    ch_names = cand if len(cand) == n_ch else psd_ch[:n_ch]\n",
    "    w = []\n",
    "    for i in range(n_ch):\n",
    "        s, e = i*n_bins, (i+1)*n_bins\n",
    "        w.append(float(np.sum(coef[s:e])))\n",
    "    return np.asarray(w, float), ch_names\n",
    "\n",
    "# -------- build weights (works regardless of CV level) --------\n",
    "coef_vec = _extract_binary_coef(final_model)\n",
    "\n",
    "if 'USE_FOOOF' in globals() and USE_FOOOF:\n",
    "    # FOOOF layout → per-channel feature blocks\n",
    "    if 'feature_channels' not in globals():\n",
    "        raise RuntimeError(\"feature_channels not defined (required for USE_FOOOF=True).\")\n",
    "    per_channel_weights, channel_names = _per_channel_weights_fooof(coef_vec, feature_channels)\n",
    "else:\n",
    "    # PSD-binned layout → [ch0_bins, ch1_bins, ...]\n",
    "    per_channel_weights, channel_names = _per_channel_weights_psd(\n",
    "        coef_vec,\n",
    "        final_n_bins if 'final_n_bins' in globals() else None,\n",
    "        PSD_META,\n",
    "        TARGET_CHANNELS=TARGET_CHANNELS if 'TARGET_CHANNELS' in globals() else None,\n",
    "        FREQ_BIN_OPTIONS=FREQ_BIN_OPTIONS if 'FREQ_BIN_OPTIONS' in globals() else None\n",
    "    )\n",
    "\n",
    "if len(channel_names) != per_channel_weights.size:\n",
    "    raise ValueError(f\"Channel/weight mismatch: {len(channel_names)} vs {per_channel_weights.size}\")\n",
    "\n",
    "# -------- plot topomap (API-compatible across MNE versions) --------\n",
    "sfreq = float(closed_payload.get('sfreq', 256.0)) if 'closed_payload' in globals() else 256.0\n",
    "info = mne.create_info(ch_names=channel_names, sfreq=sfreq, ch_types='eeg')\n",
    "\n",
    "# montage\n",
    "montage = None\n",
    "try:\n",
    "    m_name = PSD_META.get('montage', 'standard_1020') if isinstance(PSD_META, dict) else 'standard_1020'\n",
    "    montage = mne.channels.make_standard_montage(m_name)\n",
    "    info.set_montage(montage, on_missing='ignore')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "vmin_val = np.percentile(per_channel_weights, 5)\n",
    "vmax_val = np.percentile(per_channel_weights, 95)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_kwargs = dict(axes=ax, names=channel_names, show=False, outlines='head', cmap='RdBu_r')\n",
    "\n",
    "try:\n",
    "    im, _ = mne.viz.plot_topomap(per_channel_weights, info, vlim=(vmin_val, vmax_val), **plot_kwargs)\n",
    "except TypeError:\n",
    "    norm = plt.Normalize(vmin=vmin_val, vmax=vmax_val)\n",
    "    im, _ = mne.viz.plot_topomap(per_channel_weights, info, norm=norm, **plot_kwargs)\n",
    "\n",
    "cb = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cb.set_label(\"LogReg weight (signed)\")\n",
    "ax.set_title(\"Logistic Regression — Electrode weights (signed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ---- End universal scalp map ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef5dc6",
   "metadata": {},
   "source": [
    "### Quick readout of aggregate metrics\n",
    "\n",
    "This cell prints the aggregate metrics computed in the training/CV step:\n",
    "- Accuracy\n",
    "- Classification report\n",
    "- Confusion matrix\n",
    "\n",
    "It also echoes key run settings (class weights and, in PSD mode, the final binning choice).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ef933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", acc)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "if not USE_FOOOF:\n",
    "    print(f\"Final frequency bins used: {final_n_bins}\")\n",
    "print(f\"Class weight setting: {CLASS_WEIGHT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c75824",
   "metadata": {},
   "source": [
    "### Diagnostics: split composition and feature health\n",
    "\n",
    "This cell prints:\n",
    "- How many subjects were used in train vs test (and how many folds covered which subjects).\n",
    "- Chosen hyperparameters.\n",
    "- Class balance in train/test.\n",
    "- Basic feature health checks (percent NaN rows, all-zero rows).\n",
    "\n",
    "Use it to catch silent failures early (e.g., missing labels, degenerate feature extraction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1958ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostics / sanity checks ---\n",
    "print()\n",
    "print(f\"Subjects: total={unique_subjects.size}, primary-train={np.unique(train_subjects).size}, primary-test={logreg_primary_test_subjects.size}\")\n",
    "if CV_LEVEL > 1:\n",
    "    print(f\"Cross-validation covered {test_subjects.size} unique held-out subjects across {len(logreg_cv_folds)} folds.\")\n",
    "    print(\"Primary fold test subjects (model artefacts saved from this split):\", np.sort(logreg_primary_test_subjects))\n",
    "else:\n",
    "    print(\"Held-out test subjects:\", np.sort(logreg_primary_test_subjects))\n",
    "print(\"Selected final C (mode across inner folds):\", final_C)\n",
    "if not USE_FOOOF:\n",
    "    print(\"Selected final n_bins (mode across inner folds):\", final_n_bins)\n",
    "print(\"Train class counts:\", Counter(y_train))\n",
    "print(\"Test  class counts:\", Counter(y_test))\n",
    "\n",
    "def pct_nan(x):\n",
    "    \"\"\"Percent of rows that contain any NaN.\n",
    "    Accepts ndarray or list of ndarrays (ragged across folds).\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        total = sum(arr.shape[0] for arr in x if isinstance(arr, np.ndarray))\n",
    "        count = sum(np.isnan(arr).any(axis=1).sum() for arr in x if isinstance(arr, np.ndarray))\n",
    "        return 100.0 * (count / total) if total else 0.0\n",
    "    return 100.0 * np.isnan(x).any(axis=1).mean()\n",
    "\n",
    "def pct_zero_row(x):\n",
    "    \"\"\"Percent of rows that are all-zero.\n",
    "    Accepts ndarray or list of ndarrays (ragged across folds).\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        total = sum(arr.shape[0] for arr in x if isinstance(arr, np.ndarray))\n",
    "        count = 0\n",
    "        for arr in x:\n",
    "            if not isinstance(arr, np.ndarray):\n",
    "                continue\n",
    "            count += np.all(arr == 0.0, axis=1).sum()\n",
    "        return 100.0 * (count / total) if total else 0.0\n",
    "    return 100.0 * (np.all(x == 0.0, axis=1)).mean()\n",
    "\n",
    "print(f\"% rows with any NaN in X_train: {pct_nan(X_train):.2f}%\")\n",
    "print(f\"% rows that are all-zero features in X_train: {pct_zero_row(X_train):.2f}%\")\n",
    "print(f\"% rows with any NaN in X_test:  {pct_nan(X_test):.2f}%\")\n",
    "print(f\"% rows that are all-zero features in X_test:  {pct_zero_row(X_test):.2f}%\")\n",
    "print(\"=== Aggregated test results (all folds) ===\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff622b8",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "\n",
    "This cell plots the ROC curve using predicted probabilities for class **EC (label 1)**.\n",
    "\n",
    "It is mainly useful when:\n",
    "- You want a threshold-independent view of separability.\n",
    "- You are comparing feature modes (PSD vs FOOOF) or smoothing settings.\n",
    "\n",
    "It assumes the previous training cell has produced probability outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f402d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for class 1 (Eyes Closed)\n",
    "if 'y_proba' not in globals():\n",
    "    raise RuntimeError(\"Logistic regression probabilities are unavailable. Run the training cell first.\")\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"Logistic Regression (AUC = {roc_auc:.2f})\", linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b93538",
   "metadata": {},
   "source": [
    "### Plotting test accuracy per subject (LOSO)\n",
    "\n",
    "This plot summarizes **how performance varies across individuals**.\n",
    "\n",
    "It loads per-subject validation results (from the training/CV step) and:\n",
    "- Sorts subjects by accuracy.\n",
    "- Visualizes which subjects are hardest/easiest.\n",
    "\n",
    "Use it to identify outliers (e.g., bad recordings or labeling mismatches).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b53ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "val_accuracies = np.load(outpath(\"val_accuracies.npy\"))\n",
    "val_subject_ids = np.load(outpath(\"val_subject_ids.npy\"))\n",
    "\n",
    "# Sort by subject ID for better visualization\n",
    "sorted_indices = np.argsort(val_subject_ids)\n",
    "sorted_subjects = val_subject_ids[sorted_indices]\n",
    "sorted_accuracies = val_accuracies[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_subjects.astype(str), sorted_accuracies, color=\"#2d4987\")\n",
    "plt.title(\"Accuracy per Subject (LOSO) - Logistic Regression\")\n",
    "plt.xlabel(\"Subject ID\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41610014",
   "metadata": {},
   "source": [
    "### Printing the chosen hyperparameters\n",
    "\n",
    "This cell prints the hyperparameters selected by the inner loop.\n",
    "\n",
    "Typically:\n",
    "- `C` is always selected.\n",
    "- If PSD binning is enabled, `n_bins` (the PSD frequency bin count) may also be selected.\n",
    "\n",
    "This is useful for reporting and for keeping runs reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b73019",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters from inner loop:\")\n",
    "print(f\" C: {final_C}\")\n",
    "if not USE_FOOOF:\n",
    "    print(f\" n_bins: {final_n_bins}\")\n",
    "if USE_TIME_ADJUSTMENT:\n",
    "    print(f\" smoothing_L: {SELECTED_RUN_LENGTH} (tuned={RUN_LENGTH_TUNED})\")\n",
    "    if USE_EDGE_SMOOTHING:\n",
    "        print(f\" smoothing_L_edge: {SELECTED_RUN_LENGTH_EDGE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ed3f1",
   "metadata": {},
   "source": [
    "### Hold-out performance: per-subject accuracy + confusion matrix\n",
    "\n",
    "This section focuses on **interpretability of the evaluation**:\n",
    "- Aggregates predictions by held-out subject.\n",
    "- Plots per-subject accuracy in the hold-out split.\n",
    "- Displays a confusion matrix to show EO/EC error balance.\n",
    "\n",
    "Run it after the training/CV step has produced `logreg_predictions_df` (or equivalent outputs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group predictions by test subject\n",
    "if 'logreg_predictions_df' not in globals():\n",
    "    raise RuntimeError(\"Run the logistic regression cell to populate prediction records before computing per-subject metrics.\")\n",
    "subject_metrics = []\n",
    "for subj_id, group in logreg_predictions_df.groupby('subject_id'):\n",
    "    y_true_subj = group['y_true'].values\n",
    "    y_pred_subj = group['y_pred'].values\n",
    "    acc_subj = accuracy_score(y_true_subj, y_pred_subj)\n",
    "    prec_subj = precision_score(y_true_subj, y_pred_subj, zero_division=0)\n",
    "    rec_subj = recall_score(y_true_subj, y_pred_subj, zero_division=0)\n",
    "    subject_metrics.append({\n",
    "        'subject': int(subj_id),\n",
    "        'accuracy': acc_subj,\n",
    "        'precision': prec_subj,\n",
    "        'recall': rec_subj,\n",
    "    })\n",
    "subject_metrics = sorted(subject_metrics, key=lambda item: item['subject'])\n",
    "\n",
    "# 2. Print metrics per subject\n",
    "print(\"Per-subject performance:\")\n",
    "for metrics in subject_metrics:\n",
    "    print(f\"Subject {metrics['subject']}: Accuracy = {metrics['accuracy']:.2f}, Precision = {metrics['precision']:.2f}, Recall = {metrics['recall']:.2f}\")\n",
    "\n",
    "# 3. Plot accuracy per subject\n",
    "plt.figure(figsize=(14, 5))\n",
    "subject_ids_sorted = [m['subject'] for m in subject_metrics]\n",
    "accuracies = [m['accuracy'] for m in subject_metrics]\n",
    "sns.barplot(x=subject_ids_sorted, y=accuracies, color=\"#2d4987\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Accuracy per Test Subject\")\n",
    "plt.xlabel(\"Subject ID\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Combined confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(\"Confusion Matrix (All Test Subjects)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ff6cc",
   "metadata": {},
   "source": [
    "### Export: per-subject metrics\n",
    "\n",
    "This cell saves a per-subject performance table (accuracy/precision/recall, etc.) to CSV.\n",
    "\n",
    "It is useful for:\n",
    "- Reporting which participants drive errors.\n",
    "- Selecting “hard” subjects for the PSD/FOOOF diagnostic plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the per-subject metrics and highlight the most challenging participants.\n",
    "if 'subject_metrics' not in globals():\n",
    "    raise RuntimeError(\"Compute subject_metrics in the previous cell before exporting them.\")\n",
    "subject_df = pd.DataFrame(subject_metrics).sort_values('accuracy', ascending=False).reset_index(drop=True)\n",
    "display(subject_df)\n",
    "worst = subject_df.nsmallest(3, 'accuracy')\n",
    "if not worst.empty:\n",
    "    print(\"Most challenging subjects (lowest accuracy):\")\n",
    "    for _, row in worst.iterrows():\n",
    "        print(f\"  Subject {int(row['subject'])}: accuracy={row['accuracy']:.2f}, precision={row['precision']:.2f}, recall={row['recall']:.2f}\")\n",
    "export_path = outpath('logreg_per_subject_metrics.csv')\n",
    "subject_df.to_csv(export_path, index=False)\n",
    "print(f\"Saved per-subject metrics to {export_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8e6b8",
   "metadata": {},
   "source": [
    "### Diagnostic: average PSD (hard subject) for EC vs EO\n",
    "\n",
    "This cell selects a challenging subject (based on prediction accuracy) and compares:\n",
    "- Mean PSD for EO epochs vs EC epochs.\n",
    "- Optionally a subset of channels (`TARGET_CHANNELS`) if available.\n",
    "\n",
    "It helps interpret whether the model is struggling due to subtle spectral differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot average PSD for a challenging subject: EC vs EO (drop-in) ----\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Config ----------\n",
    "PSD_PLOT_FREQ_RANGE = (3.0, 40.0)     # Hz range to display\n",
    "PSD_PLOT_LOG_POWER  = True            # True = plot log10 power\n",
    "DIFFICULTY_MIN_TEST_EPOCHS = 6        # need at least this many test epochs to trust \"difficulty\"\n",
    "USE_TARGET_CHANNELS_ONLY = True       # average only across TARGET_CHANNELS if available\n",
    "\n",
    "# ---------- Pick a \"challenging\" subject ----------\n",
    "def _pick_hard_subject():\n",
    "    # Prefer the predictions DF assembled after CV\n",
    "    if 'logreg_predictions_df' in globals():\n",
    "        df = logreg_predictions_df.copy()\n",
    "        # Per-subject accuracy (or pick worst balanced accuracy if you prefer)\n",
    "        grp = df.groupby('subject_id').agg(\n",
    "            n=('y_true','size'),\n",
    "            acc=('y_true', lambda y: (y.values == df.loc[y.index, 'y_pred'].values).mean())\n",
    "        )\n",
    "        # Filter by minimum test size\n",
    "        grp = grp[grp['n'] >= DIFFICULTY_MIN_TEST_EPOCHS]\n",
    "        if grp.empty:\n",
    "            # Fallback: use all subjects if none pass the threshold\n",
    "            grp = df.groupby('subject_id').agg(n=('y_true','size'),\n",
    "                                               acc=('y_true', lambda y: (y.values == df.loc[y.index, 'y_pred'].values).mean()))\n",
    "        hard_subj = int(grp['acc'].idxmin())\n",
    "        hard_acc  = float(grp.loc[hard_subj, 'acc'])\n",
    "        return hard_subj, hard_acc, int(grp.loc[hard_subj, 'n'])\n",
    "    # Secondary fallback: pick a subject with the smallest margin between class counts\n",
    "    ids = np.asarray(subject_ids)\n",
    "    subj_list = np.unique(ids)\n",
    "    best = None\n",
    "    for s in subj_list:\n",
    "        idx = np.where(ids == s)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        y_sub = y_combined[idx]\n",
    "        n = idx.size\n",
    "        # \"difficulty\" proxy: how close EC and EO counts are (more balanced → harder sometimes)\n",
    "        margin = abs((y_sub == 1).sum() - (y_sub == 0).sum()) / n\n",
    "        score = margin  # smaller is \"harder\"\n",
    "        if best is None or score < best[0]:\n",
    "            best = (score, int(s), n)\n",
    "    if best is None:\n",
    "        raise RuntimeError(\"Could not determine a challenging subject.\")\n",
    "    return best[1], np.nan, best[2]\n",
    "\n",
    "hard_subject, hard_acc, hard_n = _pick_hard_subject()\n",
    "print(f\"Selected challenging subject: {hard_subject} (test n={hard_n}, acc={hard_acc if not np.isnan(hard_acc) else 'n/a'})\")\n",
    "\n",
    "# ---------- Prepare data for that subject ----------\n",
    "# Channel set for averaging\n",
    "if USE_TARGET_CHANNELS_ONLY and 'TARGET_CHANNELS' in globals() and isinstance(TARGET_CHANNELS, (list, tuple)) and len(TARGET_CHANNELS) > 0:\n",
    "    chs = [ch for ch in TARGET_CHANNELS if ch in PSD_META['channels']]\n",
    "    if not chs:\n",
    "        chs = list(PSD_META['channels'])\n",
    "else:\n",
    "    chs = list(PSD_META['channels'])\n",
    "\n",
    "ch_idx = [PSD_META['channels'].index(c) for c in chs]\n",
    "\n",
    "# Indices for the subject & each condition\n",
    "subj_mask = (np.asarray(subject_ids) == hard_subject)\n",
    "eo_mask = subj_mask & (np.asarray(y_combined) == 0)  # Eyes Open label assumed 0\n",
    "ec_mask = subj_mask & (np.asarray(y_combined) == 1)  # Eyes Closed label assumed 1\n",
    "\n",
    "if not np.any(eo_mask) or not np.any(ec_mask):\n",
    "    raise RuntimeError(f\"Subject {hard_subject} lacks EO or EC epochs.\")\n",
    "\n",
    "# Slice PSD cube: (epochs, channels, freqs)\n",
    "psd_eo = psd_cube[eo_mask][:, ch_idx, :]\n",
    "psd_ec = psd_cube[ec_mask][:, ch_idx, :]\n",
    "\n",
    "# Average across epochs, then across channels\n",
    "mean_psd_eo = np.nanmean(psd_eo, axis=(0,1))\n",
    "mean_psd_ec = np.nanmean(psd_ec, axis=(0,1))\n",
    "\n",
    "# Optional CI (SEM across epochs) to visualize spread\n",
    "def _sem_over_epochs(block):  # block shape: (n_epochs, n_channels, n_freqs)\n",
    "    if block.shape[0] <= 1:\n",
    "        return np.zeros(block.shape[-1], dtype=float)\n",
    "    # First average across channels per epoch, then compute SEM across epochs\n",
    "    per_epoch = np.nanmean(block, axis=1)  # (n_epochs, n_freqs)\n",
    "    return np.nanstd(per_epoch, axis=0, ddof=1) / np.sqrt(per_epoch.shape[0])\n",
    "\n",
    "sem_eo = _sem_over_epochs(psd_eo)\n",
    "sem_ec = _sem_over_epochs(psd_ec)\n",
    "\n",
    "# Frequency mask for plotting range\n",
    "freqs = psd_freqs\n",
    "fmask = (freqs >= PSD_PLOT_FREQ_RANGE[0]) & (freqs <= PSD_PLOT_FREQ_RANGE[1])\n",
    "\n",
    "# Transform to y-scale\n",
    "def _to_plot_scale(arr):\n",
    "    return np.log10(np.maximum(arr, 1e-30)) if PSD_PLOT_LOG_POWER else arr\n",
    "\n",
    "y_eo = _to_plot_scale(mean_psd_eo)\n",
    "y_ec = _to_plot_scale(mean_psd_ec)\n",
    "y_eo_sem = _to_plot_scale(mean_psd_eo + sem_eo) - _to_plot_scale(mean_psd_eo)\n",
    "y_ec_sem = _to_plot_scale(mean_psd_ec + sem_ec) - _to_plot_scale(mean_psd_ec)\n",
    "\n",
    "# ---------- Plot ----------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "titles = [\"Eyes Closed (EC)\", \"Eyes Open (EO)\"]\n",
    "data   = [(y_ec, y_ec_sem), (y_eo, y_eo_sem)]\n",
    "\n",
    "for ax, title, (y, y_sem) in zip(axes, titles, data):\n",
    "    ax.plot(freqs[fmask], y[fmask], linewidth=2, label=\"Mean PSD\")\n",
    "    # shaded SEM\n",
    "    ax.fill_between(freqs[fmask], (y - y_sem)[fmask], (y + y_sem)[fmask], alpha=0.2, label=\"±1 SEM\")\n",
    "    # mark alpha band if available\n",
    "    if 'ALPHA_BAND' in globals() and ALPHA_BAND is not None:\n",
    "        ax.axvspan(ALPHA_BAND[0], ALPHA_BAND[1], color=\"#ffbf00\", alpha=0.15, label=\"Alpha band\")\n",
    "    ax.set_title(f\"{title} — Subject {hard_subject}\")\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax.grid(alpha=0.25)\n",
    "\n",
    "axes[0].set_ylabel(\"log10 Power\" if PSD_PLOT_LOG_POWER else \"Power (AU)\")\n",
    "# de-duplicate legend entries\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "uniq = dict(zip(labels, handles))\n",
    "axes[1].legend(uniq.values(), uniq.keys(), loc=\"upper right\")\n",
    "\n",
    "fig.suptitle(\"Challenging subject: average PSD (EC vs EO)\", fontsize=14)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "plt.show()\n",
    "# ---- End drop-in ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d05a4",
   "metadata": {},
   "source": [
    "### Diagnostic: PSDs for misclassified epochs\n",
    "\n",
    "This cell drills into **two hard examples**:\n",
    "- An EC epoch misclassified as EO.\n",
    "- An EO epoch misclassified as EC.\n",
    "\n",
    "For each, it plots PSD curves across selected channels so you can visually compare spectral structure near the alpha band and beyond.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Misclassified epochs PSDs for a challenging subject (EC→EO and EO→EC) ----\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Config --------\n",
    "MISCLASS_FREQ_RANGE = (3.0, 40.0)   # Hz range to show\n",
    "MISCLASS_LOG_POWER  = True          # plot log10 power if True\n",
    "USE_TARGET_CHANNELS_ONLY = True     # use TARGET_CHANNELS subset if available\n",
    "\n",
    "# -------- Collect per-epoch predictions (with global epoch indices) --------\n",
    "mis_records = []  # each: dict(epoch_idx, subject_id, y_true, y_pred, prob_ec, fold)\n",
    "for fold in logreg_cv_folds:\n",
    "    idxs = fold['test_idx']\n",
    "    subs = fold['subject_ids']\n",
    "    y_t  = fold['y_test']\n",
    "    y_p  = fold['y_pred']\n",
    "    p_ec = fold['y_proba']  # probability for class \"EC\" (label 1)\n",
    "    for i in range(len(idxs)):\n",
    "        if y_t[i] != y_p[i]:  # misclassified\n",
    "            mis_records.append({\n",
    "                \"epoch_idx\": int(idxs[i]),\n",
    "                \"subject_id\": int(subs[i]),\n",
    "                \"y_true\": int(y_t[i]),\n",
    "                \"y_pred\": int(y_p[i]),\n",
    "                \"prob_ec\": float(p_ec[i]),\n",
    "                \"fold\": f\"r{fold['repeat']}_f{fold['fold']}\",\n",
    "            })\n",
    "\n",
    "if not mis_records:\n",
    "    raise RuntimeError(\"No misclassifications found across folds; cannot make the requested figure.\")\n",
    "\n",
    "# -------- Pick a subject that has BOTH types of mistakes --------\n",
    "from collections import defaultdict\n",
    "by_subj = defaultdict(list)\n",
    "for rec in mis_records:\n",
    "    by_subj[rec[\"subject_id\"]].append(rec)\n",
    "\n",
    "candidate_subject = None\n",
    "for sid, recs in by_subj.items():\n",
    "    has_ec_to_eo = any(r[\"y_true\"] == 1 and r[\"y_pred\"] == 0 for r in recs)\n",
    "    has_eo_to_ec = any(r[\"y_true\"] == 0 and r[\"y_pred\"] == 1 for r in recs)\n",
    "    if has_ec_to_eo and has_eo_to_ec:\n",
    "        candidate_subject = sid\n",
    "        break\n",
    "if candidate_subject is None:\n",
    "    # fallback: choose the subject with most total mistakes and then try to pick opposite pairs\n",
    "    candidate_subject = max(by_subj.items(), key=lambda kv: len(kv[1]))[0]\n",
    "\n",
    "# Extract one EC→EO and one EO→EC example for that subject (choose the one closest to decision boundary)\n",
    "def _closest_to_boundary(records, true_label, pred_label):\n",
    "    # distance to 0.5 for prob_ec (smaller = more ambiguous)\n",
    "    cand = [r for r in records if r[\"y_true\"] == true_label and r[\"y_pred\"] == pred_label]\n",
    "    if not cand:\n",
    "        return None\n",
    "    return min(cand, key=lambda r: abs(r[\"prob_ec\"] - 0.5))\n",
    "\n",
    "subject_recs = by_subj[candidate_subject]\n",
    "ec_to_eo = _closest_to_boundary(subject_recs, true_label=1, pred_label=0)\n",
    "eo_to_ec = _closest_to_boundary(subject_recs, true_label=0, pred_label=1)\n",
    "\n",
    "# If one side is missing, just take any misclassified epoch of that kind globally\n",
    "if ec_to_eo is None:\n",
    "    ec_to_eo = _closest_to_boundary(mis_records, true_label=1, pred_label=0)\n",
    "if eo_to_ec is None:\n",
    "    eo_to_ec = _closest_to_boundary(mis_records, true_label=0, pred_label=1)\n",
    "if ec_to_eo is None or eo_to_ec is None:\n",
    "    raise RuntimeError(\"Could not find both a misclassified EC epoch and a misclassified EO epoch.\")\n",
    "\n",
    "# -------- Pull PSDs for those epochs --------\n",
    "# Channel selection\n",
    "if USE_TARGET_CHANNELS_ONLY and 'TARGET_CHANNELS' in globals() and TARGET_CHANNELS:\n",
    "    channels = [ch for ch in TARGET_CHANNELS if ch in PSD_META['channels']]\n",
    "    if not channels:\n",
    "        channels = list(PSD_META['channels'])\n",
    "else:\n",
    "    channels = list(PSD_META['channels'])\n",
    "ch_idx = [PSD_META['channels'].index(c) for c in channels]\n",
    "\n",
    "# Helper to get PSDs for an epoch (returns (freqs, 2D array: n_channels x n_freqs))\n",
    "def _epoch_psds(epoch_idx):\n",
    "    freqs = psd_freqs\n",
    "    spectra = psd_cube[epoch_idx, ch_idx, :]  # shape (n_channels, n_freqs)\n",
    "    return freqs, spectra\n",
    "\n",
    "f_ec2eo, psd_ec2eo = _epoch_psds(ec_to_eo[\"epoch_idx\"])\n",
    "f_eo2ec, psd_eo2ec = _epoch_psds(eo_to_ec[\"epoch_idx\"])\n",
    "\n",
    "# -------- Plotting --------\n",
    "def _to_plot_scale(arr):\n",
    "    return np.log10(np.maximum(arr, 1e-30)) if MISCLASS_LOG_POWER else arr\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# Frequency window\n",
    "mask_ec2eo = (f_ec2eo >= MISCLASS_FREQ_RANGE[0]) & (f_ec2eo <= MISCLASS_FREQ_RANGE[1])\n",
    "mask_eo2ec = (f_eo2ec >= MISCLASS_FREQ_RANGE[0]) & (f_eo2ec <= MISCLASS_FREQ_RANGE[1])\n",
    "\n",
    "# Left: EC epoch classified as EO\n",
    "ax = axes[0]\n",
    "for i, ch in enumerate(channels):\n",
    "    ax.plot(f_ec2eo[mask_ec2eo], _to_plot_scale(psd_ec2eo[i, :])[mask_ec2eo], linewidth=1.0, alpha=0.9, label=ch)\n",
    "if 'ALPHA_BAND' in globals() and ALPHA_BAND is not None:\n",
    "    ax.axvspan(ALPHA_BAND[0], ALPHA_BAND[1], color=\"#ffbf00\", alpha=0.12)\n",
    "ax.set_title(f\"EC epoch classified as EO — subj {ec_to_eo['subject_id']} — idx {ec_to_eo['epoch_idx']}\")\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.set_ylabel(\"log10 Power\" if MISCLASS_LOG_POWER else \"Power (AU)\")\n",
    "ax.grid(alpha=0.25)\n",
    "# keep legend tidy (many channels → outside frame)\n",
    "ax.legend(loc='upper right', fontsize=8, ncol=1, frameon=False)\n",
    "\n",
    "# Right: EO epoch classified as EC\n",
    "ax = axes[1]\n",
    "for i, ch in enumerate(channels):\n",
    "    ax.plot(f_eo2ec[mask_eo2ec], _to_plot_scale(psd_eo2ec[i, :])[mask_eo2ec], linewidth=1.0, alpha=0.9, label=ch)\n",
    "if 'ALPHA_BAND' in globals() and ALPHA_BAND is not None:\n",
    "    ax.axvspan(ALPHA_BAND[0], ALPHA_BAND[1], color=\"#ffbf00\", alpha=0.12)\n",
    "ax.set_title(f\"EO epoch classified as EC — subj {eo_to_ec['subject_id']} — idx {eo_to_ec['epoch_idx']}\")\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.grid(alpha=0.25)\n",
    "# single legend for both (avoid clutter)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "uniq = dict(zip(labels, handles))\n",
    "axes[1].legend(uniq.values(), uniq.keys(), loc='upper right', fontsize=8, ncol=1, frameon=False)\n",
    "\n",
    "fig.suptitle(\"Misclassified epochs: PSDs for all selected channels\", fontsize=14)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "plt.show()\n",
    "# End misclassified-epoch PSD plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a37e6",
   "metadata": {},
   "source": [
    "### ONE_MAIN_FOOOF deep-dive: subject ROI spectrum + misclassified examples\n",
    "\n",
    "This cell combines a subject-level FOOOF view with misclassification examples.\n",
    "\n",
    "It:\n",
    "- Chooses a subject (by default the lowest-accuracy subject).\n",
    "- Fits FOOOF/specparam to the subject’s ROI-averaged spectrum.\n",
    "- Visualizes example misclassified epochs for that subject.\n",
    "\n",
    "Run it after feature extraction and after the logistic regression CV cell has produced per-epoch predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Main FOOOF + misclassified-epoch analysis (ONE_MAIN_FOOOF) ----\n",
    "# Creates 3 plots (if available):\n",
    "#  1) Subject-level averaged ROI PSD + aperiodic + main alpha peak\n",
    "#  2) Example EC epoch misclassified as EO\n",
    "#  3) Example EO epoch misclassified as EC\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "MAIN_FOOOF_SUBJECT = None  # None -> auto-select lowest-accuracy subject; or set an int subject id\n",
    "MIS_EXAMPLE_STRATEGY = \"most_confident\"  # \"most_confident\" or \"first\"\n",
    "\n",
    "if not (\"USE_FOOOF\" in globals() and USE_FOOOF and \"ONE_MAIN_FOOOF\" in globals() and ONE_MAIN_FOOOF):\n",
    "    print(\"This analysis is intended for ONE_MAIN_FOOOF mode (USE_FOOOF=True, ONE_MAIN_FOOOF=True) after running CV.\")\n",
    "elif \"logreg_predictions_df\" not in globals():\n",
    "    print(\"No logreg_predictions_df found – run the logistic regression cell first.\")\n",
    "elif \"psd_cube\" not in globals() or \"psd_freqs\" not in globals() or \"feature_channels\" not in globals():\n",
    "    print(\"No PSD data found – run the feature extraction cell first.\")\n",
    "elif \"SpectralModel\" not in globals() or SpectralModel is None:\n",
    "    print(\"SpectralModel (specparam/FOOOF) is not available in this environment.\")\n",
    "else:\n",
    "    # Choose evaluation predictions (smoothed vs raw)\n",
    "    pred_col = 'y_pred'\n",
    "    if 'USE_TIME_ADJUSTMENT' in globals() and USE_TIME_ADJUSTMENT and 'y_pred_smooth' in logreg_predictions_df.columns:\n",
    "        pred_col = 'y_pred_smooth'\n",
    "\n",
    "    df = logreg_predictions_df.copy()\n",
    "    df['y_eval'] = df[pred_col].astype(int)\n",
    "    df['y_true'] = df['y_true'].astype(int)\n",
    "\n",
    "    # Auto-select subject with the lowest accuracy (among subjects present in predictions)\n",
    "    subj_acc = df.groupby('subject_id').apply(lambda g: float(np.mean(g['y_true'].to_numpy() == g['y_eval'].to_numpy())))\n",
    "    subj_acc = subj_acc.sort_values(ascending=True)\n",
    "    if subj_acc.empty:\n",
    "        raise RuntimeError(\"No subject predictions available to compute accuracy.\")\n",
    "    auto_subject = int(subj_acc.index[0])\n",
    "    subj_id = int(MAIN_FOOOF_SUBJECT) if MAIN_FOOOF_SUBJECT is not None else auto_subject\n",
    "    if subj_id not in subj_acc.index:\n",
    "        raise ValueError(f\"Subject {subj_id} not found in predictions. Available subjects: {list(map(int, subj_acc.index[:10]))}...\")\n",
    "    print(f\"Selected subject: {subj_id} (accuracy={subj_acc.loc[subj_id]:.3f}, auto_lowest={auto_subject})\")\n",
    "\n",
    "    # ROI selection (same logic used for alpha profile)\n",
    "    roi_names = [ch for ch in (ALPHA_PROFILE_ROI if 'ALPHA_PROFILE_ROI' in globals() else []) if ch in feature_channels]\n",
    "    if not roi_names:\n",
    "        roi_names = list(feature_channels)\n",
    "    roi_idx = [feature_channels.index(ch) for ch in roi_names]\n",
    "\n",
    "    freqs_arr = np.asarray(psd_freqs, float)\n",
    "\n",
    "    def _fit_main_fooof(freqs_fit, spectrum_fit):\n",
    "        model = SpectralModel(**FOOOF_SETTINGS) if 'FOOOF_SETTINGS' in globals() else SpectralModel()\n",
    "        # Fit on provided spectrum (already restricted if caller masked)\n",
    "        model.fit(freqs_fit, spectrum_fit)\n",
    "        freqs_plot = np.asarray(getattr(model, 'freqs', freqs_fit))\n",
    "        psd_plot = np.asarray(getattr(model, 'power_spectrum', spectrum_fit))\n",
    "\n",
    "        # Reconstruct aperiodic fit from aperiodic_params_\n",
    "        ap_params = np.asarray(getattr(model, 'aperiodic_params_', []), float)\n",
    "        if ap_params.size == 0:\n",
    "            ap_fit = np.zeros_like(freqs_plot, dtype=float)\n",
    "        else:\n",
    "            if ap_params.size == 2:\n",
    "                offset, exponent = ap_params\n",
    "                ap_fit = offset - exponent * np.log10(freqs_plot)\n",
    "            elif ap_params.size == 3:\n",
    "                offset, knee, exponent = ap_params\n",
    "                ap_fit = offset - np.log10(knee + freqs_plot ** exponent)\n",
    "            else:\n",
    "                ap_fit = np.zeros_like(freqs_plot, dtype=float)\n",
    "\n",
    "        # Main alpha peak (strongest peak inside ALPHA_PROFILE_RANGE)\n",
    "        gauss_main = None\n",
    "        peaks = np.asarray(getattr(model, 'peak_params_', []), float)\n",
    "        if peaks.size:\n",
    "            lo_alpha, hi_alpha = (ALPHA_PROFILE_RANGE if 'ALPHA_PROFILE_RANGE' in globals() else (8.0, 12.0))\n",
    "            mask_peaks = (peaks[:, 0] >= lo_alpha) & (peaks[:, 0] <= hi_alpha)\n",
    "            if np.any(mask_peaks):\n",
    "                subset = peaks[mask_peaks]\n",
    "                best = subset[np.argmax(subset[:, 1])]\n",
    "                cf, amp, bw = map(float, best[:3])\n",
    "                if bw > 0:\n",
    "                    sigma = bw / (2.0 * math.sqrt(2.0 * math.log(2.0)))\n",
    "                    gauss_main = amp * np.exp(-0.5 * ((freqs_plot - cf) / sigma) ** 2)\n",
    "        return freqs_plot, psd_plot, ap_fit, gauss_main, model\n",
    "\n",
    "    def _plot_fooof_overlay(freqs_plot, psd_plot, ap_fit, gauss_main, title):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(freqs_plot, psd_plot, label='PSD', color='#1f77b4')\n",
    "        plt.plot(freqs_plot, ap_fit, label='Aperiodic fit', color='#ff7f0e', linestyle='--')\n",
    "        if gauss_main is not None:\n",
    "            plt.plot(freqs_plot, ap_fit + gauss_main, label='Aperiodic + main alpha', color='#2ca02c')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Power (model units)')\n",
    "        plt.title(title)\n",
    "        plt.grid(alpha=0.25)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # -------------------- Plot 1: subject mean ROI PSD --------------------\n",
    "    subj_mask = (subject_ids == subj_id)\n",
    "    if not np.any(subj_mask):\n",
    "        raise RuntimeError(f\"No epochs found in psd_cube for subject {subj_id}.\")\n",
    "    subj_cube = psd_cube[subj_mask][:, roi_idx, :]  # (n_epochs, n_roi, n_freqs)\n",
    "    mean_spectrum = np.nanmean(subj_cube, axis=(0, 1))\n",
    "    if not np.any(np.isfinite(mean_spectrum)):\n",
    "        raise RuntimeError(f\"Mean spectrum for subject {subj_id} is non-finite.\")\n",
    "\n",
    "    fit_lo, fit_hi = (ALPHA_FREQ_RANGE if 'ALPHA_FREQ_RANGE' in globals() else (freqs_arr[0], freqs_arr[-1]))\n",
    "    fit_lo = max(float(fit_lo), float(freqs_arr[0]))\n",
    "    fit_hi = min(float(fit_hi), float(freqs_arr[-1]))\n",
    "    fit_mask = (freqs_arr >= fit_lo) & (freqs_arr <= fit_hi)\n",
    "    freqs_fit = freqs_arr[fit_mask]\n",
    "    spec_fit = mean_spectrum[fit_mask]\n",
    "\n",
    "    freqs_plot, psd_plot, ap_fit, gauss_main, _ = _fit_main_fooof(freqs_fit, spec_fit)\n",
    "    _plot_fooof_overlay(\n",
    "        freqs_plot,\n",
    "        psd_plot,\n",
    "        ap_fit,\n",
    "        gauss_main,\n",
    "        title=f\"Subject {subj_id}: averaged ROI PSD + main FOOOF (ROI={roi_names})\",\n",
    "    )\n",
    "\n",
    "    # -------------------- Plot 2/3: example misclassifications for this subject --------------------\n",
    "    df_subj = df[df['subject_id'] == subj_id].copy()\n",
    "    if df_subj.empty:\n",
    "        print(f\"No predictions found for subject {subj_id}.\")\n",
    "    else:\n",
    "        # Helper to choose an example row\n",
    "        def _pick_example(mask):\n",
    "            cand = df_subj.loc[mask].copy()\n",
    "            if cand.empty:\n",
    "                return None\n",
    "            if MIS_EXAMPLE_STRATEGY == 'first':\n",
    "                return cand.iloc[0]\n",
    "            # most_confident: pick the wrong prediction with highest model confidence\n",
    "            # prob_ec is P(class==1). If predicted 1 -> confidence=prob_ec; predicted 0 -> confidence=1-prob_ec\n",
    "            prob = cand['prob_ec'].astype(float).to_numpy()\n",
    "            pred = cand['y_eval'].astype(int).to_numpy()\n",
    "            conf = np.where(pred == 1, prob, 1.0 - prob)\n",
    "            return cand.iloc[int(np.argmax(conf))]\n",
    "\n",
    "        # EC (true=1) misclassified as EO (pred=0)\n",
    "        row_ec2eo = _pick_example((df_subj['y_true'] == 1) & (df_subj['y_eval'] == 0))\n",
    "        if row_ec2eo is None:\n",
    "            print(f\"No EC→EO misclassifications found for subject {subj_id}.\")\n",
    "        else:\n",
    "            idx = int(row_ec2eo['epoch_idx'])\n",
    "            spec_epoch = np.nanmean(psd_cube[idx][roi_idx, :], axis=0)\n",
    "            freqs_plot2, psd_plot2, ap_fit2, gauss2, _ = _fit_main_fooof(freqs_fit, spec_epoch[fit_mask])\n",
    "            _plot_fooof_overlay(\n",
    "                freqs_plot2,\n",
    "                psd_plot2,\n",
    "                ap_fit2,\n",
    "                gauss2,\n",
    "                title=(\n",
    "                    f\"Misclassified EC→EO (subject {subj_id}) — epoch_idx {idx} — \"\n",
    "                    f\"prob_ec={float(row_ec2eo['prob_ec']):.3f} ({pred_col})\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # EO (true=0) misclassified as EC (pred=1)\n",
    "        row_eo2ec = _pick_example((df_subj['y_true'] == 0) & (df_subj['y_eval'] == 1))\n",
    "        if row_eo2ec is None:\n",
    "            print(f\"No EO→EC misclassifications found for subject {subj_id}.\")\n",
    "        else:\n",
    "            idx = int(row_eo2ec['epoch_idx'])\n",
    "            spec_epoch = np.nanmean(psd_cube[idx][roi_idx, :], axis=0)\n",
    "            freqs_plot3, psd_plot3, ap_fit3, gauss3, _ = _fit_main_fooof(freqs_fit, spec_epoch[fit_mask])\n",
    "            _plot_fooof_overlay(\n",
    "                freqs_plot3,\n",
    "                psd_plot3,\n",
    "                ap_fit3,\n",
    "                gauss3,\n",
    "                title=(\n",
    "                    f\"Misclassified EO→EC (subject {subj_id}) — epoch_idx {idx} — \"\n",
    "                    f\"prob_ec={float(row_eo2ec['prob_ec']):.3f} ({pred_col})\"\n",
    "                ),\n",
    "            )\n",
    "# ---- End main-FOOOF misclassification analysis ----\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
