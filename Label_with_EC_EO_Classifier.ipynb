{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d5004b",
   "metadata": {},
   "source": [
    "# Label unlabeled `Preprocessed_setfiles` with `EC_EO_Classifier` runs\n",
    "\n",
    "This notebook replaces the role of `Ensembling_for_labeling.ipynb`, but uses **your** classifier pipeline outputs from:\n",
    "\n",
    "- `New_EEG/EC_EO_Classifier.ipynb` (trained runs)\n",
    "- `New_EEG/outputs/<run_folder>/` (model artifacts)\n",
    "\n",
    "It loops through **all** `.set` files in:\n",
    "\n",
    "- `G:\\\\ChristianMusaeus\\\\Preprocessed_setfiles`\n",
    "\n",
    "…and writes one `label_predictions.csv` **per run** under `LABELING_DIR` (default `G:\\ChristianMusaeus\\labeling`):\n",
    "\n",
    "- `.../preprocessed_setfiles/<run_folder>/label_predictions.csv`\n",
    "\n",
    "For FOOOF runs it tries to reuse the precomputed cache from:\n",
    "\n",
    "- `New_EEG/outputs/saved_fooof/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import platform\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "# -----------------\n",
    "# User config\n",
    "# -----------------\n",
    "\n",
    "# Dataset root (unlabeled setfiles)\n",
    "DATASET_DIR = os.getenv(\"PREPROCESSED_SETFILES_DIR\", r\"G:\\\\ChristianMusaeus\\\\Preprocessed_setfiles\")\n",
    "RECURSIVE = True\n",
    "\n",
    "# If channels are generic (\"Ch1\"..\"Ch19\"), optionally map them to standard 10–20 names.\n",
    "# NOTE: This assumes the dataset uses the common 19-channel ordering.\n",
    "AUTO_RENAME_CH1_TO_1020 = True\n",
    "CH1_TO_1020_ORDER_19 = [\n",
    "    \"Fp1\",\n",
    "\"Fp2\",\n",
    "\"F3\",\n",
    "\"F4\",\n",
    "\"C3\",\n",
    "\"C4\",\n",
    "\"P3\",\n",
    "\"P4\",\n",
    "\"O1\",\n",
    "\"O2\",\n",
    "\"F7\",\n",
    "\"F8\",\n",
    "\"T7\",\n",
    "\"T8\",\n",
    "\"P7\",\n",
    "\"P8\",\n",
    "\"Fz\",\n",
    "\"Cz\",\n",
    "\"Pz\",\n",
    "]\n",
    "\n",
    "# Use cached ONE_MAIN_FOOOF features if available (strongly recommended)\n",
    "USE_SAVED_FOOOF = True\n",
    "\n",
    "# Write per-run label_predictions.csv\n",
    "OVERWRITE = False\n",
    "\n",
    "# Classification threshold for turning prob_ec into a label (0/1)\n",
    "THRESHOLD_EC = 0.5\n",
    "\n",
    "# Limit files for quick testing (None = all)\n",
    "MAX_FILES: Optional[int] = None\n",
    "\n",
    "# -----------------\n",
    "# Paths\n",
    "# -----------------\n",
    "\n",
    "def _detect_notebook_path() -> Optional[Path]:\n",
    "    try:\n",
    "        vsc = globals().get(\"__vsc_ipynb_file__\", None)\n",
    "        if vsc:\n",
    "            p = Path(str(vsc)).expanduser()\n",
    "            if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for key in (\"NOTEBOOK_PATH\", \"IPYNB_PATH\"):\n",
    "        v = os.getenv(key)\n",
    "        if v:\n",
    "            try:\n",
    "                p = Path(v).expanduser()\n",
    "                if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                    return p.resolve()\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        here = Path.cwd().resolve()\n",
    "        for _ in range(6):\n",
    "            cand = here / \"New_EEG\" / \"Project-main\" / \"Label_with_EC_EO_Classifier.ipynb\"\n",
    "            if cand.exists():\n",
    "                return cand.resolve()\n",
    "            here = here.parent\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "NOTEBOOK_PATH = _detect_notebook_path()\n",
    "NOTEBOOK_DIR = NOTEBOOK_PATH.parent if NOTEBOOK_PATH is not None else Path.cwd().resolve()\n",
    "NEW_EEG_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUTS_ROOT = NEW_EEG_DIR / \"outputs\"\n",
    "\n",
    "# Output roots (can be outside repo; defaults point to your shared drive)\n",
    "SAVED_FOOOF_DIR = os.getenv(\"SAVED_FOOOF_DIR\", r\"G:\\ChristianMusaeus\\saved_fooof\")\n",
    "LABELING_DIR = os.getenv(\"LABELING_DIR\", r\"G:\\ChristianMusaeus\\labeling\")\n",
    "\n",
    "def resolve_output_dir(p: str, fallback: Path) -> Path:\n",
    "    candidates = []\n",
    "    p_str = str(p)\n",
    "\n",
    "    if platform.system() == \"Windows\":\n",
    "        candidates.append(p_str)\n",
    "    else:\n",
    "        # WSL drive-letter conversion: G:\\... -> /mnt/g/...\n",
    "        try:\n",
    "            if \"microsoft\" in platform.uname().release.lower():\n",
    "                if len(p_str) >= 3 and p_str[1] == \":\" and p_str[0].isalpha():\n",
    "                    drive = p_str[0].lower()\n",
    "                    rest = p_str[2:].lstrip(\"\\\\/\").replace(\"\\\\\", \"/\")\n",
    "                    candidates.append(f\"/mnt/{drive}/{rest}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    candidates.append(str(fallback))\n",
    "\n",
    "    for c in candidates:\n",
    "        if not c:\n",
    "            continue\n",
    "        try:\n",
    "            pp = Path(str(c)).expanduser()\n",
    "            pp.mkdir(parents=True, exist_ok=True)\n",
    "            return pp.resolve()\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"Could not create output dir: {p}\")\n",
    "\n",
    "SAVED_FOOOF_ROOT = resolve_output_dir(SAVED_FOOOF_DIR, OUTPUTS_ROOT / \"saved_fooof\")\n",
    "LABELING_ROOT = resolve_output_dir(LABELING_DIR, OUTPUTS_ROOT / \"labeling\") / \"preprocessed_setfiles\"\n",
    "LABELING_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saved-FOOOF root:\", SAVED_FOOOF_ROOT)\n",
    "print(\"Labeling root:\", LABELING_ROOT)\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"New_EEG dir:\", NEW_EEG_DIR)\n",
    "print(\"Outputs root:\", OUTPUTS_ROOT)\n",
    "\n",
    "def _safe_tag(s: str) -> str:\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", str(s))\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s[:160] if len(s) > 160 else s\n",
    "\n",
    "def is_wsl() -> bool:\n",
    "    try:\n",
    "        return \"microsoft\" in platform.uname().release.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _maybe_wsl_path(p: str) -> Optional[str]:\n",
    "    m = re.match(r\"^([A-Za-z]):[\\\\\\\\/](.*)$\", str(p))\n",
    "    if not m:\n",
    "        return None\n",
    "    drive = m.group(1).lower()\n",
    "    rest = m.group(2).replace(\"\\\\\\\\\", \"/\").replace(\"\\\\\", \"/\")\n",
    "    return f\"/mnt/{drive}/{rest}\"\n",
    "\n",
    "def resolve_dataset_dir() -> Path:\n",
    "    candidates = [DATASET_DIR]\n",
    "    w = _maybe_wsl_path(str(DATASET_DIR))\n",
    "    if w:\n",
    "        candidates.append(w)\n",
    "    candidates.append(r\"/mnt/g/ChristianMusaeus/Preprocessed_setfiles\")\n",
    "    for c in candidates:\n",
    "        if not c:\n",
    "            continue\n",
    "        try:\n",
    "            p = Path(str(c)).expanduser()\n",
    "            if p.exists() and p.is_dir():\n",
    "                return p.resolve()\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"Could not resolve DATASET_DIR: {DATASET_DIR}\")\n",
    "\n",
    "DATASET_PATH = resolve_dataset_dir()\n",
    "print(\"Resolved dataset dir:\", DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------\n",
    "# Dataset sanity check: show file + channel info\n",
    "# -----------------\n",
    "\n",
    "# Collect .set files (lightweight preview)\n",
    "pats = (\"*.set\", \"*.SET\")\n",
    "set_files = sorted({p.resolve() for pat in pats for p in Path(DATASET_PATH).rglob(pat)}) if RECURSIVE else sorted({p.resolve() for pat in pats for p in Path(DATASET_PATH).glob(pat)})\n",
    "print(\"Resolved dataset dir:\", DATASET_PATH)\n",
    "print(\"Recursive:\", RECURSIVE)\n",
    "print(\"Total .set files:\", len(set_files))\n",
    "if set_files:\n",
    "    print(\"First file:\", set_files[0])\n",
    "    if len(set_files) > 1:\n",
    "        print(\"Last file:\", set_files[-1])\n",
    "\n",
    "# Load the first file and print channel metadata\n",
    "if not set_files:\n",
    "    raise FileNotFoundError(f\"No .set files found under {DATASET_PATH}\")\n",
    "\n",
    "first_path = set_files[0]\n",
    "epochs = mne.io.read_epochs_eeglab(str(first_path), verbose='ERROR')\n",
    "\n",
    "print(\"\\n--- MNE epochs info (first file) ---\")\n",
    "print(\"n_epochs:\", len(epochs))\n",
    "print(\"n_channels:\", len(epochs.ch_names))\n",
    "print(\"sfreq:\", float(epochs.info['sfreq']))\n",
    "print(\"ch_names:\", epochs.ch_names)\n",
    "\n",
    "print(\"\\n--- Channel dict entries (epochs.info['chs']) ---\")\n",
    "for ch in epochs.info['chs']:\n",
    "    # Print a compact subset of fields that are usually informative\n",
    "    print({\n",
    "        'ch_name': ch.get('ch_name'),\n",
    "        'kind': ch.get('kind'),\n",
    "        'unit': ch.get('unit'),\n",
    "        'coil_type': ch.get('coil_type'),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398291ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Detect the 4 run folders (old/new × fooof/no_fooof)\n",
    "# -----------------\n",
    "\n",
    "def list_run_folders(outputs_root: Path) -> List[Path]:\n",
    "    if not outputs_root.exists():\n",
    "        return []\n",
    "    out = []\n",
    "    for p in outputs_root.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        if p.name in {\"saved_fooof\", \"comparisons\", \"labeling\"}:\n",
    "            continue\n",
    "        if p.name.startswith(\"compare__\"):\n",
    "            continue\n",
    "        out.append(p)\n",
    "    return sorted(out, key=lambda x: x.name)\n",
    "\n",
    "def auto_pick_four_runs(outputs_root: Path) -> Dict[str, Path]:\n",
    "    runs = list_run_folders(outputs_root)\n",
    "    by_name = {p.name: p for p in runs}\n",
    "    picked: Dict[str, Path] = {}\n",
    "    for p in runs:\n",
    "        name = p.name\n",
    "        if name.startswith(\"old_dataset__\") and \"__fooof__\" in name:\n",
    "            picked[\"old_fooof\"] = p\n",
    "        if name.startswith(\"old_dataset__\") and \"__no_fooof__\" in name:\n",
    "            picked[\"old_no_fooof\"] = p\n",
    "        if name.startswith(\"new_dataset__\") and \"__fooof__\" in name:\n",
    "            picked[\"new_fooof\"] = p\n",
    "        if name.startswith(\"new_dataset__\") and \"__no_fooof__\" in name:\n",
    "            picked[\"new_no_fooof\"] = p\n",
    "    # sanity\n",
    "    missing = [k for k in (\"old_fooof\", \"old_no_fooof\", \"new_fooof\", \"new_no_fooof\") if k not in picked]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Could not auto-detect the 4 runs in {outputs_root}. Missing: {missing}. Found: {list(by_name)[:8]}...\")\n",
    "    return picked\n",
    "\n",
    "RUNS = auto_pick_four_runs(OUTPUTS_ROOT)\n",
    "for k, p in RUNS.items():\n",
    "    print(f\"{k}: {p.name}\")\n",
    "\n",
    "# Which classifier runs to apply\n",
    "# - \"all\": run all 4 back-to-back\n",
    "# - \"single\": run just RUN_KEY\n",
    "# - \"custom\": run RUN_KEYS_CUSTOM\n",
    "RUN_MODE = \"custom\"\n",
    "RUN_KEY = \"old_fooof\"\n",
    "RUN_KEYS_CUSTOM = [\"old_fooof\", \"new_no_fooof\" , \"new_fooof\"]\n",
    "\n",
    "if RUN_MODE == \"all\":\n",
    "    RUN_KEYS_TO_PROCESS = [\"old_fooof\", \"old_no_fooof\", \"new_fooof\", \"new_no_fooof\"]\n",
    "elif RUN_MODE == \"single\":\n",
    "    RUN_KEYS_TO_PROCESS = [str(RUN_KEY)]\n",
    "elif RUN_MODE == \"custom\":\n",
    "    RUN_KEYS_TO_PROCESS = [str(x) for x in RUN_KEYS_CUSTOM]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown RUN_MODE: {RUN_MODE}\")\n",
    "\n",
    "missing = [k for k in RUN_KEYS_TO_PROCESS if k not in RUNS]\n",
    "if missing:\n",
    "    raise ValueError(f\"Unknown run keys: {missing}. Available: {list(RUNS)}\")\n",
    "\n",
    "# Persist selection for reproducibility\n",
    "cfg = {\n",
    "    \"DATASET_DIR\": str(DATASET_PATH),\n",
    "    \"USE_SAVED_FOOOF\": bool(USE_SAVED_FOOOF),\n",
    "    \"THRESHOLD_EC\": float(THRESHOLD_EC),\n",
    "    \"MAX_FILES\": MAX_FILES,\n",
    "    \"RUNS\": {k: str(v) for k, v in RUNS.items()},\n",
    "    \"RUN_MODE\": str(RUN_MODE),\n",
    "    \"RUN_KEY\": str(RUN_KEY),\n",
    "    \"RUN_KEYS_CUSTOM\": list(RUN_KEYS_CUSTOM),\n",
    "    \"RUN_KEYS_TO_PROCESS\": list(RUN_KEYS_TO_PROCESS),\n",
    "}\n",
    "(LABELING_ROOT / f\"labeling_config__{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\").write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Helpers: channels, PSD, saved-FOOOF loader\n",
    "# -----------------\n",
    "\n",
    "def canonical_channel_name(ch_name: str) -> str:\n",
    "    name = str(ch_name).strip()\n",
    "    name = re.sub(r\"^EEG\\s+\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"-REF$\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \"\", name)\n",
    "    key = name.upper()\n",
    "    if globals().get('AUTO_RENAME_CH1_TO_1020', False):\n",
    "        order = globals().get('CH1_TO_1020_ORDER_19', None)\n",
    "        if isinstance(order, (list, tuple)) and len(order) == 19:\n",
    "            ch_map = {f'CH{i+1}': str(order[i]) for i in range(19)}\n",
    "            if key in ch_map:\n",
    "                return ch_map[key]\n",
    "    return name\n",
    "\n",
    "def rename_epochs_channels_canonical(epochs):\n",
    "    new_names = [canonical_channel_name(ch) for ch in epochs.ch_names]\n",
    "    if len(set(new_names)) != len(new_names):\n",
    "        return epochs\n",
    "    mapping = {old: new for old, new in zip(epochs.ch_names, new_names) if old != new}\n",
    "    if mapping:\n",
    "        epochs.rename_channels(mapping)\n",
    "    return epochs\n",
    "\n",
    "def parse_subject_id(path: Path) -> int:\n",
    "    m = re.search(r\"(\\d{3,})\", path.stem)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return int(abs(hash(path.stem)) % 1_000_000_000)\n",
    "\n",
    "def psd_array_welch_clean(data: np.ndarray, sfreq: float, fmin=1.0, fmax=45.0, target_secs=2.0):\n",
    "    n_epochs, _, n_times = data.shape\n",
    "    n_per_seg = max(8, min(n_times, int(round(float(target_secs) * float(sfreq)))))\n",
    "    n_overlap = n_per_seg // 2 if n_per_seg >= 16 else 0\n",
    "    psds, freqs = mne.time_frequency.psd_array_welch(\n",
    "        data,\n",
    "        sfreq=float(sfreq),\n",
    "        fmin=float(fmin),\n",
    "        fmax=float(fmax),\n",
    "        n_per_seg=int(n_per_seg),\n",
    "        n_overlap=int(n_overlap),\n",
    "        window=\"hann\",\n",
    "        average=\"mean\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    return psds, freqs\n",
    "\n",
    "def reduce_freq_resolution(psd_cube: np.ndarray, n_bins: int) -> np.ndarray:\n",
    "    n_samples, n_channels, n_freqs = psd_cube.shape\n",
    "    bin_size = n_freqs // int(n_bins)\n",
    "    if bin_size == 0:\n",
    "        raise ValueError(f\"n_bins={n_bins} is too high for n_freqs={n_freqs}\")\n",
    "    trimmed = psd_cube[:, :, : bin_size * int(n_bins)]\n",
    "    trimmed = trimmed.reshape(n_samples, n_channels, int(n_bins), bin_size)\n",
    "    reduced = trimmed.mean(axis=3)\n",
    "    return reduced\n",
    "\n",
    "def _find_saved_fooof_npz(file_path: Path, subject_id: int) -> Optional[Path]:\n",
    "    if not SAVED_FOOOF_ROOT.exists():\n",
    "        return None\n",
    "    file_tag = _safe_tag(file_path.stem)\n",
    "    pattern = f\"features_subject_{int(subject_id)}__{file_tag}.npz\"\n",
    "    matches = [p for p in SAVED_FOOOF_ROOT.rglob(pattern) if p.is_file()]\n",
    "    if not matches:\n",
    "        return None\n",
    "    # prefer matches that sit under cache_b\n",
    "    matches = sorted(matches, key=lambda p: (\"cache_b\" not in str(p.parent), len(str(p))))\n",
    "    return matches[0]\n",
    "\n",
    "def load_saved_fooof_features(npz_path: Path) -> Tuple[np.ndarray, List[str]]:\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    X = np.asarray(d[\"X\"], dtype=float)\n",
    "    names = [str(x) for x in np.asarray(d[\"feature_names\"]).ravel().tolist()]\n",
    "    return X, names\n",
    "\n",
    "def align_by_feature_names(X: np.ndarray, names: List[str], desired_names: List[str]) -> np.ndarray:\n",
    "    # Map (channel, feat) -> column index using canonical channel key\n",
    "    mapping: Dict[Tuple[str, str], int] = {}\n",
    "    for idx, n in enumerate(names):\n",
    "        try:\n",
    "            ch_part, feat = str(n).rsplit(\"_\", 1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        mapping[(canonical_channel_name(ch_part).upper(), str(feat))] = int(idx)\n",
    "    out = np.zeros((int(X.shape[0]), int(len(desired_names))), dtype=float)\n",
    "    for j, dn in enumerate(desired_names):\n",
    "        ch_part, feat = str(dn).rsplit(\"_\", 1)\n",
    "        src = mapping.get((canonical_channel_name(ch_part).upper(), str(feat)), None)\n",
    "        if src is None:\n",
    "            continue\n",
    "        out[:, j] = X[:, src]\n",
    "    return np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def make_desired_fooof_feature_names(feature_channels: List[str], selected_feats: List[str]) -> List[str]:\n",
    "    base_order = [\"offset\", \"exponent\", \"alpha_cf\", \"alpha_amp\", \"alpha_bw\"]\n",
    "    selected = [f for f in (selected_feats or base_order) if f in base_order]\n",
    "    if not selected:\n",
    "        selected = base_order\n",
    "    names = []\n",
    "    for ch in feature_channels:\n",
    "        for feat in selected:\n",
    "            names.append(f\"{ch}_{feat}\")\n",
    "    return names\n",
    "\n",
    "def load_feature_channels(run_dir: Path) -> Optional[List[str]]:\n",
    "    p = run_dir / \"feature_channels.npy\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    arr = np.load(p, allow_pickle=True)\n",
    "    return [str(x) for x in np.asarray(arr).ravel().tolist()]\n",
    "\n",
    "def load_psd_freqs(run_dir: Path) -> Optional[np.ndarray]:\n",
    "    p = run_dir / \"psd_freqs.npy\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    return np.asarray(np.load(p), dtype=float).ravel()\n",
    "\n",
    "# -----------------\n",
    "# FOOOF backend + ONE_MAIN_FOOOF on-the-fly fallback\n",
    "# -----------------\n",
    "\n",
    "# Defaults mirror `New_EEG/EC_EO_Classifier.ipynb`.\n",
    "ALPHA_FREQ_RANGE = (3.0, 40.0)\n",
    "ALPHA_PROFILE_RANGE = (4.0, 16.0)\n",
    "ALPHA_PROFILE_ROI = [\"O1\", \"O2\", \"P3\", \"P4\", \"P7\", \"P8\", \"Pz\"]\n",
    "FOOOF_SETTINGS = {\n",
    "    \"aperiodic_mode\": \"fixed\",\n",
    "    \"peak_width_limits\": (0.5, 12.0),\n",
    "    \"max_n_peaks\": 6,\n",
    "    \"min_peak_height\": 0.05,\n",
    "    \"peak_threshold\": 2.0,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "SpectralModel = None\n",
    "FitError = Exception\n",
    "try:\n",
    "    from specparam import SpectralModel as _SpecModel\n",
    "    try:\n",
    "        from specparam.core.errors import FitError as _FitError\n",
    "    except Exception:\n",
    "        class _FitError(Exception):\n",
    "            pass\n",
    "    SpectralModel = _SpecModel\n",
    "    FitError = _FitError\n",
    "    FOOOF_BACKEND = \"specparam\"\n",
    "except Exception:\n",
    "    try:\n",
    "        from fooof import FOOOF as _FooofModel\n",
    "        from fooof.core.errors import FitError as _FitError\n",
    "        SpectralModel = _FooofModel\n",
    "        FitError = _FitError\n",
    "        FOOOF_BACKEND = \"fooof\"\n",
    "    except Exception:\n",
    "        SpectralModel = None\n",
    "        FitError = Exception\n",
    "        FOOOF_BACKEND = \"unavailable\"\n",
    "\n",
    "def select_alpha_peak(peaks: np.ndarray, lo: float, hi: float):\n",
    "    peaks_arr = np.asarray(peaks, float)\n",
    "    if peaks_arr.size == 0:\n",
    "        return None\n",
    "    if peaks_arr.ndim == 1:\n",
    "        peaks_arr = peaks_arr.reshape(1, -1)\n",
    "    mask = (peaks_arr[:, 0] >= float(lo)) & (peaks_arr[:, 0] <= float(hi))\n",
    "    if not np.any(mask):\n",
    "        return None\n",
    "    subset = peaks_arr[mask]\n",
    "    return subset[np.argmax(subset[:, 1])]\n",
    "\n",
    "def build_alpha_profile(freqs: np.ndarray, psd_cube: np.ndarray, channels: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"Fit a single alpha peak on the mean ROI spectrum (returns alpha_cf, alpha_bw).\"\"\"\n",
    "    if SpectralModel is None:\n",
    "        return 0.0, 0.0\n",
    "    roi_names = [ch for ch in ALPHA_PROFILE_ROI if ch in channels]\n",
    "    if not roi_names:\n",
    "        roi_idx = list(range(len(channels)))\n",
    "    else:\n",
    "        roi_idx = [channels.index(ch) for ch in roi_names]\n",
    "    roi_cube = psd_cube[:, roi_idx, :]\n",
    "    mean_spectrum = np.nanmean(roi_cube, axis=(0, 1))\n",
    "    if not np.any(np.isfinite(mean_spectrum)):\n",
    "        return 0.0, 0.0\n",
    "    try:\n",
    "        model = SpectralModel(**FOOOF_SETTINGS)\n",
    "        lo, hi = ALPHA_PROFILE_RANGE\n",
    "        model.fit(np.asarray(freqs, float), np.asarray(mean_spectrum, float), freq_range=(float(lo), float(hi)))\n",
    "        peaks = np.asarray(getattr(model, \"peak_params_\", []))\n",
    "        chosen = select_alpha_peak(peaks, float(lo), float(hi))\n",
    "        if chosen is None:\n",
    "            return 0.0, 0.0\n",
    "        cf, _, bw = map(float, chosen[:3])\n",
    "        return float(cf), float(bw)\n",
    "    except Exception:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def compute_one_main_fooof_features(freqs: np.ndarray, psd_cube: np.ndarray, subject_id: int, alpha_profile_map, include_aperiodic: bool = True) -> np.ndarray:\n",
    "    \"\"\"Compute ONE_MAIN_FOOOF features (matches `New_EEG/EC_EO_Classifier.ipynb`).\"\"\"\n",
    "    if SpectralModel is None:\n",
    "        raise RuntimeError(\"FOOOF backend unavailable\")\n",
    "    subj = int(subject_id)\n",
    "    profile = alpha_profile_map.get(subj) if alpha_profile_map is not None else None\n",
    "    has_profile = profile is not None and len(profile) == 2\n",
    "    if has_profile:\n",
    "        alpha_cf, alpha_bw = map(float, profile)\n",
    "    else:\n",
    "        alpha_cf, alpha_bw = 0.0, 0.0\n",
    "    freqs_arr = np.asarray(freqs, float)\n",
    "    import math\n",
    "    if has_profile and alpha_bw > 0:\n",
    "        sigma = float(alpha_bw) / (2.0 * math.sqrt(2.0 * math.log(2.0)))\n",
    "        gauss = np.exp(-0.5 * ((freqs_arr - alpha_cf) / sigma) ** 2)\n",
    "    else:\n",
    "        gauss = np.zeros_like(freqs_arr)\n",
    "    denom = float(np.sum(gauss ** 2)) if gauss.size else 0.0\n",
    "    features = []\n",
    "    ap_settings = dict(FOOOF_SETTINGS)\n",
    "    try:\n",
    "        ap_settings[\"max_n_peaks\"] = 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    for epoch_psd in psd_cube:\n",
    "        epoch_feats = []\n",
    "        for spectrum in epoch_psd:\n",
    "            try:\n",
    "                if not np.all(np.isfinite(spectrum)):\n",
    "                    raise ValueError(\"Non-finite in spectrum\")\n",
    "                offset, exponent = 0.0, 0.0\n",
    "                alpha_amp = 0.0\n",
    "                if include_aperiodic or has_profile:\n",
    "                    model = SpectralModel(**ap_settings)\n",
    "                    model.fit(freqs_arr, spectrum, freq_range=ALPHA_FREQ_RANGE)\n",
    "                    if hasattr(model, \"aperiodic_params_\"):\n",
    "                        params = np.asarray(model.aperiodic_params_)\n",
    "                        if params.size > 0:\n",
    "                            offset = float(params[0])\n",
    "                        if params.size > 1:\n",
    "                            exponent = float(params[1])\n",
    "                    try:\n",
    "                        ap_fit = None\n",
    "                        get_fun = getattr(model, \"get_model_spectrum\", None)\n",
    "                        if callable(get_fun):\n",
    "                            ap_fit = np.asarray(get_fun(freqs_arr))\n",
    "                    except Exception:\n",
    "                        ap_fit = None\n",
    "                    if ap_fit is None:\n",
    "                        for name in (\"fooofed_spectrum_\", \"modeled_spectrum_\", \"model_spectrum_\", \"model_spectrum__\"):\n",
    "                            if hasattr(model, name):\n",
    "                                ap_fit = np.asarray(getattr(model, name))\n",
    "                                break\n",
    "                    if ap_fit is None or ap_fit.shape != spectrum.shape:\n",
    "                        ap_fit = np.zeros_like(spectrum)\n",
    "                    if has_profile and denom > 0.0:\n",
    "                        residual = spectrum - ap_fit\n",
    "                        num = float(np.sum(gauss * residual))\n",
    "                        alpha_amp = max(num / denom, 0.0)\n",
    "                epoch_feats.extend([offset, exponent, alpha_cf if has_profile else 0.0, alpha_amp, alpha_bw if has_profile else 0.0])\n",
    "            except (FitError, RuntimeError, ValueError, np.linalg.LinAlgError):\n",
    "                epoch_feats.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        features.append(epoch_feats)\n",
    "    return np.asarray(features, dtype=float)\n",
    "\n",
    "def select_from_full_one_main(full_X: np.ndarray, n_channels: int, selected_feats: List[str]) -> np.ndarray:\n",
    "    base_order = [\"offset\", \"exponent\", \"alpha_cf\", \"alpha_amp\", \"alpha_bw\"]\n",
    "    selected = [f for f in (selected_feats or base_order) if f in base_order]\n",
    "    if not selected:\n",
    "        selected = base_order\n",
    "    stride = len(base_order)\n",
    "    keep_offsets = [base_order.index(s) for s in selected]\n",
    "    idx = []\n",
    "    for ch_i in range(int(n_channels)):\n",
    "        base = ch_i * stride\n",
    "        for off in keep_offsets:\n",
    "            idx.append(base + off)\n",
    "    return np.asarray(full_X, dtype=float)[:, idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6bc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Main labeling loop\n",
    "# -----------------\n",
    "\n",
    "def collect_set_files(directory: Path, recursive: bool = True) -> List[Path]:\n",
    "    if not directory.exists():\n",
    "        return []\n",
    "    pats = (\"*.set\", \"*.SET\")\n",
    "    if recursive:\n",
    "        files = [p.resolve() for pat in pats for p in directory.rglob(pat)]\n",
    "    else:\n",
    "        files = [p.resolve() for pat in pats for p in directory.glob(pat)]\n",
    "    # de-dup preserve order\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for p in sorted(files, key=lambda x: str(x)):\n",
    "        s = str(p)\n",
    "        if s in seen:\n",
    "            continue\n",
    "        seen.add(s)\n",
    "        out.append(p)\n",
    "    return out\n",
    "\n",
    "all_files = collect_set_files(DATASET_PATH, recursive=RECURSIVE)\n",
    "if MAX_FILES is not None:\n",
    "    all_files = all_files[: int(MAX_FILES)]\n",
    "print(\".set files to process:\", len(all_files))\n",
    "if all_files:\n",
    "    print(\"Example:\", all_files[0])\n",
    "\n",
    "def _fmt_secs(seconds: float) -> str:\n",
    "    seconds = max(0.0, float(seconds))\n",
    "    m, s = divmod(int(round(seconds)), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:d}h{m:02d}m\" if h else f\"{m:d}m{s:02d}s\"\n",
    "\n",
    "t_global = time.time()\n",
    "for run_key in RUN_KEYS_TO_PROCESS:\n",
    "    run_dir = RUNS[run_key]\n",
    "    run_name = run_dir.name\n",
    "    is_fooof = \"__fooof__\" in run_name\n",
    "    out_dir = LABELING_ROOT / run_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = out_dir / \"label_predictions.csv\"\n",
    "    log_path = out_dir / \"labeling_log.csv\"\n",
    "\n",
    "    if out_csv.exists() and (not OVERWRITE):\n",
    "        print(f\"[SKIP] {run_key}: output exists: {out_csv}\")\n",
    "        continue\n",
    "\n",
    "    # Load run artifacts\n",
    "    model_path = run_dir / \"final_model_lr.pkl\"\n",
    "    scaler_path = run_dir / \"final_scaler_lr.pkl\"\n",
    "    imputer_path = run_dir / \"final_imputer_lr.pkl\"\n",
    "    component_path = run_dir / \"final_component_lr.pkl\"\n",
    "    if not (model_path.exists() and scaler_path.exists() and imputer_path.exists()):\n",
    "        raise RuntimeError(f\"Missing model artifacts in {run_dir}\")\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    imputer = joblib.load(imputer_path)\n",
    "    component = joblib.load(component_path) if component_path.exists() else None\n",
    "\n",
    "    feature_channels = load_feature_channels(run_dir)\n",
    "    psd_freqs_ref = load_psd_freqs(run_dir)\n",
    "\n",
    "    selected_feats = None\n",
    "    if is_fooof:\n",
    "        sf = run_dir / \"fooof_selected_features.npy\"\n",
    "        if sf.exists():\n",
    "            selected_feats = [str(x) for x in np.asarray(np.load(sf, allow_pickle=True)).ravel().tolist()]\n",
    "        else:\n",
    "            selected_feats = [\"offset\", \"exponent\", \"alpha_amp\"]\n",
    "\n",
    "    n_bins = None\n",
    "    if not is_fooof:\n",
    "        nb = run_dir / \"final_n_bins_lr.npy\"\n",
    "        if not nb.exists():\n",
    "            raise RuntimeError(f\"Missing final_n_bins_lr.npy for PSD run: {run_dir}\")\n",
    "        n_bins = int(np.asarray(np.load(nb)).ravel()[0])\n",
    "\n",
    "    # Open output CSVs\n",
    "    with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out, log_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_log:\n",
    "        w = csv.DictWriter(\n",
    "            f_out,\n",
    "            fieldnames=[\"Test subject ID\", \"Epoch number\", \"Label\", \"Probability\", \"prob_ec\", \"run\", \"file\"],\n",
    "        )\n",
    "        w.writeheader()\n",
    "        wlog = csv.DictWriter(\n",
    "            f_log,\n",
    "            fieldnames=[\"file\", \"subject_id\", \"status\", \"n_epochs\", \"note\"],\n",
    "        )\n",
    "        wlog.writeheader()\n",
    "\n",
    "        print(f\"\\n[RUN] {run_key}: {run_name}\")\n",
    "        print(\"  features:\", \"FOOOF\" if is_fooof else \"PSD\", \"| n_bins:\", n_bins)\n",
    "        print(\"  output:\", out_csv)\n",
    "\n",
    "        t_run = time.time()\n",
    "        for i, path in enumerate(all_files, start=1):\n",
    "            subj = parse_subject_id(path)\n",
    "            try:\n",
    "                epochs = mne.io.read_epochs_eeglab(str(path), verbose=\"ERROR\")\n",
    "                epochs = rename_epochs_channels_canonical(epochs)\n",
    "                sfreq = float(epochs.info[\"sfreq\"])\n",
    "\n",
    "                # Skip files with NaNs (same policy as the original project)\n",
    "                data_full = epochs.get_data()\n",
    "                if not np.all(np.isfinite(data_full)):\n",
    "                    wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"skipped\", \"n_epochs\": int(len(epochs)), \"note\": \"NaN/Inf in raw data\"})\n",
    "                    continue\n",
    "\n",
    "                # Determine channel order to match the trained run\n",
    "                if feature_channels is None:\n",
    "                    # fallback: use all EEG channels in file order\n",
    "                    picks = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=\"bads\")\n",
    "                    if len(picks) == 0:\n",
    "                        picks = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=[])\n",
    "                    channels = [epochs.ch_names[idx] for idx in picks]\n",
    "                else:\n",
    "                    channels = list(feature_channels)\n",
    "\n",
    "                # Load features\n",
    "                if is_fooof:\n",
    "                    desired_names = make_desired_fooof_feature_names(channels, selected_feats)\n",
    "                    X_raw = None\n",
    "                    note = \"\"\n",
    "                    if USE_SAVED_FOOOF:\n",
    "                        npz = _find_saved_fooof_npz(path, subject_id=int(subj))\n",
    "                        if npz is not None:\n",
    "                            X_saved, names_saved = load_saved_fooof_features(npz)\n",
    "                            X_raw = align_by_feature_names(X_saved, names_saved, desired_names)\n",
    "                            note = f\"cache:{npz.name}\"\n",
    "\n",
    "                    # Fallback: compute ONE_MAIN_FOOOF features on-the-fly when cache is missing.\n",
    "                    if X_raw is None:\n",
    "                        if SpectralModel is None:\n",
    "                            wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"skipped\", \"n_epochs\": int(len(epochs)), \"note\": \"Missing saved_fooof cache and no specparam/fooof installed\"})\n",
    "                            continue\n",
    "\n",
    "                        # Compute PSD for the desired channel list, filling missing channels with zeros.\n",
    "                        lookup = {canonical_channel_name(ch).upper(): idx for idx, ch in enumerate(epochs.ch_names)}\n",
    "                        present_pairs = [(out_i, lookup.get(canonical_channel_name(ch).upper(), None)) for out_i, ch in enumerate(channels)]\n",
    "                        have = [(o, i_in) for o, i_in in present_pairs if i_in is not None]\n",
    "                        if not have:\n",
    "                            wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"skipped\", \"n_epochs\": int(len(epochs)), \"note\": \"No matching channels for on-the-fly FOOOF\"})\n",
    "                            continue\n",
    "                        out_idx, in_idx = zip(*have)\n",
    "                        data_present = epochs.get_data(picks=list(in_idx))\n",
    "                        target_secs = 1.0 if data_present.shape[-1] < sfreq * 3 else 2.0\n",
    "                        psd_present, freqs = psd_array_welch_clean(data_present, sfreq=sfreq, fmin=1.0, fmax=45.0, target_secs=target_secs)\n",
    "                        psd_full = np.full((psd_present.shape[0], len(channels), psd_present.shape[2]), np.nan, dtype=float)\n",
    "                        psd_full[:, list(out_idx), :] = psd_present\n",
    "                        psd_full = np.nan_to_num(psd_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                        alpha_cf, alpha_bw = build_alpha_profile(freqs, psd_full, channels)\n",
    "                        alpha_profile_map = {int(subj): (float(alpha_cf), float(alpha_bw))}\n",
    "                        full_X = compute_one_main_fooof_features(freqs=freqs, psd_cube=psd_full, subject_id=int(subj), alpha_profile_map=alpha_profile_map, include_aperiodic=True)\n",
    "                        X_raw = select_from_full_one_main(full_X, n_channels=len(channels), selected_feats=selected_feats)\n",
    "                        note = \"computed_on_fly\"\n",
    "\n",
    "                else:\n",
    "                    # PSD features: compute PSD for all epochs\n",
    "                    if feature_channels is None:\n",
    "                        picks = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=\"bads\")\n",
    "                        if len(picks) == 0:\n",
    "                            picks = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=[])\n",
    "                        data = epochs.get_data(picks=picks)\n",
    "                        psd, freqs = psd_array_welch_clean(data, sfreq=sfreq, fmin=1.0, fmax=45.0, target_secs=2.0)\n",
    "                        psd_full = psd\n",
    "                    else:\n",
    "                        # compute PSD only for channels present, then place into full channel array\n",
    "                        lookup = {canonical_channel_name(ch).upper(): idx for idx, ch in enumerate(epochs.ch_names)}\n",
    "                        present_pairs = [(out_i, lookup.get(canonical_channel_name(ch).upper(), None)) for out_i, ch in enumerate(feature_channels)]\n",
    "                        have = [(o, i_in) for o, i_in in present_pairs if i_in is not None]\n",
    "                        if not have:\n",
    "                            wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"skipped\", \"n_epochs\": int(len(epochs)), \"note\": \"No matching channels\"})\n",
    "                            continue\n",
    "                        out_idx, in_idx = zip(*have)\n",
    "                        data_present = epochs.get_data(picks=list(in_idx))\n",
    "                        psd_present, freqs = psd_array_welch_clean(data_present, sfreq=sfreq, fmin=1.0, fmax=45.0, target_secs=2.0)\n",
    "                        psd_full = np.full((psd_present.shape[0], len(feature_channels), psd_present.shape[2]), np.nan, dtype=float)\n",
    "                        psd_full[:, list(out_idx), :] = psd_present\n",
    "\n",
    "                    if psd_freqs_ref is not None:\n",
    "                        if freqs.shape != psd_freqs_ref.shape or not np.allclose(freqs, psd_freqs_ref):\n",
    "                            wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"skipped\", \"n_epochs\": int(len(epochs)), \"note\": \"PSD freqs mismatch vs trained run\"})\n",
    "                            continue\n",
    "\n",
    "                    psd_full = np.nan_to_num(psd_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                    if n_bins is not None:\n",
    "                        reduced = reduce_freq_resolution(psd_full, int(n_bins))\n",
    "                        X_raw = reduced.reshape(reduced.shape[0], -1)\n",
    "                    else:\n",
    "                        X_raw = psd_full.reshape(psd_full.shape[0], -1)\n",
    "\n",
    "                # Apply preprocessing and predict\n",
    "                X_imp = imputer.transform(X_raw)\n",
    "                X_scaled = scaler.transform(X_imp)\n",
    "                X_proc = component.transform(X_scaled) if component is not None else X_scaled\n",
    "\n",
    "                prob_ec = model.predict_proba(X_proc)[:, 1].astype(float)\n",
    "                y_pred = (prob_ec >= float(THRESHOLD_EC)).astype(int)\n",
    "                prob_pred = np.where(y_pred == 1, prob_ec, 1.0 - prob_ec)\n",
    "\n",
    "                for ep in range(int(prob_ec.shape[0])):\n",
    "                    w.writerow(\n",
    "                        {\n",
    "                            \"Test subject ID\": int(subj),\n",
    "                            \"Epoch number\": int(ep),\n",
    "                            \"Label\": int(y_pred[ep]),\n",
    "                            \"Probability\": float(prob_pred[ep]),\n",
    "                            \"prob_ec\": float(prob_ec[ep]),\n",
    "                            \"run\": str(run_name),\n",
    "                            \"file\": str(path),\n",
    "                        }\n",
    "                    )\n",
    "                wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"ok\", \"n_epochs\": int(prob_ec.shape[0]), \"note\": note if is_fooof else \"\"})\n",
    "\n",
    "            except Exception as exc:\n",
    "                wlog.writerow({\"file\": str(path), \"subject_id\": int(subj), \"status\": \"error\", \"n_epochs\": 0, \"note\": str(exc)[:240]})\n",
    "\n",
    "            if i % 25 == 0:\n",
    "                elapsed = time.time() - t_run\n",
    "                avg = elapsed / max(i, 1)\n",
    "                eta = avg * max(len(all_files) - i, 0)\n",
    "                print(f\"  [{i}/{len(all_files)}] elapsed {_fmt_secs(elapsed)} | ETA {_fmt_secs(eta)}\")\n",
    "\n",
    "        print(f\"[DONE] {run_key} in {_fmt_secs(time.time() - t_run)}\")\n",
    "\n",
    "print(\"All runs done. Total elapsed:\", _fmt_secs(time.time() - t_global))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
