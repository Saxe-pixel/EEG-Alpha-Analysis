{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification Analysis\n",
    "\n",
    "This notebook performs comprehensive analysis of labeled EEG dataset with FOOOF parameters:\n",
    "- Mean center frequency (FOOOF CF) vs age\n",
    "- Top 60 EC/EO epochs selection\n",
    "- Mean absolute alpha power calculations\n",
    "- Visualizations and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "This notebook analyzes classifier labeling outputs and relates them to age/sex metadata.\n",
    "\n",
    "Key ideas:\n",
    "- Epoch selection is based on classifier confidence (from `prob_ec`).\n",
    "- Relative alpha is computed from the EEG time series using Welch PSD (alpha 8–13 Hz divided by total 1–40 Hz).\n",
    "- Expensive steps write caches under the run-specific `OUTPUT_DIR` so reruns can be faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from mne.time_frequency import psd_array_welch\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (repo-connected + Windows/WSL friendly) ---\n",
    "import platform\n",
    "\n",
    "def _detect_notebook_path() -> Optional[Path]:\n",
    "    \"\"\"Detect the current notebook path (best-effort).\"\"\"\n",
    "    try:\n",
    "        vsc = globals().get(\"__vsc_ipynb_file__\", None)\n",
    "        if vsc:\n",
    "            p = Path(str(vsc)).expanduser()\n",
    "            if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for key in (\"NOTEBOOK_PATH\", \"IPYNB_PATH\"):\n",
    "        v = os.getenv(key)\n",
    "        if v:\n",
    "            try:\n",
    "                p = Path(v).expanduser()\n",
    "                if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                    return p.resolve()\n",
    "            except Exception:\n",
    "                pass\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for _ in range(6):\n",
    "        cand = cwd / \"New_EEG\" / \"analysis_script.ipynb\"\n",
    "        if cand.exists():\n",
    "            return cand.resolve()\n",
    "        cwd = cwd.parent\n",
    "    return None\n",
    "\n",
    "def _is_wsl() -> bool:\n",
    "    try:\n",
    "        return \"microsoft\" in platform.uname().release.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resolve_windows_path(p: str) -> Path:\n",
    "    \"\"\"Convert Windows drive paths when running on WSL; otherwise return as-is.\"\"\"\n",
    "    s = str(p)\n",
    "    if _is_wsl():\n",
    "        m = re.match(r\"^([A-Za-z]):[\\\\/](.*)$\", s)\n",
    "        if m:\n",
    "            drive = m.group(1).lower()\n",
    "            rest = m.group(2).replace(\"\\\\\", \"/\")\n",
    "            return Path(f\"/mnt/{drive}/{rest}\")\n",
    "    return Path(s)\n",
    "\n",
    "NOTEBOOK_PATH = _detect_notebook_path()\n",
    "NOTEBOOK_DIR = NOTEBOOK_PATH.parent if NOTEBOOK_PATH is not None else Path.cwd().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"New_EEG\" else NOTEBOOK_DIR\n",
    "\n",
    "# Which labeling run to analyze\n",
    "RUN_FOLDER = os.getenv(\n",
    "    \"RUN_FOLDER\",\n",
    "    \"old_dataset__fooof__allch__cv2__time_align_conditions__one_main_fooof__mainfooof_all_epochs__pen_l2\",\n",
    ")\n",
    "\n",
    "# --- Analysis toggles ---\n",
    "# Drop center-frequency datapoints where cf==0 (often indicates a failed/empty fit)\n",
    "DROP_ZERO_CF = os.getenv(\"DROP_ZERO_CF\", \"1\").strip() not in {\"0\", \"false\", \"False\"}\n",
    "ZERO_CF_EPS = float(os.getenv(\"ZERO_CF_EPS\", \"0\"))  # set e.g. 1e-6 to drop near-zero too\n",
    "\n",
    "# Label mapping used throughout this notebook (from Label_with_EC_EO_Classifier):\n",
    "# prob_ec = P(EC), so label 1 == EC and label 0 == EO\n",
    "LABEL_EO = 0\n",
    "LABEL_EC = 1\n",
    "\n",
    "# Epoch-selection settings (section 6)\n",
    "CONF_THRESH = float(os.getenv(\"CONF_THRESH\", \"0.90\"))  # symmetric rule uses prob_ec >= t for EC and <= (1-t) for EO\n",
    "USE_SELECTION_CACHE = os.getenv(\"USE_SELECTION_CACHE\", \"1\").strip() not in {\"0\", \"false\", \"False\"}\n",
    "\n",
    "# Epoch selection mode:\n",
    "# - \"threshold\": keep ALL epochs above CONF_THRESH (symmetric rule on prob_ec)\n",
    "# - \"top_k\": keep TOP_K most confident epochs per subject per class (EC and EO)\n",
    "SELECTION_MODE = os.getenv(\"SELECTION_MODE\", \"top_k\").strip().lower()  # threshold | top_k\n",
    "TOP_K = int(os.getenv(\"TOP_K\", \"60\"))\n",
    "\n",
    "# Alpha power computation settings (section 7)\n",
    "USE_ALPHA_CACHE = os.getenv(\"USE_ALPHA_CACHE\", \"1\").strip() not in {\"0\", \"false\", \"False\"}\n",
    "PSD_ALPHA_FMIN = 8.0\n",
    "PSD_ALPHA_FMAX = 13.0\n",
    "PSD_TOTAL_FMIN = 1.0\n",
    "PSD_TOTAL_FMAX = 40.0\n",
    "# Backwards-compatible aliases\n",
    "PSD_FMIN = PSD_ALPHA_FMIN\n",
    "PSD_FMAX = PSD_ALPHA_FMAX\n",
    "PSD_N_FFT = 200  # matches Project-main/mean_alpha_power.ipynb defaults\n",
    "PSD_CHUNK_EPOCHS = 200  # process epochs in chunks to limit memory\n",
    "OCCIPITAL_ROI = [\"O1\", \"O2\"]  # matches Project-main/mean_alpha_power.ipynb Step 5\n",
    "\n",
    "# If channels are generic (\"Ch1\"..\"Ch19\"), optionally map them to standard 10–20 names.\n",
    "# NOTE: This assumes the dataset uses the common 19-channel ordering (Ch1..Ch19).\n",
    "AUTO_RENAME_CH1_TO_1020 = os.getenv(\"AUTO_RENAME_CH1_TO_1020\", \"1\").strip() not in {\"0\", \"false\", \"False\"}\n",
    "CH1_TO_1020_ORDER_19 = [\n",
    "    \"Fp1\", \"Fp2\", \"F3\", \"F4\", \"C3\", \"C4\", \"P3\", \"P4\", \"O1\", \"O2\",\n",
    "    \"F7\", \"F8\", \"T7\", \"T8\", \"P7\", \"P8\", \"Fz\", \"Cz\", \"Pz\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Labeling outputs root (external default on Windows; repo fallback)\n",
    "LABELING_DIR = os.getenv(\"LABELING_DIR\", r\"G:\\\\ChristianMusaeus\\\\labeling\")\n",
    "labeling_root = resolve_windows_path(LABELING_DIR)\n",
    "if labeling_root.name.lower() == \"labeling\":\n",
    "    labeling_root = labeling_root / \"preprocessed_setfiles\"\n",
    "if not labeling_root.exists():\n",
    "    labeling_root = NOTEBOOK_DIR / \"outputs\" / \"labeling\" / \"preprocessed_setfiles\"\n",
    "\n",
    "# Labeled predictions CSV (override with LABEL_PREDICTIONS_CSV)\n",
    "LABELED_DATASET_PATH = os.getenv(\n",
    "    \"LABEL_PREDICTIONS_CSV\",\n",
    "    str(labeling_root / RUN_FOLDER / \"label_predictions.csv\"),\n",
    ")\n",
    "\n",
    "# Saved ONE_MAIN_FOOOF cache root (override with SAVED_FOOOF_DIR)\n",
    "_saved_fooof_env = os.getenv(\"SAVED_FOOOF_DIR\", \"\") or os.getenv(\"SAVED_FOOOF_ROOT\", \"\")\n",
    "if _saved_fooof_env:\n",
    "    SAVED_FOOOF_PATH = str(resolve_windows_path(_saved_fooof_env))\n",
    "else:\n",
    "    cand_win = Path(r\"G:\\\\ChristianMusaeus\\\\saved_fooof\")\n",
    "    if platform.system() == \"Windows\" and cand_win.exists():\n",
    "        SAVED_FOOOF_PATH = str(cand_win)\n",
    "    else:\n",
    "        SAVED_FOOOF_PATH = str(NOTEBOOK_DIR / \"outputs\" / \"saved_fooof\")\n",
    "\n",
    "# Metadata (old/new external defaults; override with METADATA_CSV)\n",
    "METADATA_OLD_DEFAULT = os.getenv(\"METADATA_OLD_CSV\", r\"G:\\\\ChristianMusaeus\\\\metadata_time_filtered.csv\")\n",
    "METADATA_NEW_DEFAULT = os.getenv(\"METADATA_NEW_CSV\", r\"G:\\\\ChristianMusaeus\\\\EEG_sub_data_pseudoanonym.csv\")\n",
    "default_meta = METADATA_NEW_DEFAULT if str(RUN_FOLDER).startswith(\"new_dataset\") else METADATA_OLD_DEFAULT\n",
    "METADATA_PATH = os.getenv(\"METADATA_CSV\", default_meta)\n",
    "meta_path_obj = resolve_windows_path(METADATA_PATH)\n",
    "if not meta_path_obj.exists():\n",
    "    # Repo fallback (old metadata is available in-repo; new metadata may not be)\n",
    "    cand_repo = REPO_ROOT / \"data\" / \"metadata\" / \"metadata_time_filtered.csv\"\n",
    "    if cand_repo.exists():\n",
    "        meta_path_obj = cand_repo\n",
    "METADATA_PATH = str(meta_path_obj)\n",
    "\n",
    "# Output folder (always under this notebook's outputs)\n",
    "OUTPUTS_ROOT = NOTEBOOK_DIR / \"outputs\"\n",
    "OUTPUT_DIR = str(OUTPUTS_ROOT / \"analysis_script\" / RUN_FOLDER)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Notebook dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"Outputs root: {OUTPUTS_ROOT}\")\n",
    "print(f\"Run folder: {RUN_FOLDER}\")\n",
    "print(f\"Label predictions: {LABELED_DATASET_PATH}\")\n",
    "print(f\"Saved FOOOF root: {SAVED_FOOOF_PATH}\")\n",
    "print(f\"Metadata: {METADATA_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load labeled predictions CSV files and concatenate them.\"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        # Single CSV file\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory with multiple CSV files\n",
    "        csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in {path}\")\n",
    "        \n",
    "        all_data = []\n",
    "        for csv_file in csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Extract subject ID from filename if not in dataframe\n",
    "            if 'subject_id' not in df.columns and 'Test subject ID' not in df.columns:\n",
    "                filename = os.path.basename(csv_file)\n",
    "                # Try to extract subject ID from filename\n",
    "                parts = filename.replace('.csv', '').split('_')\n",
    "                for part in parts:\n",
    "                    if part.startswith('sub-'):\n",
    "                        df['subject_id'] = part\n",
    "                        break\n",
    "            all_data.append(df)\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Path not found: {path}\")\n",
    "\n",
    "def normalize_subject_id(subject_id) -> str:\n",
    "    \"\"\"Normalize subject ID to consistent format (sub-XXX).\"\"\"\n",
    "    if pd.isna(subject_id):\n",
    "        return None\n",
    "    \n",
    "    subject_str = str(subject_id).strip()\n",
    "    \n",
    "    # If already in sub-XXX format\n",
    "    if subject_str.startswith('sub-'):\n",
    "        return subject_str\n",
    "    \n",
    "    # If it's a number, convert to sub-XXX\n",
    "    try:\n",
    "        subject_num = int(float(subject_str))\n",
    "        return f\"sub-{subject_num:03d}\"\n",
    "    except (ValueError, TypeError):\n",
    "        # If it's not a number, try to extract number from string\n",
    "        match = re.search(r'\\d+', subject_str)\n",
    "        if match:\n",
    "            subject_num = int(match.group())\n",
    "            return f\"sub-{subject_num:03d}\"\n",
    "    \n",
    "    return subject_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_subject_num(subject_id) -> Optional[int]:\n",
    "    # Extract numeric subject id from strings like 'sub-001', 'Sub001', 1.\n",
    "    if subject_id is None or (isinstance(subject_id, float) and np.isnan(subject_id)):\n",
    "        return None\n",
    "    s = str(subject_id).strip()\n",
    "    m = re.search(r\"(\\d+)\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return int(m.group(1))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_SAVED_FOOOF_INDEX = None\n",
    "_SAVED_FOOOF_INDEX_ROOT = None\n",
    "\n",
    "def _build_saved_fooof_index(saved_fooof_path: str) -> Dict[str, str]:\n",
    "    # Build an index {subject_key -> npz_path} by scanning saved_fooof_path.\n",
    "    root = Path(saved_fooof_path)\n",
    "    index: Dict[str, str] = {}\n",
    "    if not root.exists():\n",
    "        return index\n",
    "\n",
    "    npz_files = list(root.rglob('*.npz'))\n",
    "    for p in npz_files:\n",
    "        p_str = str(p).lower()\n",
    "        name = p.name.lower()\n",
    "\n",
    "        m = re.search(r\"(?:subject|sub)[-_]?(\\d{1,6})\", name)\n",
    "        if not m:\n",
    "            m = re.search(r\"(\\d{3})\", name)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        z3 = f\"{n:03d}\"\n",
    "        keys = [\n",
    "            str(n),\n",
    "            z3,\n",
    "            f\"sub-{z3}\",\n",
    "            f\"sub_{z3}\",\n",
    "            f\"sub{z3}\",\n",
    "            f\"subject_{n}\",\n",
    "            f\"subject_{z3}\",\n",
    "        ]\n",
    "\n",
    "        for k in keys:\n",
    "            if k not in index:\n",
    "                index[k] = str(p)\n",
    "            else:\n",
    "                prev = index[k].lower()\n",
    "                if 'cache_b' in p_str and 'cache_b' not in prev:\n",
    "                    index[k] = str(p)\n",
    "\n",
    "    print(f\"Indexed {len(npz_files)} .npz files from: {saved_fooof_path}\")\n",
    "    print(f\"Index contains {len(index)} subject keys\")\n",
    "    return index\n",
    "\n",
    "def find_saved_fooof_npz(subject_id, saved_fooof_path: str) -> Optional[str]:\n",
    "    # Find saved FOOOF .npz file for a given subject (fast path + robust fallback).\n",
    "    if not os.path.exists(saved_fooof_path):\n",
    "        print(f\"Saved FOOOF path not found: {saved_fooof_path}\")\n",
    "        return None\n",
    "\n",
    "    subject_num = _extract_subject_num(subject_id)\n",
    "    if subject_num is None:\n",
    "        return None\n",
    "\n",
    "    z3 = f\"{subject_num:03d}\"\n",
    "\n",
    "    # Fast glob patterns\n",
    "    patterns = [\n",
    "        f\"*subject_{subject_num}*.npz\",\n",
    "        f\"*subject_{z3}*.npz\",\n",
    "        f\"*sub-{z3}*.npz\",\n",
    "        f\"*sub_{z3}*.npz\",\n",
    "        f\"*sub{z3}*.npz\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = glob.glob(os.path.join(saved_fooof_path, \"**\", pattern), recursive=True)\n",
    "        if matches:\n",
    "            cache_b = [m for m in matches if 'cache_b' in m.lower()]\n",
    "            return cache_b[0] if cache_b else matches[0]\n",
    "\n",
    "    # Robust fallback: indexed scan (built once per root)\n",
    "    global _SAVED_FOOOF_INDEX, _SAVED_FOOOF_INDEX_ROOT\n",
    "    if _SAVED_FOOOF_INDEX is None or _SAVED_FOOOF_INDEX_ROOT != saved_fooof_path:\n",
    "        _SAVED_FOOOF_INDEX = _build_saved_fooof_index(saved_fooof_path)\n",
    "        _SAVED_FOOOF_INDEX_ROOT = saved_fooof_path\n",
    "\n",
    "    keys = [\n",
    "        f\"sub-{z3}\",\n",
    "        f\"sub_{z3}\",\n",
    "        f\"sub{z3}\",\n",
    "        z3,\n",
    "        str(subject_num),\n",
    "        f\"subject_{subject_num}\",\n",
    "        f\"subject_{z3}\",\n",
    "    ]\n",
    "    for k in keys:\n",
    "        if k in _SAVED_FOOOF_INDEX:\n",
    "            return _SAVED_FOOOF_INDEX[k]\n",
    "\n",
    "    return None\n",
    "\n",
    "def load_fooof_data(npz_path: str) -> Tuple[Optional[np.ndarray], Optional[List[str]], Optional[float]]:\n",
    "    # Load saved FOOOF data from an .npz file and return (X, feature_names, alpha_cf).\n",
    "    try:\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "        X = None\n",
    "        feature_names = None\n",
    "        alpha_cf = None\n",
    "\n",
    "        if 'X' in data:\n",
    "            X = np.asarray(data['X'], dtype=float)\n",
    "        if 'feature_names' in data:\n",
    "            feature_names = [str(x) for x in np.asarray(data['feature_names']).ravel().tolist()]\n",
    "\n",
    "        # Support a few possible key names\n",
    "        for k in ('alpha_cf', 'cf', 'center_frequency', 'alpha_center_frequency'):\n",
    "            if k in data:\n",
    "                arr = np.asarray(data[k])\n",
    "                if arr.size:\n",
    "                    alpha_cf = float(np.nanmean(arr))\n",
    "                break\n",
    "\n",
    "        return X, feature_names, alpha_cf\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {npz_path}: {e}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alpha_power_from_features(X: np.ndarray, feature_names: List[str], channel_filter: Optional[List[str]] = None) -> np.ndarray:\n",
    "    \"\"\"Extract alpha power (alpha_amp) from feature matrix.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (n_epochs x n_features)\n",
    "        feature_names: List of feature names\n",
    "        channel_filter: Optional list of channels to include (e.g., occipital channels)\n",
    "    \n",
    "    Returns:\n",
    "        alpha_power: Array of alpha power per epoch (mean across selected channels)\n",
    "    \"\"\"\n",
    "    if X is None or feature_names is None:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Find indices of alpha_amp features\n",
    "    alpha_amp_indices = []\n",
    "    alpha_amp_channels = []\n",
    "    \n",
    "    for idx, name in enumerate(feature_names):\n",
    "        if name.endswith('_alpha_amp'):\n",
    "            # Extract channel name (e.g., \"O1_alpha_amp\" -> \"O1\")\n",
    "            channel = name.replace('_alpha_amp', '').upper()\n",
    "            \n",
    "            # Filter by channel if specified\n",
    "            if channel_filter is None or channel in [c.upper() for c in channel_filter]:\n",
    "                alpha_amp_indices.append(idx)\n",
    "                alpha_amp_channels.append(channel)\n",
    "    \n",
    "    if not alpha_amp_indices:\n",
    "        # No alpha_amp features found\n",
    "        return np.zeros(X.shape[0])\n",
    "    \n",
    "    # Extract alpha power values (absolute value)\n",
    "    alpha_power = np.abs(X[:, alpha_amp_indices])\n",
    "    \n",
    "    # Mean across selected channels\n",
    "    mean_alpha_power = np.nanmean(alpha_power, axis=1)\n",
    "    \n",
    "    return mean_alpha_power\n",
    "\n",
    "def get_occipital_channels() -> List[str]:\n",
    "    \"\"\"Return list of occipital channel names.\"\"\"\n",
    "    return ['O1', 'O2', 'OZ', 'PO3', 'POZ', 'PO4']\n",
    "\n",
    "def canonical_channel_name(ch_name: str) -> str:\n",
    "    name = str(ch_name).strip()\n",
    "    name = re.sub(r\"^EEG\\s+\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"-REF$\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \"\", name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def maybe_rename_ch1_to_1020(epochs):\n",
    "    # If channels are named Ch1..Ch19 (generic), rename them to a standard 10–20 montage.\n",
    "    # Safe only when you are confident about the ordering.\n",
    "    if not globals().get('AUTO_RENAME_CH1_TO_1020', False):\n",
    "        return epochs\n",
    "\n",
    "    try:\n",
    "        names = [str(n).strip() for n in epochs.ch_names]\n",
    "    except Exception:\n",
    "        return epochs\n",
    "\n",
    "    if len(names) != 19:\n",
    "        return epochs\n",
    "\n",
    "    # Accept CH1/ch1/Ch1\n",
    "    nums = []\n",
    "    for n in names:\n",
    "        m = re.fullmatch(r\"(?i)ch(\\d+)\", n)\n",
    "        if not m:\n",
    "            return epochs\n",
    "        nums.append(int(m.group(1)))\n",
    "\n",
    "    if set(nums) != set(range(1, 20)):\n",
    "        return epochs\n",
    "\n",
    "    order = globals().get('CH1_TO_1020_ORDER_19', None)\n",
    "    if not order or len(order) != 19:\n",
    "        return epochs\n",
    "\n",
    "    mapping = {orig: str(order[num-1]) for orig, num in zip(names, nums)}\n",
    "\n",
    "    # Avoid accidental duplicate names\n",
    "    if len(set(mapping.values())) != len(mapping.values()):\n",
    "        return epochs\n",
    "\n",
    "    try:\n",
    "        epochs.rename_channels(mapping)\n",
    "    except Exception:\n",
    "        return epochs\n",
    "\n",
    "    return epochs\n",
    "\n",
    "def load_epochs_any(path_str: str):\n",
    "    # Load epoched EEG from either EEGLAB .set or MNE .fif epochs.\n",
    "    p = resolve_windows_path(path_str)\n",
    "    suf = p.suffix.lower()\n",
    "    if suf == '.set':\n",
    "        epochs = mne.io.read_epochs_eeglab(str(p), verbose='ERROR')\n",
    "    else:\n",
    "        epochs = mne.read_epochs(str(p), verbose='ERROR')\n",
    "\n",
    "    # Optional: map generic Ch1..Ch19 to 10–20 names\n",
    "    epochs = maybe_rename_ch1_to_1020(epochs)\n",
    "\n",
    "    # Canonicalize names (strip EEG prefix / -REF suffix) to match ROI filters\n",
    "    try:\n",
    "        epochs.rename_channels({ch: canonical_channel_name(ch) for ch in epochs.ch_names})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return epochs\n",
    "\n",
    "\n",
    "def mean_relative_alpha_psd(epochs, epoch_indices: np.ndarray, picks_occ: np.ndarray) -> tuple[float, float, int]:\n",
    "    # Compute mean relative alpha = bandpower(8-13 Hz) / bandpower(1-40 Hz)\n",
    "    # Returns (relative_all_channels, relative_occipital, n_epochs_used)\n",
    "    epoch_indices = np.asarray(epoch_indices, dtype=int)\n",
    "    if epoch_indices.size == 0:\n",
    "        return float('nan'), float('nan'), 0\n",
    "\n",
    "    picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude='bads')\n",
    "    if len(picks_all) == 0:\n",
    "        picks_all = np.arange(len(epochs.ch_names), dtype=int)\n",
    "\n",
    "    # Map occipital picks (original channel indices) to indices within picks_all\n",
    "    occ_in_picks_all = []\n",
    "    if picks_occ is not None and len(picks_occ) > 0:\n",
    "        idx_map = {int(full): i for i, full in enumerate(picks_all)}\n",
    "        occ_in_picks_all = [idx_map[int(x)] for x in np.asarray(picks_occ, dtype=int) if int(x) in idx_map]\n",
    "\n",
    "    sum_alpha_ch = None\n",
    "    sum_total_ch = None\n",
    "    n_used = 0\n",
    "\n",
    "    for start in range(0, int(epoch_indices.size), int(PSD_CHUNK_EPOCHS)):\n",
    "        chunk = epoch_indices[start:start+int(PSD_CHUNK_EPOCHS)]\n",
    "        try:\n",
    "            data = epochs[chunk].get_data()\n",
    "        except Exception:\n",
    "            data = epochs.get_data()[chunk]\n",
    "\n",
    "        data = data[:, picks_all, :]\n",
    "\n",
    "        psds_a, _ = psd_array_welch(\n",
    "            data,\n",
    "            sfreq=float(epochs.info['sfreq']),\n",
    "            fmin=float(PSD_ALPHA_FMIN),\n",
    "            fmax=float(PSD_ALPHA_FMAX),\n",
    "            n_fft=int(PSD_N_FFT),\n",
    "            verbose=False,\n",
    "        )\n",
    "        psds_t, _ = psd_array_welch(\n",
    "            data,\n",
    "            sfreq=float(epochs.info['sfreq']),\n",
    "            fmin=float(PSD_TOTAL_FMIN),\n",
    "            fmax=float(PSD_TOTAL_FMAX),\n",
    "            n_fft=int(PSD_N_FFT),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # integrate across frequency bins to approximate bandpower\n",
    "        pow_a = psds_a.sum(axis=-1)  # (epochs, channels)\n",
    "        pow_t = psds_t.sum(axis=-1)  # (epochs, channels)\n",
    "\n",
    "        if sum_alpha_ch is None:\n",
    "            sum_alpha_ch = np.zeros(pow_a.shape[1], dtype=np.float64)\n",
    "            sum_total_ch = np.zeros(pow_t.shape[1], dtype=np.float64)\n",
    "\n",
    "        sum_alpha_ch += np.nansum(pow_a, axis=0)\n",
    "        sum_total_ch += np.nansum(pow_t, axis=0)\n",
    "        n_used += int(pow_a.shape[0])\n",
    "\n",
    "    if sum_alpha_ch is None or sum_total_ch is None or n_used == 0:\n",
    "        return float('nan'), float('nan'), 0\n",
    "\n",
    "    mean_alpha_ch = sum_alpha_ch / float(n_used)\n",
    "    mean_total_ch = sum_total_ch / float(n_used)\n",
    "\n",
    "    rel_all = float(np.nanmean(mean_alpha_ch) / np.nanmean(mean_total_ch)) if np.nanmean(mean_total_ch) else float('nan')\n",
    "\n",
    "    rel_occ = float('nan')\n",
    "    if occ_in_picks_all:\n",
    "        denom = float(np.nanmean(mean_total_ch[occ_in_picks_all]))\n",
    "        if denom:\n",
    "            rel_occ = float(np.nanmean(mean_alpha_ch[occ_in_picks_all]) / denom)\n",
    "\n",
    "    return rel_all, rel_occ, n_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading labeled data...\")\n",
    "try:\n",
    "    labeled_df = load_labeled_data(LABELED_DATASET_PATH)\n",
    "    print(f\"Loaded {len(labeled_df)} labeled epochs\")\n",
    "    print(f\"Columns: {labeled_df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(labeled_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading labeled data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "if 'Test subject ID' in labeled_df.columns:\n",
    "    labeled_df['subject_id'] = labeled_df['Test subject ID'].apply(normalize_subject_id)\n",
    "elif 'subject_id' in labeled_df.columns:\n",
    "    labeled_df['subject_id'] = labeled_df['subject_id'].apply(normalize_subject_id)\n",
    "\n",
    "if 'Epoch number' in labeled_df.columns:\n",
    "    labeled_df['epoch_idx'] = labeled_df['Epoch number']\n",
    "elif 'epoch' in labeled_df.columns:\n",
    "    labeled_df['epoch_idx'] = labeled_df['epoch']\n",
    "elif 'epoch_idx' not in labeled_df.columns:\n",
    "    # Assume epochs are sequential from 0\n",
    "    labeled_df['epoch_idx'] = labeled_df.index\n",
    "\n",
    "if 'Label' in labeled_df.columns:\n",
    "    labeled_df['prediction'] = labeled_df['Label']\n",
    "elif 'prediction' not in labeled_df.columns:\n",
    "    raise ValueError(\"Could not find 'Label' or 'prediction' column\")\n",
    "\n",
    "# Keep both probability conventions when available:\n",
    "# - prob_ec: probability of EC (label=1)\n",
    "# - probability: probability of the predicted label (for confidence sorting)\n",
    "if 'prob_ec' in labeled_df.columns:\n",
    "    labeled_df['prob_ec'] = pd.to_numeric(labeled_df['prob_ec'], errors='coerce')\n",
    "\n",
    "if 'Probability' in labeled_df.columns:\n",
    "    labeled_df['probability'] = pd.to_numeric(labeled_df['Probability'], errors='coerce')\n",
    "elif 'probability' in labeled_df.columns:\n",
    "    labeled_df['probability'] = pd.to_numeric(labeled_df['probability'], errors='coerce')\n",
    "elif 'prob_ec' in labeled_df.columns:\n",
    "    # prob_ec = P(EC). Label mapping: 1=EC, 0=EO.\n",
    "    # Probability of predicted label is prob_ec for EC predictions, and 1-prob_ec for EO predictions.\n",
    "    labeled_df['probability'] = np.where(\n",
    "        labeled_df['prediction'].astype(int) == LABEL_EC,\n",
    "        labeled_df['prob_ec'],\n",
    "        1.0 - labeled_df['prob_ec'],\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Could not find 'Probability', 'probability', or 'prob_ec' column\")\n",
    "\n",
    "print(f\"Standardized columns. Unique subjects: {labeled_df['subject_id'].nunique()}\")\n",
    "print(f\"Label distribution: {labeled_df['prediction'].value_counts().to_dict()}\")\n",
    "\n",
    "# Sanity check: if prob_ec exists, verify probability is the probability of the predicted label\n",
    "if 'prob_ec' in labeled_df.columns:\n",
    "    df_tmp = labeled_df.dropna(subset=[\"prob_ec\",\"prediction\",\"probability\"]).copy()\n",
    "    df_tmp[\"prediction\"] = df_tmp[\"prediction\"].astype(int)\n",
    "    ec = df_tmp[df_tmp[\"prediction\"] == LABEL_EC]\n",
    "    eo = df_tmp[df_tmp[\"prediction\"] == LABEL_EO]\n",
    "    if len(ec):\n",
    "        print(\"Sanity check (EC preds): mean(probability - prob_ec)=\", float((ec[\"probability\"]-ec[\"prob_ec\"]).mean()))\n",
    "    if len(eo):\n",
    "        print(\"Sanity check (EO preds): mean(probability - (1-prob_ec))=\", float((eo[\"probability\"]-(1.0-eo[\"prob_ec\"])).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading metadata...\")\n",
    "try:\n",
    "    metadata_df = pd.read_csv(METADATA_PATH)\n",
    "    print(f\"Loaded {len(metadata_df)} metadata entries\")\n",
    "    print(f\"Columns: {metadata_df.columns.tolist()}\")\n",
    "    \n",
    "    # Normalize subject ID column\n",
    "    if 'Subject_ID' in metadata_df.columns:\n",
    "        metadata_df['subject_id'] = metadata_df['Subject_ID'].apply(normalize_subject_id)\n",
    "    elif 'subject_id' in metadata_df.columns:\n",
    "        metadata_df['subject_id'] = metadata_df['subject_id'].apply(normalize_subject_id)\n",
    "    else:\n",
    "        # Fallback: use first column\n",
    "        first = metadata_df.columns[0]\n",
    "        metadata_df['subject_id'] = metadata_df[first].apply(normalize_subject_id)\n",
    "    \n",
    "    # Ensure Age column exists (supports old CSV: age/Age, and new: Y)\n",
    "    if 'Age' not in metadata_df.columns:\n",
    "        if 'age' in metadata_df.columns:\n",
    "            metadata_df['Age'] = metadata_df['age']\n",
    "        elif 'Y' in metadata_df.columns:\n",
    "            metadata_df['Age'] = metadata_df['Y']\n",
    "        else:\n",
    "            raise ValueError(\"Could not find an age column (Age/age/Y) in metadata\")\n",
    "    \n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(metadata_df[['subject_id', 'Age']].head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading metadata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labeled data with metadata\n",
    "cols = ['subject_id', 'epoch_idx', 'prediction', 'probability']\n",
    "if 'prob_ec' in labeled_df.columns:\n",
    "    cols.append('prob_ec')\n",
    "if 'file' in labeled_df.columns:\n",
    "    cols.append('file')\n",
    "\n",
    "analysis_df = pd.merge(\n",
    "    labeled_df[cols],\n",
    "    metadata_df[['subject_id', 'Age']],\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Merged data: {len(analysis_df)} entries\")\n",
    "print(f\"Subjects with metadata: {analysis_df['Age'].notna().sum() / len(analysis_df) * 100:.1f}%\")\n",
    "print(f\"Unique subjects: {analysis_df['subject_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract FOOOF Parameters (CF only)\n",
    "\n",
    "We use the `saved_fooof` cache to extract **alpha center frequency (CF)** per subject.\n",
    "\n",
    "Important:\n",
    "- This is a **single value per subject** (not per epoch).\n",
    "- We intentionally avoid loading the full per-epoch feature matrix, because it is large and not needed for the PSD-based alpha power computation later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting subject-level alpha center frequency (CF) from saved FOOOF cache...\")\n",
    "\n",
    "# This step only extracts the subject-level alpha center frequency (alpha_cf).\n",
    "# It does NOT load the full per-epoch feature matrix (X), which would be slow and memory-heavy.\n",
    "\n",
    "USE_CF_CACHE = os.getenv('USE_CF_CACHE', '1').strip() not in {'0','false','False'}\n",
    "cf_cache = os.path.join(OUTPUT_DIR, 'cf_by_subject.csv.gz')\n",
    "\n",
    "subject_cf_map = {}\n",
    "\n",
    "if USE_CF_CACHE and os.path.exists(cf_cache):\n",
    "    cf_df = pd.read_csv(cf_cache, compression='gzip')\n",
    "    if {'subject_id','cf'}.issubset(cf_df.columns):\n",
    "        subject_cf_map = dict(zip(cf_df['subject_id'].astype(str), pd.to_numeric(cf_df['cf'], errors='coerce')))\n",
    "        subject_cf_map = {k: float(v) for k,v in subject_cf_map.items() if pd.notna(v)}\n",
    "        print(f\"Loaded cached CF for {len(subject_cf_map)} subjects: {cf_cache}\")\n",
    "    else:\n",
    "        print(f\"CF cache missing required columns; ignoring: {cf_cache}\")\n",
    "\n",
    "if not subject_cf_map:\n",
    "    unique_subjects = sorted([s for s in analysis_df['subject_id'].dropna().unique()])\n",
    "\n",
    "    for i, subject_id in enumerate(unique_subjects, start=1):\n",
    "        npz_path = find_saved_fooof_npz(subject_id, SAVED_FOOOF_PATH)\n",
    "        if not npz_path:\n",
    "            continue\n",
    "        _, _, alpha_cf = load_fooof_data(npz_path)\n",
    "        if alpha_cf is None or (isinstance(alpha_cf, float) and np.isnan(alpha_cf)):\n",
    "            continue\n",
    "        subject_cf_map[str(subject_id)] = float(alpha_cf)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  processed {i}/{len(unique_subjects)} subjects... (CF found for {len(subject_cf_map)})\")\n",
    "\n",
    "    cf_df = pd.DataFrame({\n",
    "        'subject_id': list(subject_cf_map.keys()),\n",
    "        'cf': list(subject_cf_map.values()),\n",
    "    })\n",
    "    cf_df.to_csv(cf_cache, index=False, compression='gzip')\n",
    "    print(f\"Saved CF cache: {cf_cache}\")\n",
    "\n",
    "print()\n",
    "print(f\"Extracted CF for {len(subject_cf_map)} subjects\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot: Mean Center Frequency vs Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for CF vs Age\n",
    "cf_data = []\n",
    "for subject_id, cf in subject_cf_map.items():\n",
    "    age_row = metadata_df[metadata_df['subject_id'] == subject_id]\n",
    "    if not age_row.empty and pd.notna(age_row.iloc[0]['Age']):\n",
    "        cf_data.append({\n",
    "            'subject_id': subject_id,\n",
    "            'cf': cf,\n",
    "            'Age': age_row.iloc[0]['Age']\n",
    "        })\n",
    "\n",
    "cf_df = pd.DataFrame(cf_data)\n",
    "\n",
    "if len(cf_df) > 0:\n",
    "    # Optional: drop cf == 0 datapoints (can heavily affect mean/CI)\n",
    "    cf_df['cf'] = pd.to_numeric(cf_df['cf'], errors='coerce')\n",
    "    cf_df['Age'] = pd.to_numeric(cf_df['Age'], errors='coerce')\n",
    "\n",
    "    before_n = len(cf_df)\n",
    "    before_mean = cf_df['cf'].replace([np.inf, -np.inf], np.nan).dropna().mean()\n",
    "\n",
    "    if DROP_ZERO_CF:\n",
    "        if ZERO_CF_EPS > 0:\n",
    "            cf_df = cf_df[cf_df['cf'].abs() > float(ZERO_CF_EPS)]\n",
    "        else:\n",
    "            cf_df = cf_df[cf_df['cf'] != 0]\n",
    "\n",
    "    after_n = len(cf_df)\n",
    "    after_mean = cf_df['cf'].replace([np.inf, -np.inf], np.nan).dropna().mean()\n",
    "\n",
    "    print(f\"CF rows before filter: {before_n} | after: {after_n} | DROP_ZERO_CF={DROP_ZERO_CF} | ZERO_CF_EPS={ZERO_CF_EPS}\")\n",
    "    try:\n",
    "        print(f\"Mean CF before: {before_mean:.2f} | after: {after_mean:.2f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x='Age', y='cf', data=cf_df, scatter_kws={'alpha': 0.3}, label='Data points')\n",
    "    sns.lineplot(x='Age', y='cf', data=cf_df, estimator='mean', errorbar=('ci', 95),\n",
    "                  color='red', linewidth=2, label='Mean with 95% CI')\n",
    "    plt.title('Mean Center Frequency (FOOOF CF) vs Age (95% CI)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Age', fontsize=12)\n",
    "    plt.ylabel('Mean Center Frequency (Hz)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = 'mean_cf_vs_age.png' if not DROP_ZERO_CF else 'mean_cf_vs_age_drop0.png'\n",
    "    output_path = os.path.join(OUTPUT_DIR, fname)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {output_path}\")\n",
    "    print(f\"Mean CF: {cf_df['cf'].mean():.2f} Hz, SD: {cf_df['cf'].std():.2f} Hz\")\n",
    "else:\n",
    "    print(\"No CF data available for plotting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Select EC and EO Epochs\n",
    "\n",
    "This step decides *which epochs* to use for downstream analyses. You can choose between two modes:\n",
    "\n",
    "- `SELECTION_MODE = 'threshold'`: keep **all** epochs above the confidence threshold using the symmetric rule on `prob_ec = P(EC)`:\n",
    "  - EC kept if `prob_ec >= CONF_THRESH`\n",
    "  - EO kept if `prob_ec <= 1 - CONF_THRESH`\n",
    "- `SELECTION_MODE = 'top_k'`: keep the **TOP_K most confident** EC epochs and the **TOP_K most confident** EO epochs per subject (sorted by probability of the predicted label).\n",
    "\n",
    "The selected epoch list is cached under `OUTPUT_DIR` so reruns don’t need to re-filter millions of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting EC/EO epochs...\")\n",
    "\n",
    "# We support two selection modes:\n",
    "# - SELECTION_MODE='threshold': keep ALL epochs above the confidence threshold (symmetric rule on prob_ec)\n",
    "# - SELECTION_MODE='top_k': keep TOP_K most confident epochs per subject per class\n",
    "#\n",
    "# prob_ec is always P(EC). With label mapping: 1=EC, 0=EO.\n",
    "# The confidence of the predicted label is:\n",
    "#   probability = prob_ec           if prediction==1 (EC)\n",
    "#   probability = 1 - prob_ec       if prediction==0 (EO)\n",
    "\n",
    "mode = str(SELECTION_MODE).strip().lower()\n",
    "if mode not in {'threshold','top_k','topk'}:\n",
    "    raise ValueError(f\"Unknown SELECTION_MODE: {SELECTION_MODE!r} (use 'threshold' or 'top_k')\")\n",
    "\n",
    "if mode in {'top_k','topk'}:\n",
    "    cache_path = os.path.join(OUTPUT_DIR, f\"selected_epochs_topk{int(TOP_K)}.csv.gz\")\n",
    "else:\n",
    "    cache_path = os.path.join(OUTPUT_DIR, f\"selected_epochs_threshold_conf{int(round(CONF_THRESH*100))}.csv.gz\")\n",
    "\n",
    "if USE_SELECTION_CACHE and os.path.exists(cache_path):\n",
    "    top_epochs_df = pd.read_csv(cache_path, compression='gzip')\n",
    "    print(f\"Loaded cached selection: {cache_path}\")\n",
    "else:\n",
    "    if 'prob_ec' not in analysis_df.columns:\n",
    "        raise ValueError(\"analysis_df is missing 'prob_ec'. Ensure the labeled predictions include prob_ec.\")\n",
    "\n",
    "    cols = ['subject_id','epoch_idx','prediction','prob_ec']\n",
    "    if 'file' in analysis_df.columns:\n",
    "        cols.append('file')\n",
    "    df = analysis_df[cols].copy()\n",
    "\n",
    "    df['prob_ec'] = pd.to_numeric(df['prob_ec'], errors='coerce')\n",
    "    df['prediction'] = pd.to_numeric(df['prediction'], errors='coerce')\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['subject_id','epoch_idx','prediction','prob_ec'])\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "\n",
    "    # probability of predicted label\n",
    "    df['probability'] = np.where(df['prediction'] == LABEL_EC, df['prob_ec'], 1.0 - df['prob_ec'])\n",
    "\n",
    "    if mode in {'threshold'}:\n",
    "        ec = df[(df['prediction'] == LABEL_EC) & (df['prob_ec'] >= float(CONF_THRESH))].copy()\n",
    "        eo = df[(df['prediction'] == LABEL_EO) & (df['prob_ec'] <= float(1.0 - CONF_THRESH))].copy()\n",
    "    else:\n",
    "        # Top-K per subject per class based on probability\n",
    "        ec = df[df['prediction'] == LABEL_EC].sort_values(['subject_id','probability'], ascending=[True, False]).groupby('subject_id', as_index=False).head(int(TOP_K)).copy()\n",
    "        eo = df[df['prediction'] == LABEL_EO].sort_values(['subject_id','probability'], ascending=[True, False]).groupby('subject_id', as_index=False).head(int(TOP_K)).copy()\n",
    "\n",
    "    ec['class'] = 'EC'\n",
    "    eo['class'] = 'EO'\n",
    "\n",
    "    top_epochs_df = pd.concat([ec, eo], ignore_index=True)\n",
    "\n",
    "    keep_cols = ['subject_id','epoch_idx','class','prediction','prob_ec','probability']\n",
    "    if 'file' in top_epochs_df.columns:\n",
    "        keep_cols.append('file')\n",
    "    top_epochs_df = top_epochs_df[keep_cols]\n",
    "\n",
    "    if USE_SELECTION_CACHE:\n",
    "        top_epochs_df.to_csv(cache_path, index=False, compression='gzip')\n",
    "        print(f\"Saved cached selection: {cache_path}\")\n",
    "\n",
    "# Report\n",
    "if top_epochs_df is None or len(top_epochs_df) == 0:\n",
    "    print(\"No epochs selected\")\n",
    "else:\n",
    "    n_total = len(top_epochs_df)\n",
    "    n_ec = int((top_epochs_df['class'] == 'EC').sum())\n",
    "    n_eo = int((top_epochs_df['class'] == 'EO').sum())\n",
    "    n_subj = top_epochs_df['subject_id'].nunique()\n",
    "    print(f\"Mode={mode} | Selected epochs: total={n_total} | EC={n_ec} | EO={n_eo} | subjects={n_subj}\")\n",
    "\n",
    "    # Class/prediction mapping check\n",
    "    try:\n",
    "        print(\"Class→unique prediction labels:\")\n",
    "        print(top_epochs_df.groupby('class')['prediction'].unique().to_dict())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"First few rows:\")\n",
    "    print(top_epochs_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate Mean Relative Alpha (PSD-based)\n",
    "\n",
    "This step computes **relative alpha** from the EEG time series using Welch PSD (`psd_array_welch`).\n",
    "\n",
    "- Alpha band: 8–13 Hz (`PSD_ALPHA_FMIN`–`PSD_ALPHA_FMAX`)\n",
    "- Relative alpha is unitless: bandpower(8–13) / bandpower(1–40)\n",
    "- Occipital ROI: `O1`, `O2` (computed separately)\n",
    "\n",
    "Results are cached under `OUTPUT_DIR` with a filename that depends on `SELECTION_MODE` (and `CONF_THRESH` or `TOP_K`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating mean relative alpha (8–13 / 1–40 Hz) from EEG PSD for selected epochs...\")\n",
    "\n",
    "# Computes relative alpha from EEG using Welch PSD.\n",
    "# Definition: bandpower(8–13 Hz) / bandpower(1–40 Hz) (unitless).\n",
    "\n",
    "mode = str(SELECTION_MODE).strip().lower()\n",
    "if mode in {'top_k','topk'}:\n",
    "    alpha_cache = os.path.join(OUTPUT_DIR, f\"relative_alpha_psd_topk{int(TOP_K)}.csv.gz\")\n",
    "else:\n",
    "    alpha_cache = os.path.join(OUTPUT_DIR, f\"relative_alpha_psd_threshold_conf{int(round(CONF_THRESH*100))}.csv.gz\")\n",
    "\n",
    "if USE_ALPHA_CACHE and os.path.exists(alpha_cache):\n",
    "    alpha_power_df = pd.read_csv(alpha_cache, compression='gzip')\n",
    "    print(f\"Loaded cached alpha results: {alpha_cache}\")\n",
    "else:\n",
    "    if 'top_epochs_df' not in globals() or top_epochs_df is None or len(top_epochs_df) == 0:\n",
    "        print('No epochs selected (top_epochs_df is empty). Skipping alpha power computation.')\n",
    "        alpha_power_df = pd.DataFrame()\n",
    "    else:\n",
    "        required = {'subject_id','epoch_idx','class'}\n",
    "        if not required.issubset(set(top_epochs_df.columns)):\n",
    "            print('top_epochs_df columns:', top_epochs_df.columns.tolist())\n",
    "            raise KeyError(f\"top_epochs_df must contain {sorted(required)}\")\n",
    "\n",
    "        has_file = 'file' in top_epochs_df.columns\n",
    "        records = []\n",
    "        n_groups = 0\n",
    "\n",
    "        group_cols = ['subject_id'] + (['file'] if has_file else [])\n",
    "        for keys, df_sub in top_epochs_df.groupby(group_cols):\n",
    "            n_groups += 1\n",
    "            if has_file:\n",
    "                subject_id, file_path = keys\n",
    "            else:\n",
    "                subject_id, file_path = keys, None\n",
    "\n",
    "            if not file_path:\n",
    "                if 'file' in analysis_df.columns:\n",
    "                    cand = analysis_df[analysis_df['subject_id'] == subject_id]['file'].dropna()\n",
    "                    file_path = cand.iloc[0] if len(cand) else None\n",
    "\n",
    "            if not file_path:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                epochs = load_epochs_any(str(file_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load epochs for subject {subject_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            ch_can = [canonical_channel_name(ch).upper() for ch in epochs.ch_names]\n",
    "            roi_set = {c.upper() for c in OCCIPITAL_ROI}\n",
    "            picks_occ = np.array([i for i,name in enumerate(ch_can) if name in roi_set], dtype=int)\n",
    "\n",
    "            age_row = metadata_df[metadata_df['subject_id'] == subject_id]\n",
    "            age = age_row.iloc[0]['Age'] if (not age_row.empty and pd.notna(age_row.iloc[0]['Age'])) else None\n",
    "\n",
    "            for cls in ['EC','EO']:\n",
    "                ep_idx = pd.to_numeric(df_sub[df_sub['class'] == cls]['epoch_idx'], errors='coerce').dropna().astype(int).unique()\n",
    "                if ep_idx.size == 0:\n",
    "                    continue\n",
    "\n",
    "                mean_all, mean_occ, n_used = mean_relative_alpha_psd(epochs, ep_idx, picks_occ)\n",
    "                if n_used == 0:\n",
    "                    continue\n",
    "\n",
    "                records.append({\n",
    "                    'subject_id': subject_id,\n",
    "                    'class': cls,\n",
    "                    'Age': age,\n",
    "                    'mean_rel_alpha_all_channels': mean_all,\n",
    "                    'mean_rel_alpha_occipital_channels': mean_occ,\n",
    "                    'n_epochs': n_used,\n",
    "                })\n",
    "\n",
    "            if n_groups % 50 == 0:\n",
    "                print(f\"Processed {n_groups} subject/file groups...\")\n",
    "\n",
    "        alpha_power_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "        if USE_ALPHA_CACHE:\n",
    "            alpha_power_df.to_csv(alpha_cache, index=False, compression='gzip')\n",
    "            print(f\"Saved cached alpha results: {alpha_cache}\")\n",
    "\n",
    "if alpha_power_df is not None and len(alpha_power_df) > 0:\n",
    "    output_path_all = os.path.join(OUTPUT_DIR, 'mean_relative_alpha_all_channels.csv')\n",
    "    alpha_power_df[['subject_id', 'class', 'Age', 'mean_rel_alpha_all_channels', 'n_epochs']].to_csv(output_path_all, index=False)\n",
    "    print(f\"Saved mean alpha power (all channels) to {output_path_all}\")\n",
    "\n",
    "    output_path_occ = os.path.join(OUTPUT_DIR, 'mean_relative_alpha_occipital_channels.csv')\n",
    "    alpha_power_df[['subject_id', 'class', 'Age', 'mean_rel_alpha_occipital_channels', 'n_epochs']].to_csv(output_path_occ, index=False)\n",
    "    print(f\"Saved mean alpha power (occipital channels) to {output_path_occ}\")\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(alpha_power_df.groupby('class').agg({\n",
    "        'mean_rel_alpha_all_channels': ['mean', 'std'],\n",
    "        'mean_rel_alpha_occipital_channels': ['mean', 'std'],\n",
    "        'n_epochs': ['mean', 'min', 'max'],\n",
    "    }))\n",
    "else:\n",
    "    print('No alpha power results to save.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations: Alpha Power vs Age\n",
    "\n",
    "We aggregate per-subject alpha power by age and class (EC vs EO), compute a 95% confidence interval, and plot:\n",
    "\n",
    "- scatter points for the age-binned means\n",
    "- a spline-smoothed mean curve\n",
    "- shaded 95% CI\n",
    "\n",
    "This is done for both **all channels** and the **occipital ROI**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Mean relative alpha per class vs age (All Channels)\n",
    "from math import ceil, floor\n",
    "\n",
    "if len(alpha_power_df) > 0:\n",
    "    df = alpha_power_df[alpha_power_df['Age'].notna()].copy()\n",
    "    if len(df) == 0:\n",
    "        print('No data with age information for plotting')\n",
    "    else:\n",
    "        df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "        df = df.dropna(subset=['Age','mean_rel_alpha_all_channels','class']).copy()\n",
    "        df['mean_rel_alpha_all_channels'] = pd.to_numeric(df['mean_rel_alpha_all_channels'], errors='coerce')\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Age','mean_rel_alpha_all_channels','class'])\n",
    "\n",
    "        grouped = df.groupby(['Age','class']).agg(\n",
    "            MeanAlpha=('mean_rel_alpha_all_channels','mean'),\n",
    "            StdAlpha=('mean_rel_alpha_all_channels','std'),\n",
    "            N=('mean_rel_alpha_all_channels','count'),\n",
    "        ).reset_index()\n",
    "\n",
    "        def _ci(row):\n",
    "            n = float(row['N'])\n",
    "            mean = float(row['MeanAlpha'])\n",
    "            std = float(row['StdAlpha']) if pd.notna(row['StdAlpha']) else 0.0\n",
    "            if n < 2:\n",
    "                return (mean, mean)\n",
    "            se = std / np.sqrt(n)\n",
    "            try:\n",
    "                t = float(stats.t.ppf(0.975, df=int(n-1)))\n",
    "            except Exception:\n",
    "                t = 1.96\n",
    "            return (mean - t*se, mean + t*se)\n",
    "\n",
    "        ci_pairs = grouped.apply(_ci, axis=1)\n",
    "        ci_df = pd.DataFrame(ci_pairs.tolist(), columns=['Lower','Upper'], index=grouped.index)\n",
    "        grouped = pd.concat([grouped, ci_df], axis=1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        palette = {'EC':'tab:red', 'EO':'tab:blue'}\n",
    "\n",
    "        for cls in ['EC','EO']:\n",
    "            g = grouped[grouped['class'] == cls].sort_values('Age')\n",
    "            if len(g) == 0:\n",
    "                continue\n",
    "\n",
    "            grouped_clean = g.replace([np.inf, -np.inf], np.nan).dropna(subset=['Age','MeanAlpha','Lower','Upper'])\n",
    "            x = grouped_clean['Age'].values\n",
    "            y = grouped_clean['MeanAlpha'].values\n",
    "            y_lower = grouped_clean['Lower'].values\n",
    "            y_upper = grouped_clean['Upper'].values\n",
    "\n",
    "            # ensure strictly increasing x for spline\n",
    "            order = np.argsort(x)\n",
    "            x, y, y_lower, y_upper = x[order], y[order], y_lower[order], y_upper[order]\n",
    "            x_unique, unique_idx = np.unique(x, return_index=True)\n",
    "            x, y, y_lower, y_upper = x_unique, y[unique_idx], y_lower[unique_idx], y_upper[unique_idx]\n",
    "\n",
    "            color = palette.get(cls, 'gray')\n",
    "            plt.scatter(x, y, color=color, s=20, alpha=0.75, label=f\"{cls} mean\")\n",
    "\n",
    "            if len(x) < 3:\n",
    "                plt.plot(x, y, color=color, linewidth=2, label=f\"{cls} line\")\n",
    "                continue\n",
    "\n",
    "            x_smooth = np.linspace(float(x.min()), float(x.max()), 1000)\n",
    "            k = int(min(5, max(1, len(x)-1)))\n",
    "\n",
    "            try:\n",
    "                spline_mean = make_interp_spline(x, y, k=k)\n",
    "                spline_lower = make_interp_spline(x, y_lower, k=k)\n",
    "                spline_upper = make_interp_spline(x, y_upper, k=k)\n",
    "\n",
    "                y_smooth = spline_mean(x_smooth)\n",
    "                y_lower_smooth = spline_lower(x_smooth)\n",
    "                y_upper_smooth = spline_upper(x_smooth)\n",
    "\n",
    "                plt.plot(x_smooth, y_smooth, color=color, linewidth=2, label=f\"{cls} smoothed\")\n",
    "                plt.fill_between(x_smooth, y_lower_smooth, y_upper_smooth, color=color, alpha=0.18)\n",
    "            except Exception:\n",
    "                plt.plot(x, y, color=color, linewidth=2, label=f\"{cls} line\")\n",
    "\n",
    "        plt.xlabel('Age')\n",
    "        plt.ylabel('Mean Relative Alpha (8–13 / 1–40 Hz)')\n",
    "        plt.title('Mean Relative Alpha vs. Age (All Channels) with 95% CI')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        try:\n",
    "            lo = int(floor(float(grouped['Age'].min())/10)*10)\n",
    "            hi = int(ceil(float(grouped['Age'].max())/10)*10)\n",
    "            plt.xticks(np.arange(lo, hi+1, 10))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        seen = set()\n",
    "        uniq_h, uniq_l = [], []\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l in seen:\n",
    "                continue\n",
    "            seen.add(l)\n",
    "            uniq_h.append(h)\n",
    "            uniq_l.append(l)\n",
    "        plt.legend(uniq_h, uniq_l, fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'mean_relative_alpha_all_channels_vs_age_spline.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "else:\n",
    "    print('No alpha power data available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Mean relative alpha per class vs age (Occipital ROI)\n",
    "from math import ceil, floor\n",
    "\n",
    "if len(alpha_power_df) > 0:\n",
    "    df = alpha_power_df[alpha_power_df['Age'].notna()].copy()\n",
    "    if len(df) == 0:\n",
    "        print('No data with age information for plotting')\n",
    "    else:\n",
    "        df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "        df = df.dropna(subset=['Age','mean_rel_alpha_occipital_channels','class']).copy()\n",
    "        df['mean_rel_alpha_occipital_channels'] = pd.to_numeric(df['mean_rel_alpha_occipital_channels'], errors='coerce')\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Age','mean_rel_alpha_occipital_channels','class'])\n",
    "\n",
    "        grouped = df.groupby(['Age','class']).agg(\n",
    "            MeanAlpha=('mean_rel_alpha_occipital_channels','mean'),\n",
    "            StdAlpha=('mean_rel_alpha_occipital_channels','std'),\n",
    "            N=('mean_rel_alpha_occipital_channels','count'),\n",
    "        ).reset_index()\n",
    "\n",
    "        def _ci(row):\n",
    "            n = float(row['N'])\n",
    "            mean = float(row['MeanAlpha'])\n",
    "            std = float(row['StdAlpha']) if pd.notna(row['StdAlpha']) else 0.0\n",
    "            if n < 2:\n",
    "                return (mean, mean)\n",
    "            se = std / np.sqrt(n)\n",
    "            try:\n",
    "                t = float(stats.t.ppf(0.975, df=int(n-1)))\n",
    "            except Exception:\n",
    "                t = 1.96\n",
    "            return (mean - t*se, mean + t*se)\n",
    "\n",
    "        ci_pairs = grouped.apply(_ci, axis=1)\n",
    "        ci_df = pd.DataFrame(ci_pairs.tolist(), columns=['Lower','Upper'], index=grouped.index)\n",
    "        grouped = pd.concat([grouped, ci_df], axis=1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        palette = {'EC':'tab:red', 'EO':'tab:blue'}\n",
    "\n",
    "        for cls in ['EC','EO']:\n",
    "            g = grouped[grouped['class'] == cls].sort_values('Age')\n",
    "            if len(g) == 0:\n",
    "                continue\n",
    "\n",
    "            grouped_clean = g.replace([np.inf, -np.inf], np.nan).dropna(subset=['Age','MeanAlpha','Lower','Upper'])\n",
    "            x = grouped_clean['Age'].values\n",
    "            y = grouped_clean['MeanAlpha'].values\n",
    "            y_lower = grouped_clean['Lower'].values\n",
    "            y_upper = grouped_clean['Upper'].values\n",
    "\n",
    "            # ensure strictly increasing x for spline\n",
    "            order = np.argsort(x)\n",
    "            x, y, y_lower, y_upper = x[order], y[order], y_lower[order], y_upper[order]\n",
    "            x_unique, unique_idx = np.unique(x, return_index=True)\n",
    "            x, y, y_lower, y_upper = x_unique, y[unique_idx], y_lower[unique_idx], y_upper[unique_idx]\n",
    "\n",
    "            color = palette.get(cls, 'gray')\n",
    "            plt.scatter(x, y, color=color, s=20, alpha=0.75, label=f\"{cls} mean\")\n",
    "\n",
    "            if len(x) < 3:\n",
    "                plt.plot(x, y, color=color, linewidth=2, label=f\"{cls} line\")\n",
    "                continue\n",
    "\n",
    "            x_smooth = np.linspace(float(x.min()), float(x.max()), 1000)\n",
    "            k = int(min(5, max(1, len(x)-1)))\n",
    "\n",
    "            try:\n",
    "                spline_mean = make_interp_spline(x, y, k=k)\n",
    "                spline_lower = make_interp_spline(x, y_lower, k=k)\n",
    "                spline_upper = make_interp_spline(x, y_upper, k=k)\n",
    "\n",
    "                y_smooth = spline_mean(x_smooth)\n",
    "                y_lower_smooth = spline_lower(x_smooth)\n",
    "                y_upper_smooth = spline_upper(x_smooth)\n",
    "\n",
    "                plt.plot(x_smooth, y_smooth, color=color, linewidth=2, label=f\"{cls} smoothed\")\n",
    "                plt.fill_between(x_smooth, y_lower_smooth, y_upper_smooth, color=color, alpha=0.18)\n",
    "            except Exception:\n",
    "                plt.plot(x, y, color=color, linewidth=2, label=f\"{cls} line\")\n",
    "\n",
    "        plt.xlabel('Age')\n",
    "        plt.ylabel('Mean Relative Alpha (8–13 / 1–40 Hz)')\n",
    "        plt.title('Mean Relative Alpha vs. Age (Occipital ROI) with 95% CI')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        try:\n",
    "            lo = int(floor(float(grouped['Age'].min())/10)*10)\n",
    "            hi = int(ceil(float(grouped['Age'].max())/10)*10)\n",
    "            plt.xticks(np.arange(lo, hi+1, 10))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        seen = set()\n",
    "        uniq_h, uniq_l = [], []\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l in seen:\n",
    "                continue\n",
    "            seen.add(l)\n",
    "            uniq_h.append(h)\n",
    "            uniq_l.append(l)\n",
    "        plt.legend(uniq_h, uniq_l, fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'mean_relative_alpha_occipital_channels_vs_age_spline.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "else:\n",
    "    print('No alpha power data available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. EC vs EO Alpha Power Comparison\n",
    "\n",
    "This section compares, per subject, whether mean alpha power is higher in the model-labeled EC epochs than in the model-labeled EO epochs.\n",
    "\n",
    "Note: these are **predicted** labels (high-confidence epochs), not ground-truth EC/EO annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare EC and EO relative alpha per subject\n",
    "# NOTE: This compares the model's *predicted* EC vs predicted EO epochs (high-confidence per subject),\n",
    "# not ground-truth EC/EO labels.\n",
    "\n",
    "if len(alpha_power_df) > 0:\n",
    "    ec_eo_comparison = alpha_power_df.pivot_table(\n",
    "        index=['subject_id', 'Age'],\n",
    "        columns='class',\n",
    "        values=['mean_rel_alpha_all_channels', 'mean_rel_alpha_occipital_channels'],\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "\n",
    "    ec_eo_comparison.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in ec_eo_comparison.columns.values]\n",
    "\n",
    "    required_cols = [\n",
    "        'mean_rel_alpha_all_channels_EC',\n",
    "        'mean_rel_alpha_all_channels_EO',\n",
    "        'mean_rel_alpha_occipital_channels_EC',\n",
    "        'mean_rel_alpha_occipital_channels_EO',\n",
    "    ]\n",
    "    missing = [c for c in required_cols if c not in ec_eo_comparison.columns]\n",
    "    if missing:\n",
    "        print('Missing required EC/EO columns:', missing)\n",
    "    else:\n",
    "        ec_eo_comparison['EC_higher_than_EO_all'] = (\n",
    "            ec_eo_comparison['mean_rel_alpha_all_channels_EC'] >\n",
    "            ec_eo_comparison['mean_rel_alpha_all_channels_EO']\n",
    "        )\n",
    "\n",
    "        ec_eo_comparison['EC_higher_than_EO_occ'] = (\n",
    "            ec_eo_comparison['mean_rel_alpha_occipital_channels_EC'] >\n",
    "            ec_eo_comparison['mean_rel_alpha_occipital_channels_EO']\n",
    "        )\n",
    "\n",
    "        total_subjects = len(ec_eo_comparison)\n",
    "        ec_higher_count_all = int(ec_eo_comparison['EC_higher_than_EO_all'].sum())\n",
    "        ec_higher_count_occ = int(ec_eo_comparison['EC_higher_than_EO_occ'].sum())\n",
    "\n",
    "        percentage_ec_higher_all = (ec_higher_count_all / total_subjects) * 100 if total_subjects > 0 else 0\n",
    "        percentage_ec_higher_occ = (ec_higher_count_occ / total_subjects) * 100 if total_subjects > 0 else 0\n",
    "\n",
    "        print(f\"Total subjects with both predicted EC and predicted EO data: {total_subjects}\")\n",
    "        print(\"All channels (predicted):\")\n",
    "        print(f\"  Subjects with EC alpha > EO alpha: {ec_higher_count_all}/{total_subjects} ({percentage_ec_higher_all:.1f}%)\")\n",
    "        print(\"Occipital channels (predicted):\")\n",
    "        print(f\"  Subjects with EC alpha > EO alpha: {ec_higher_count_occ}/{total_subjects} ({percentage_ec_higher_occ:.1f}%)\")\n",
    "        print(f\"  (So EO > EC for occipital in ~{100.0 - percentage_ec_higher_occ:.1f}% of subjects, excluding ties)\")\n",
    "\n",
    "        diff = ec_eo_comparison['mean_rel_alpha_occipital_channels_EO'] - ec_eo_comparison['mean_rel_alpha_occipital_channels_EC']\n",
    "        ec_eo_comparison['EO_minus_EC_occ'] = diff\n",
    "        print(\"Top 10 subjects where EO exceeds EC most (occipital):\")\n",
    "        print(ec_eo_comparison.sort_values('EO_minus_EC_occ', ascending=False).head(10)[['subject_id','Age','EO_minus_EC_occ']].to_string(index=False))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        categories = ['All Channels', 'Occipital Channels']\n",
    "        percentages = [percentage_ec_higher_all, percentage_ec_higher_occ]\n",
    "        colors = sns.color_palette(\"viridis\", len(categories))\n",
    "\n",
    "        bars = plt.bar(categories, percentages, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "        for bar, pct in zip(bars, percentages):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.title('Percentage of Subjects with Higher Mean Alpha Power in EC vs EO (predicted)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Percentage (%)', fontsize=12)\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'percentage_ec_higher_alpha.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "\n",
    "        comparison_path = os.path.join(OUTPUT_DIR, 'ec_eo_comparison.csv')\n",
    "        ec_eo_comparison.to_csv(comparison_path, index=False)\n",
    "        print(f\"Comparison results saved to {comparison_path}\")\n",
    "else:\n",
    "    print(\"No relative alpha data available for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Analysis complete! All outputs have been saved to the run-specific output directory printed at the top (default: `New_EEG/outputs/analysis_script/<RUN_FOLDER>/`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
