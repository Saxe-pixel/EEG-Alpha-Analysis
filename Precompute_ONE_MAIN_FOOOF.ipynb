{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c399a46",
   "metadata": {},
   "source": [
    "# Precompute ONE_MAIN_FOOOF cache (A/B)\n",
    "\n",
    "This notebook scans a directory of EEGLAB `.set` **epochs** files (e.g. `G:\\\\ChristianMusaeus\\\\Preprocessed_setfiles`) and precomputes `ONE_MAIN_FOOOF` artifacts so they don't need to be recomputed every time.\n",
    "\n",
    "At the top you can choose between:\n",
    "\n",
    "- **(A) cache alpha profile only**: saves per-subject `(alpha_cf, alpha_bw)`.\n",
    "- **(B) cache full per-epoch features**: saves `(X, feature_names, metadata)` so inference becomes fast.\n",
    "\n",
    "Outputs are written under `SAVED_FOOOF_DIR` (default `G:\\ChristianMusaeus\\saved_fooof`) in a config-tagged subfolder, never into the notebook directory itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import platform\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "# -----------------\n",
    "# User config\n",
    "# -----------------\n",
    "\n",
    "# Choose caching mode:\n",
    "# - \"A\" = alpha profile only (subject-level MAIN_FOOOF params)\n",
    "# - \"B\" = full per-epoch ONE_MAIN_FOOOF features\n",
    "CACHE_MODE = \"B\"  # \"A\" or \"B\"\n",
    "\n",
    "# Data selection (choose which files to scan)\n",
    "# 1 = Preprocessed_setfiles (default, same as before)\n",
    "# 2 = Old marked EO/EC folders (Open_marked + Closed_marked)\n",
    "# 3 = New_EEG processed folder\n",
    "DATASET_OPTION = 2  # 1, 2, or 3\n",
    "\n",
    "PREPROCESSED_SETFILES_DIR = os.getenv(\"PREPROCESSED_SETFILES_DIR\", r\"G:\\\\ChristianMusaeus\\\\Preprocessed_setfiles\")\n",
    "OLD_OPEN_MARKED_DIR = os.getenv(\"OLD_OPEN_MARKED_DIR\", r\"E:\\\\Saxe_sandkasse\\\\30EOEC_filer\\\\Open_marked\")\n",
    "OLD_CLOSED_MARKED_DIR = os.getenv(\"OLD_CLOSED_MARKED_DIR\", r\"E:\\\\Saxe_sandkasse\\\\30EOEC_filer\\\\Closed_marked\")\n",
    "NEW_EEG_PROCESSED_DIR = os.getenv(\"NEW_EEG_PROCESSED_DIR\", r\"G:\\\\ChristianMusaeus\\\\New_EEG\\\\Processed\")\n",
    "\n",
    "if int(DATASET_OPTION) == 1:\n",
    "    INPUT_DIRS = [PREPROCESSED_SETFILES_DIR]\n",
    "    DATASET_TAG = \"old_dataset_preprocessed_setfiles\"\n",
    "elif int(DATASET_OPTION) == 2:\n",
    "    INPUT_DIRS = [OLD_OPEN_MARKED_DIR, OLD_CLOSED_MARKED_DIR]\n",
    "    DATASET_TAG = \"old_dataset_open_closed_marked\"\n",
    "elif int(DATASET_OPTION) == 3:\n",
    "    INPUT_DIRS = [NEW_EEG_PROCESSED_DIR]\n",
    "    DATASET_TAG = \"new_dataset_processed\"\n",
    "else:\n",
    "    raise ValueError(\"DATASET_OPTION must be 1, 2, or 3\")\n",
    "\n",
    "RECURSIVE = True\n",
    "\n",
    "# Output behavior\n",
    "OVERWRITE = False\n",
    "SAVE_PER_FILE = True  # if False, write one big combined file (B only)\n",
    "\n",
    "# Channels\n",
    "CHANNEL_SELECTION = [\"all\"]  # or e.g. [\"O1\", \"O2\", \"P3\", \"P4\", \"P7\", \"P8\", \"Pz\"]\n",
    "ALL_CHANNELS = any(str(x).lower() == \"all\" for x in CHANNEL_SELECTION)\n",
    "\n",
    "# If channels are generic (\"Ch1\"..\"Ch19\"), optionally map them to standard 10â€“20 names.\n",
    "# NOTE: This assumes the dataset uses the common 19-channel ordering.\n",
    "AUTO_RENAME_CH1_TO_1020 = True\n",
    "CH1_TO_1020_ORDER_19 = [\n",
    "    \"Fp1\",\n",
    "\"Fp2\",\n",
    "\"F3\",\n",
    "\"F4\",\n",
    "\"C3\",\n",
    "\"C4\",\n",
    "\"P3\",\n",
    "\"P4\",\n",
    "\"O1\",\n",
    "\"O2\",\n",
    "\"F7\",\n",
    "\"F8\",\n",
    "\"T7\",\n",
    "\"T8\",\n",
    "\"P7\",\n",
    "\"P8\",\n",
    "\"Fz\",\n",
    "\"Cz\",\n",
    "\"Pz\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Alpha profile (subject MAIN_FOOOF)\n",
    "ALPHA_PROFILE_RANGE = (4.0, 16.0)\n",
    "ALPHA_PROFILE_ROI = [\"O1\", \"O2\", \"P3\", \"P4\", \"P7\", \"P8\", \"Pz\"]\n",
    "\n",
    "# PSD computation\n",
    "PSD_KWARGS = dict(fmin=1.0, fmax=45.0)\n",
    "TARGET_SECS = 2.0\n",
    "COMBINE_ADJACENT_EPOCHS = False  # If True, pair consecutive epochs by concatenating time (matches EC_EO_Classifier option)\n",
    "ALPHA_FREQ_RANGE = (3.0, 40.0)   # Fit range used for per-epoch aperiodic-only fits (ONE_MAIN_FOOOF)\n",
    "ETA_EVERY = 1  # Print an ETA every N files\n",
    "\n",
    "# FOOOF/specparam settings\n",
    "\n",
    "# Which library to use for spectral parameterization:\n",
    "# - \"auto\": try specparam first, then fooof\n",
    "# - \"specparam\": require specparam\n",
    "# - \"fooof\": require fooof\n",
    "BACKEND_PREFERENCE = os.getenv(\"FOOOF_BACKEND\", \"auto\").strip().lower()\n",
    "FOOOF_SETTINGS = {\n",
    "    \"aperiodic_mode\": \"fixed\",\n",
    "    \"peak_width_limits\": (0.5, 12.0),\n",
    "    \"max_n_peaks\": 6,\n",
    "    \"min_peak_height\": 0.05,\n",
    "    \"peak_threshold\": 2.0,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "# Feature selection: keep subset of [\"offset\",\"exponent\",\"alpha_cf\",\"alpha_amp\",\"alpha_bw\"]\n",
    "FOOOF_SELECTED_FEATURES = [\"offset\", \"exponent\", \"alpha_amp\"]\n",
    "\n",
    "# -----------------\n",
    "# Output folder setup\n",
    "# -----------------\n",
    "\n",
    "def _detect_notebook_path() -> Optional[Path]:\n",
    "    try:\n",
    "        vsc = globals().get(\"__vsc_ipynb_file__\", None)\n",
    "        if vsc:\n",
    "            p = Path(str(vsc)).expanduser()\n",
    "            if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for key in (\"NOTEBOOK_PATH\", \"IPYNB_PATH\"):\n",
    "        v = os.getenv(key)\n",
    "        if v:\n",
    "            try:\n",
    "                p = Path(v).expanduser()\n",
    "                if p.suffix.lower() == \".ipynb\" and p.exists():\n",
    "                    return p.resolve()\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Repo-local fallback\n",
    "    try:\n",
    "        here = Path.cwd().resolve()\n",
    "        for _ in range(6):\n",
    "            cand = here / \"New_EEG\" / \"Precompute_ONE_MAIN_FOOOF.ipynb\"\n",
    "            if cand.exists():\n",
    "                return cand.resolve()\n",
    "            here = here.parent\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "NOTEBOOK_PATH = _detect_notebook_path()\n",
    "NOTEBOOK_DIR = NOTEBOOK_PATH.parent if NOTEBOOK_PATH is not None else Path.cwd().resolve()\n",
    "\n",
    "# Output folder (can be outside repo; default points to your shared drive)\n",
    "SAVED_FOOOF_DIR = os.getenv(\"SAVED_FOOOF_DIR\", r\"G:\\ChristianMusaeus\\saved_fooof\")\n",
    "\n",
    "def _maybe_wsl_path(p: str) -> str | None:\n",
    "    m = re.match(r\"^([A-Za-z]):[\\\\/](.*)$\", str(p))\n",
    "    if not m:\n",
    "        return None\n",
    "    drive = m.group(1).lower()\n",
    "    rest = m.group(2).replace(\"\\\\\", \"/\")\n",
    "    return f\"/mnt/{drive}/{rest}\"\n",
    "\n",
    "def is_wsl() -> bool:\n",
    "    try:\n",
    "        return 'microsoft' in platform.uname().release.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resolve_saved_fooof_root() -> Path:\n",
    "    candidates = []\n",
    "    if platform.system() == 'Windows':\n",
    "        candidates.append(str(SAVED_FOOOF_DIR))\n",
    "    elif is_wsl():\n",
    "        w = _maybe_wsl_path(str(SAVED_FOOOF_DIR))\n",
    "        if w:\n",
    "            candidates.append(w)\n",
    "    # repo-local fallback (safe on macOS/Linux)\n",
    "    candidates.append(str(NOTEBOOK_DIR / 'outputs' / 'saved_fooof'))\n",
    "\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            p = Path(str(c)).expanduser()\n",
    "            p.mkdir(parents=True, exist_ok=True)\n",
    "            return p.resolve()\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f'Could not create SAVED_FOOOF_DIR: {SAVED_FOOOF_DIR}')\n",
    "\n",
    "OUTPUTS_ROOT = resolve_saved_fooof_root()\n",
    "\n",
    "\n",
    "def _safe_tag(s: str) -> str:\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", str(s))\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s[:120] if len(s) > 120 else s\n",
    "\n",
    "def _channel_tag() -> str:\n",
    "    sel = [str(x).strip() for x in (CHANNEL_SELECTION or []) if x is not None]\n",
    "    if any(x.lower() == \"all\" for x in sel):\n",
    "        return \"allch\"\n",
    "    if not sel:\n",
    "        return \"ch_unknown\"\n",
    "    joined = \"-\".join(_safe_tag(x.upper()) for x in sel)\n",
    "    return f\"ch_{joined}\"[:80]\n",
    "\n",
    "def _config_tag() -> str:\n",
    "    parts = [\n",
    "        str(DATASET_TAG),\n",
    "        \"fooof\",\n",
    "        \"one_main_fooof\",\n",
    "        f\"cache_{_safe_tag(CACHE_MODE).lower()}\",\n",
    "        _channel_tag(),\n",
    "        f\"psd_{PSD_KWARGS.get('fmin', 1.0)}-{PSD_KWARGS.get('fmax', 45.0)}Hz\",\n",
    "    ]\n",
    "    if COMBINE_ADJACENT_EPOCHS:\n",
    "        parts.append(\"pair_epochs\")\n",
    "    parts.append(f\"fit_{ALPHA_FREQ_RANGE[0]}-{ALPHA_FREQ_RANGE[1]}Hz\")\n",
    "    if FOOOF_SELECTED_FEATURES:\n",
    "        parts.append(\"feat_\" + _safe_tag(\"-\".join(FOOOF_SELECTED_FEATURES)))\n",
    "    return \"__\".join([p for p in parts if p])\n",
    "\n",
    "def get_output_dir() -> Path:\n",
    "    out_dir = OUTPUTS_ROOT / _config_tag()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "def outpath(name: str) -> Path:\n",
    "    return get_output_dir() / str(name)\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"Outputs root:\", OUTPUTS_ROOT)\n",
    "print(\"Output dir:\", get_output_dir())\n",
    "\n",
    "# Persist config for reproducibility\n",
    "config_dump = {\n",
    "    \"CACHE_MODE\": CACHE_MODE,\n",
    "    \"DATASET_OPTION\": int(DATASET_OPTION),\n",
    "    \"DATASET_TAG\": str(DATASET_TAG),\n",
    "    \"INPUT_DIRS\": list(INPUT_DIRS),\n",
    "    \"PREPROCESSED_SETFILES_DIR\": PREPROCESSED_SETFILES_DIR,\n",
    "    \"OLD_OPEN_MARKED_DIR\": OLD_OPEN_MARKED_DIR,\n",
    "    \"OLD_CLOSED_MARKED_DIR\": OLD_CLOSED_MARKED_DIR,\n",
    "    \"NEW_EEG_PROCESSED_DIR\": NEW_EEG_PROCESSED_DIR,\n",
    "    \"RECURSIVE\": RECURSIVE,\n",
    "    \"OVERWRITE\": OVERWRITE,\n",
    "    \"SAVE_PER_FILE\": SAVE_PER_FILE,\n",
    "    \"CHANNEL_SELECTION\": CHANNEL_SELECTION,\n",
    "    \"ALPHA_PROFILE_RANGE\": list(ALPHA_PROFILE_RANGE),\n",
    "    \"ALPHA_PROFILE_ROI\": list(ALPHA_PROFILE_ROI),\n",
    "    \"PSD_KWARGS\": dict(PSD_KWARGS),\n",
    "    \"TARGET_SECS\": TARGET_SECS,\n",
    "    \"COMBINE_ADJACENT_EPOCHS\": COMBINE_ADJACENT_EPOCHS,\n",
    "    \"ALPHA_FREQ_RANGE\": list(ALPHA_FREQ_RANGE),\n",
    "    \"ETA_EVERY\": ETA_EVERY,\n",
    "    \"FOOOF_SETTINGS\": dict(FOOOF_SETTINGS),\n",
    "    \"FOOOF_BACKEND_PREFERENCE\": str(BACKEND_PREFERENCE),\n",
    "    \"FOOOF_SELECTED_FEATURES\": list(FOOOF_SELECTED_FEATURES),\n",
    "}\n",
    "outpath(\"config.json\").write_text(json.dumps(config_dump, indent=2), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532eae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Backend imports\n",
    "# -----------------\n",
    "\n",
    "SpectralModel = None\n",
    "FitError = Exception\n",
    "FOOOF_BACKEND = \"unavailable\"\n",
    "_backend_error = None\n",
    "\n",
    "pref = str(globals().get('BACKEND_PREFERENCE', 'auto')).strip().lower()\n",
    "if pref not in {'auto', 'specparam', 'fooof'}:\n",
    "    print(f\"Unknown BACKEND_PREFERENCE={pref!r}; falling back to 'auto'.\")\n",
    "    pref = 'auto'\n",
    "\n",
    "# Try specparam\n",
    "if pref in {'auto', 'specparam'}:\n",
    "    try:\n",
    "        from specparam import SpectralModel as _SpecModel\n",
    "        from specparam.core.errors import FitError as _FitError\n",
    "        SpectralModel = _SpecModel\n",
    "        FitError = _FitError\n",
    "        FOOOF_BACKEND = 'specparam'\n",
    "    except Exception as exc:\n",
    "        _backend_error = exc\n",
    "        if pref == 'specparam':\n",
    "            raise RuntimeError(\n",
    "                \"BACKEND_PREFERENCE=specparam but specparam could not be imported.\\n\"\n",
    "                \"Install specparam or set BACKEND_PREFERENCE=auto/fooof.\"\n",
    "            ) from exc\n",
    "\n",
    "# Try fooof\n",
    "if SpectralModel is None and pref in {'auto', 'fooof'}:\n",
    "    try:\n",
    "        from fooof import FOOOF as _FooofModel\n",
    "        from fooof.core.errors import FitError as _FitError\n",
    "        SpectralModel = _FooofModel\n",
    "        FitError = _FitError\n",
    "        FOOOF_BACKEND = 'fooof'\n",
    "    except Exception as exc:\n",
    "        _backend_error = exc\n",
    "        if pref == 'fooof':\n",
    "            raise RuntimeError(\n",
    "                \"BACKEND_PREFERENCE=fooof but fooof could not be imported.\\n\"\n",
    "                \"Install fooof or set BACKEND_PREFERENCE=auto/specparam.\"\n",
    "            ) from exc\n",
    "\n",
    "print('FOOOF/specparam backend used:', FOOOF_BACKEND)\n",
    "if SpectralModel is None:\n",
    "    raise RuntimeError('FOOOF/specparam backend unavailable in this environment.')\n",
    "\n",
    "# Update config.json with the chosen backend (best-effort)\n",
    "try:\n",
    "    cfg_path = outpath('config.json')\n",
    "    if cfg_path.exists():\n",
    "        cfg = json.loads(cfg_path.read_text(encoding='utf-8'))\n",
    "    else:\n",
    "        cfg = {}\n",
    "    cfg['FOOOF_BACKEND_USED'] = str(FOOOF_BACKEND)\n",
    "    cfg_path.write_text(json.dumps(cfg, indent=2), encoding='utf-8')\n",
    "except Exception as exc:\n",
    "    print('Warning: could not update config.json with backend:', exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81365038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Helpers\n",
    "# -----------------\n",
    "\n",
    "def parse_subject_id(path: Path) -> int:\n",
    "    stem = path.stem\n",
    "    m = re.search(r\"(\\d{3,})\", stem)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return int(abs(hash(stem)) % 1_000_000_000)\n",
    "\n",
    "def canonical_channel_name(ch_name: str) -> str:\n",
    "    name = str(ch_name).strip()\n",
    "    name = re.sub(r\"^EEG\\s+\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"-REF$\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \"\", name)\n",
    "    key = name.upper()\n",
    "    if globals().get('AUTO_RENAME_CH1_TO_1020', False):\n",
    "        order = globals().get('CH1_TO_1020_ORDER_19', None)\n",
    "        if isinstance(order, (list, tuple)) and len(order) == 19:\n",
    "            ch_map = {f'CH{i+1}': str(order[i]) for i in range(19)}\n",
    "            if key in ch_map:\n",
    "                return ch_map[key]\n",
    "    return name\n",
    "\n",
    "def rename_epochs_channels_canonical(epochs):\n",
    "    new_names = [canonical_channel_name(ch) for ch in epochs.ch_names]\n",
    "    if len(set(new_names)) != len(new_names):\n",
    "        return epochs\n",
    "    mapping = {old: new for old, new in zip(epochs.ch_names, new_names) if old != new}\n",
    "    if mapping:\n",
    "        epochs.rename_channels(mapping)\n",
    "    return epochs\n",
    "\n",
    "def psd_array_welch_clean(data: np.ndarray, sfreq: float, fmin=1.0, fmax=45.0, target_secs=2.0):\n",
    "    n_epochs, _, n_times = data.shape\n",
    "    n_per_seg = max(8, min(n_times, int(round(target_secs * sfreq))))\n",
    "    n_overlap = n_per_seg // 2 if n_per_seg >= 16 else 0\n",
    "    psds, freqs = mne.time_frequency.psd_array_welch(\n",
    "        data,\n",
    "        sfreq=sfreq,\n",
    "        fmin=float(fmin),\n",
    "        fmax=float(fmax),\n",
    "        n_per_seg=n_per_seg,\n",
    "        n_overlap=n_overlap,\n",
    "        window=\"hann\",\n",
    "        average=\"mean\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    return psds, freqs\n",
    "\n",
    "def select_alpha_peak(peaks: np.ndarray, lo: float, hi: float):\n",
    "    peaks_arr = np.asarray(peaks, float)\n",
    "    if peaks_arr.size == 0:\n",
    "        return None\n",
    "    if peaks_arr.ndim == 1:\n",
    "        peaks_arr = peaks_arr.reshape(1, -1)\n",
    "    mask = (peaks_arr[:, 0] >= lo) & (peaks_arr[:, 0] <= hi)\n",
    "    if not np.any(mask):\n",
    "        return None\n",
    "    subset = peaks_arr[mask]\n",
    "    return subset[np.argmax(subset[:, 1])]\n",
    "\n",
    "def compute_one_main_fooof_features(freqs: np.ndarray, psd_cube: np.ndarray, subject_id: int, alpha_profile_map, include_aperiodic: bool = True) -> np.ndarray:\n",
    "    \"\"\"Compute ONE_MAIN_FOOOF features (matches `New_EEG/EC_EO_Classifier.ipynb`).\n",
    "\n",
    "    For each subject, alpha center frequency and bandwidth are fixed from alpha_profile_map.\n",
    "    For each epoch/channel, fit only the aperiodic component (max_n_peaks=0) and then fit\n",
    "    the amplitude of a Gaussian alpha template to the residual.\n",
    "\n",
    "    Feature layout per channel: [offset, exponent, alpha_cf, alpha_amp, alpha_bw].\n",
    "    \"\"\"\n",
    "    if SpectralModel is None:\n",
    "        raise RuntimeError(\"FOOOF backend unavailable.\")\n",
    "    subj = int(subject_id)\n",
    "    profile = alpha_profile_map.get(subj) if alpha_profile_map is not None else None\n",
    "    has_profile = profile is not None and len(profile) == 2\n",
    "    if has_profile:\n",
    "        alpha_cf, alpha_bw = map(float, profile)\n",
    "    else:\n",
    "        alpha_cf, alpha_bw = 0.0, 0.0\n",
    "    freqs_arr = np.asarray(freqs, float)\n",
    "    import math\n",
    "    if has_profile and alpha_bw > 0:\n",
    "        sigma = float(alpha_bw) / (2.0 * math.sqrt(2.0 * math.log(2.0)))\n",
    "        gauss = np.exp(-0.5 * ((freqs_arr - alpha_cf) / sigma) ** 2)\n",
    "    else:\n",
    "        gauss = np.zeros_like(freqs_arr)\n",
    "    denom = float(np.sum(gauss ** 2)) if gauss.size else 0.0\n",
    "    features = []\n",
    "    ap_settings = dict(FOOOF_SETTINGS)\n",
    "    try:\n",
    "        ap_settings[\"max_n_peaks\"] = 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    for epoch_psd in psd_cube:\n",
    "        epoch_feats = []\n",
    "        for spectrum in epoch_psd:\n",
    "            try:\n",
    "                if not np.all(np.isfinite(spectrum)):\n",
    "                    raise ValueError(\"Non-finite in spectrum\")\n",
    "                offset, exponent = 0.0, 0.0\n",
    "                alpha_amp = 0.0\n",
    "                if include_aperiodic or has_profile:\n",
    "                    model = SpectralModel(**ap_settings)\n",
    "                    model.fit(freqs_arr, spectrum, freq_range=ALPHA_FREQ_RANGE)\n",
    "                    if hasattr(model, \"aperiodic_params_\"):\n",
    "                        params = np.asarray(model.aperiodic_params_)\n",
    "                        if params.size > 0:\n",
    "                            offset = float(params[0])\n",
    "                        if params.size > 1:\n",
    "                            exponent = float(params[1])\n",
    "                    try:\n",
    "                        ap_fit = None\n",
    "                        get_fun = getattr(model, \"get_model_spectrum\", None)\n",
    "                        if callable(get_fun):\n",
    "                            ap_fit = np.asarray(get_fun(freqs_arr))\n",
    "                    except Exception:\n",
    "                        ap_fit = None\n",
    "                    if ap_fit is None:\n",
    "                        for name in (\"fooofed_spectrum_\", \"modeled_spectrum_\", \"model_spectrum_\", \"model_spectrum__\"):\n",
    "                            if hasattr(model, name):\n",
    "                                ap_fit = np.asarray(getattr(model, name))\n",
    "                                break\n",
    "                    if ap_fit is None or ap_fit.shape != spectrum.shape:\n",
    "                        ap_fit = np.zeros_like(spectrum)\n",
    "                    if has_profile and denom > 0.0:\n",
    "                        residual = spectrum - ap_fit\n",
    "                        num = float(np.sum(gauss * residual))\n",
    "                        alpha_amp = max(num / denom, 0.0)\n",
    "                epoch_feats.extend([offset, exponent, alpha_cf if has_profile else 0.0, alpha_amp, alpha_bw if has_profile else 0.0])\n",
    "            except (FitError, RuntimeError, ValueError, np.linalg.LinAlgError):\n",
    "                epoch_feats.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        features.append(epoch_feats)\n",
    "    return np.asarray(features, dtype=float)\n",
    "\n",
    "def fooof_feature_names(channels: List[str]) -> List[str]:\n",
    "    base_order = [\"offset\", \"exponent\", \"alpha_cf\", \"alpha_amp\", \"alpha_bw\"]\n",
    "    selected = [f for f in (FOOOF_SELECTED_FEATURES or base_order) if f in base_order]\n",
    "    if not selected:\n",
    "        selected = base_order\n",
    "    names = []\n",
    "    for ch in channels:\n",
    "        for feat in selected:\n",
    "            names.append(f\"{ch}_{feat}\")\n",
    "    return names\n",
    "\n",
    "def select_feature_columns_full(full_X: np.ndarray, channels: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    base_order = [\"offset\", \"exponent\", \"alpha_cf\", \"alpha_amp\", \"alpha_bw\"]\n",
    "    selected = [f for f in (FOOOF_SELECTED_FEATURES or base_order) if f in base_order]\n",
    "    if not selected:\n",
    "        selected = base_order\n",
    "    stride = len(base_order)\n",
    "    keep_offsets = [base_order.index(s) for s in selected]\n",
    "    idx = []\n",
    "    for ch_i in range(len(channels)):\n",
    "        base = ch_i * stride\n",
    "        for off in keep_offsets:\n",
    "            idx.append(base + off)\n",
    "    X_sel = full_X[:, idx]\n",
    "    return X_sel, fooof_feature_names(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Scan input files\n",
    "# -----------------\n",
    "\n",
    "def _maybe_wsl_path(p: str) -> Optional[str]:\n",
    "    m = re.match(r\"^([A-Za-z]):[\\\\\\\\/](.*)$\", str(p))\n",
    "    if not m:\n",
    "        return None\n",
    "    drive = m.group(1).lower()\n",
    "    rest = m.group(2).replace(\"\\\\\\\\\", \"/\").replace(\"\\\\\", \"/\")\n",
    "    return f\"/mnt/{drive}/{rest}\"\n",
    "\n",
    "def resolve_input_dirs() -> List[Path]:\n",
    "    candidates = list(INPUT_DIRS or [])\n",
    "    # WSL convenience for Windows paths\n",
    "    for c in list(candidates):\n",
    "        w = _maybe_wsl_path(str(c))\n",
    "        if w:\n",
    "            candidates.append(w)\n",
    "    # Common WSL shortcuts (harmless if they don't exist)\n",
    "    candidates += [\n",
    "        r\"/mnt/g/ChristianMusaeus/Preprocessed_setfiles\",\n",
    "        r\"/mnt/e/Saxe_sandkasse/30EOEC_filer/Open_marked\",\n",
    "        r\"/mnt/e/Saxe_sandkasse/30EOEC_filer/Closed_marked\",\n",
    "        r\"/mnt/g/ChristianMusaeus/New_EEG/Processed\",\n",
    "    ]\n",
    "    # Local fallback\n",
    "    candidates += [str((NOTEBOOK_DIR.parent / \"data\").resolve())]\n",
    "    found: List[Path] = []\n",
    "    for c in candidates:\n",
    "        if not c:\n",
    "            continue\n",
    "        try:\n",
    "            p = Path(str(c)).expanduser()\n",
    "            if p.exists() and p.is_dir():\n",
    "                found.append(p.resolve())\n",
    "        except Exception:\n",
    "            pass\n",
    "    # de-dup preserve order\n",
    "    seen = set()\n",
    "    out: List[Path] = []\n",
    "    for p in found:\n",
    "        key = str(p)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(p)\n",
    "    return out\n",
    "\n",
    "roots = resolve_input_dirs()\n",
    "print(\"Resolved input dirs:\", roots)\n",
    "if not roots:\n",
    "    raise RuntimeError(\"Could not resolve any INPUT_DIRS for this DATASET_OPTION. Update paths or set env vars.\")\n",
    "\n",
    "paths: List[Path] = []\n",
    "\n",
    "# Decide which file types to scan\n",
    "if int(DATASET_OPTION) == 3:\n",
    "    patterns = (\"*_epo.fif\", \"*_epo.FIF\", \"*.fif\", \"*.FIF\")\n",
    "else:\n",
    "    patterns = (\"*.set\", \"*.SET\")\n",
    "\n",
    "for root in roots:\n",
    "    for pat in patterns:\n",
    "        paths += sorted(root.rglob(pat) if RECURSIVE else root.glob(pat))\n",
    "paths = [p.resolve() for p in dict.fromkeys(paths)]\n",
    "print(\"Input files found:\", len(paths))\n",
    "if not paths:\n",
    "    raise RuntimeError(f\"No input files found under {roots} for patterns={patterns}. Check DATASET_OPTION and directory paths.\")\n",
    "if paths:\n",
    "    print(\"Example:\", paths[0])\n",
    "\n",
    "# Persist the input file list for reproducibility\n",
    "try:\n",
    "    outpath(\"input_files.txt\").write_text(\"\\n\".join(str(p) for p in paths), encoding=\"utf-8\")\n",
    "except Exception as exc:\n",
    "    print(\"Warning: could not write input_files.txt:\", exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Epoch loader + validations\n",
    "# -----------------\n",
    "\n",
    "def load_epochs(path: Path):\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == '.set':\n",
    "        return mne.io.read_epochs_eeglab(str(path), verbose='ERROR')\n",
    "    if suf == '.fif':\n",
    "        # MNE epochs saved as FIF (used by the NEW processed dataset)\n",
    "        return mne.read_epochs(str(path), preload=False, verbose='ERROR')\n",
    "    raise ValueError(f\"Unsupported file type: {path}\")\n",
    "\n",
    "def validate_epochs_basic(epochs, path: Path) -> Tuple[bool, str]:\n",
    "    try:\n",
    "        n_epochs = len(epochs)\n",
    "        n_ch = len(getattr(epochs, 'ch_names', []))\n",
    "        sfreq = float(epochs.info['sfreq'])\n",
    "        if n_epochs <= 0:\n",
    "            return False, 'no epochs'\n",
    "        if n_ch <= 0:\n",
    "            return False, 'no channels'\n",
    "        if not np.isfinite(sfreq) or sfreq <= 0:\n",
    "            return False, 'invalid sfreq'\n",
    "        return True, ''\n",
    "    except Exception as exc:\n",
    "        return False, f'bad epochs object: {exc}'\n",
    "\n",
    "\n",
    "\n",
    "def validate_saved_npz(npz_path: Path, expected_n_epochs: int, expected_feature_names: List[str]) -> Tuple[bool, str]:\n",
    "    try:\n",
    "        d = np.load(npz_path, allow_pickle=True)\n",
    "        if 'X' not in d or 'feature_names' not in d:\n",
    "            return False, 'missing keys'\n",
    "        X = np.asarray(d['X'])\n",
    "        names = [str(x) for x in np.asarray(d['feature_names']).ravel().tolist()]\n",
    "        if X.ndim != 2:\n",
    "            return False, f'X has wrong ndim: {X.ndim}'\n",
    "        if int(X.shape[0]) != int(expected_n_epochs):\n",
    "            return False, f'epoch count mismatch: {X.shape[0]} vs {expected_n_epochs}'\n",
    "        if len(names) != int(X.shape[1]):\n",
    "            return False, 'feature_names length mismatch'\n",
    "        if expected_feature_names and names != list(expected_feature_names):\n",
    "            return False, 'feature_names order mismatch'\n",
    "        if not np.all(np.isfinite(X)):\n",
    "            return False, 'non-finite values in X'\n",
    "        return True, ''\n",
    "    except Exception as exc:\n",
    "        return False, str(exc)\n",
    "# -----------------\n",
    "# Main loop\n",
    "# -----------------\n",
    "\n",
    "@dataclass\n",
    "class ProfileRow:\n",
    "    subject_id: int\n",
    "    file: str\n",
    "    alpha_cf: float\n",
    "    alpha_bw: float\n",
    "    n_epochs_used: int\n",
    "    roi_channels: str\n",
    "    n_channels: int\n",
    "    n_freqs: int\n",
    "\n",
    "profile_rows: List[ProfileRow] = []\n",
    "file_info_rows: List[dict] = []\n",
    "feature_manifest_rows: List[dict] = []\n",
    "\n",
    "def _fmt_secs(seconds: float) -> str:\n",
    "    seconds = max(0.0, float(seconds))\n",
    "    m, s = divmod(int(round(seconds)), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:d}h{m:02d}m\" if h else f\"{m:d}m{s:02d}s\"\n",
    "\n",
    "n_skipped = 0\n",
    "n_processed = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for i, path in enumerate(paths, start=1):\n",
    "    subj = parse_subject_id(path)\n",
    "    file_tag = _safe_tag(path.stem)\n",
    "    t_file = time.time()\n",
    "    print(f\"[{i}/{len(paths)}] {path.name} (subject_id={subj})\")\n",
    "\n",
    "    # Skip if already cached\n",
    "    if CACHE_MODE.upper() == \"B\" and SAVE_PER_FILE:\n",
    "        target_npz = outpath(f\"features_subject_{subj}__{file_tag}.npz\")\n",
    "        if target_npz.exists() and not OVERWRITE:\n",
    "            n_skipped += 1\n",
    "            msg = f\"  -> exists, skipping: {target_npz.name}\"\n",
    "            if ETA_EVERY and (i % int(ETA_EVERY) == 0):\n",
    "                elapsed = time.time() - t0\n",
    "                avg = elapsed / max(i, 1)\n",
    "                eta = avg * max(len(paths) - i, 0)\n",
    "                msg += f\" | elapsed {_fmt_secs(elapsed)} | ETA {_fmt_secs(eta)}\"\n",
    "            print(msg)\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        epochs = load_epochs(path)\n",
    "        ok, why = validate_epochs_basic(epochs, path)\n",
    "        if not ok:\n",
    "            print(\"  -> invalid epochs file (skipping):\", why)\n",
    "            continue\n",
    "        file_info_rows.append({\n",
    "            \"subject_id\": int(subj),\n",
    "            \"file\": str(path),\n",
    "            \"suffix\": str(path.suffix),\n",
    "            \"n_epochs\": int(len(epochs)),\n",
    "            \"n_channels\": int(len(epochs.ch_names)),\n",
    "            \"sfreq\": float(epochs.info[\"sfreq\"]),\n",
    "            \"original_ch_names\": \",\".join(map(str, epochs.ch_names)),\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        print(\"  -> failed to read as epochs .set (skipping):\", exc)\n",
    "        continue\n",
    "\n",
    "    epochs = rename_epochs_channels_canonical(epochs)\n",
    "    try:\n",
    "        if file_info_rows and int(file_info_rows[-1].get(\"subject_id\", -1)) == int(subj):\n",
    "            file_info_rows[-1][\"mapped_ch_names\"] = \",\".join(map(str, epochs.ch_names))\n",
    "    except Exception:\n",
    "        pass\n",
    "    sfreq = float(epochs.info['sfreq'])\n",
    "\n",
    "    if ALL_CHANNELS:\n",
    "        picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude='bads')\n",
    "        if len(picks_all) == 0:\n",
    "            picks_all = mne.pick_types(epochs.info, eeg=True, meg=False, stim=False, eog=False, exclude=[])\n",
    "        picks = picks_all\n",
    "        channels = [epochs.ch_names[idx] for idx in picks]\n",
    "    else:\n",
    "        requested = [str(ch).upper() for ch in CHANNEL_SELECTION]\n",
    "        name_lookup = {canonical_channel_name(ch).upper(): ch for ch in epochs.ch_names}\n",
    "        missing = [ch for ch in requested if ch not in name_lookup]\n",
    "        if missing:\n",
    "            print(\"  -> missing requested channels; skipping:\", missing)\n",
    "            continue\n",
    "        channels = [name_lookup[ch] for ch in requested]\n",
    "        picks = [epochs.ch_names.index(ch) for ch in channels]\n",
    "\n",
    "    data_all = epochs.get_data(picks=picks)\n",
    "    finite_mask = np.all(np.isfinite(data_all), axis=(1, 2))\n",
    "    if not np.any(finite_mask):\n",
    "        print(\"  -> no finite epochs; skipping\")\n",
    "        continue\n",
    "    if COMBINE_ADJACENT_EPOCHS:\n",
    "        finite_idx = np.flatnonzero(finite_mask)\n",
    "        pairs = []\n",
    "        cursor = 0\n",
    "        while cursor < len(finite_idx) - 1:\n",
    "            i0 = int(finite_idx[cursor])\n",
    "            i1 = int(finite_idx[cursor + 1])\n",
    "            if i1 == i0 + 1:\n",
    "                pairs.append((i0, i1))\n",
    "                cursor += 2\n",
    "            else:\n",
    "                cursor += 1\n",
    "        if not pairs:\n",
    "            print(\"  -> no adjacent finite epochs found for pairing; using single epochs\")\n",
    "            data = data_all[finite_mask]\n",
    "        else:\n",
    "            combined = [np.concatenate([data_all[i0], data_all[i1]], axis=1) for i0, i1 in pairs]\n",
    "            data = np.stack(combined, axis=0)\n",
    "    else:\n",
    "        data = data_all[finite_mask]\n",
    "\n",
    "    # PSD cube: (epochs, channels, freqs)\n",
    "    try:\n",
    "        psd, freqs = psd_array_welch_clean(\n",
    "            data,\n",
    "            sfreq=sfreq,\n",
    "            fmin=PSD_KWARGS.get('fmin', 1.0),\n",
    "            fmax=PSD_KWARGS.get('fmax', 45.0),\n",
    "            target_secs=TARGET_SECS,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(\"  -> PSD failed; skipping:\", exc)\n",
    "        continue\n",
    "\n",
    "    # Alpha profile from ROI mean spectrum\n",
    "    roi_names = [ch for ch in ALPHA_PROFILE_ROI if ch in channels]\n",
    "    if not roi_names:\n",
    "        print(\"  -> no ROI channels present; using all channels for profile\")\n",
    "        roi_idx = list(range(len(channels)))\n",
    "        roi_names = channels\n",
    "    else:\n",
    "        roi_idx = [channels.index(ch) for ch in roi_names]\n",
    "\n",
    "    roi_cube = psd[:, roi_idx, :]\n",
    "    mean_spectrum = np.nanmean(roi_cube, axis=(0, 1))\n",
    "    alpha_cf = 0.0\n",
    "    alpha_bw = 0.0\n",
    "    try:\n",
    "        model = SpectralModel(**FOOOF_SETTINGS)\n",
    "        lo, hi = ALPHA_PROFILE_RANGE\n",
    "        model.fit(freqs, mean_spectrum, freq_range=(float(lo), float(hi)))\n",
    "        peaks = np.asarray(getattr(model, 'peak_params_', []))\n",
    "        chosen = select_alpha_peak(peaks, float(lo), float(hi))\n",
    "        if chosen is not None:\n",
    "            alpha_cf, amp, alpha_bw = map(float, chosen[:3])\n",
    "    except Exception as exc:\n",
    "        print(\"  -> alpha profile fit failed; continuing with zeros:\", exc)\n",
    "\n",
    "    profile_rows.append(\n",
    "        ProfileRow(\n",
    "            subject_id=int(subj),\n",
    "            file=str(path),\n",
    "            alpha_cf=float(alpha_cf),\n",
    "            alpha_bw=float(alpha_bw),\n",
    "            n_epochs_used=int(psd.shape[0]),\n",
    "            roi_channels=','.join(map(str, roi_names)),\n",
    "            n_channels=int(psd.shape[1]),\n",
    "            n_freqs=int(psd.shape[2]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if CACHE_MODE.upper() == \"A\":\n",
    "        n_processed += 1\n",
    "        if ETA_EVERY and (i % int(ETA_EVERY) == 0):\n",
    "            elapsed = time.time() - t0\n",
    "            avg = elapsed / max(i, 1)\n",
    "            eta = avg * max(len(paths) - i, 0)\n",
    "            print(f\"  -> done in {_fmt_secs(time.time() - t_file)} | elapsed {_fmt_secs(elapsed)} | ETA {_fmt_secs(eta)}\")\n",
    "        continue\n",
    "\n",
    "    # Full per-epoch features\n",
    "    alpha_profile_map = {int(subj): (float(alpha_cf), float(alpha_bw))}\n",
    "    full_X = compute_one_main_fooof_features(freqs=freqs, psd_cube=psd, subject_id=int(subj), alpha_profile_map=alpha_profile_map, include_aperiodic=True)\n",
    "    X, feature_names = select_feature_columns_full(full_X, channels)\n",
    "\n",
    "    if SAVE_PER_FILE:\n",
    "        target_npz = outpath(f\"features_subject_{subj}__{file_tag}.npz\")\n",
    "        if target_npz.exists() and not OVERWRITE:\n",
    "            print(\"  -> exists, skipping save:\", target_npz.name)\n",
    "        else:\n",
    "            np.savez_compressed(\n",
    "                target_npz,\n",
    "                X=X.astype(np.float32),\n",
    "                feature_names=np.asarray(feature_names, dtype=object),\n",
    "                freqs=freqs.astype(np.float32),\n",
    "                channels=np.asarray(channels, dtype=object),\n",
    "                subject_id=np.int64(subj),\n",
    "                source_file=str(path),\n",
    "                alpha_cf=np.float32(alpha_cf),\n",
    "                alpha_bw=np.float32(alpha_bw),\n",
    "            )\n",
    "            feature_manifest_rows.append(\n",
    "                {\n",
    "                    \"subject_id\": int(subj),\n",
    "                    \"file\": str(path),\n",
    "                    \"file_tag\": str(file_tag),\n",
    "                    \"npz\": str(target_npz),\n",
    "                    \"n_epochs\": int(X.shape[0]),\n",
    "                    \"n_features\": int(X.shape[1]),\n",
    "                    \"alpha_cf\": float(alpha_cf),\n",
    "                    \"alpha_bw\": float(alpha_bw),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    n_processed += 1\n",
    "    if ETA_EVERY and (i % int(ETA_EVERY) == 0):\n",
    "        elapsed = time.time() - t0\n",
    "        avg = elapsed / max(i, 1)\n",
    "        eta = avg * max(len(paths) - i, 0)\n",
    "        print(f\"  -> done in {_fmt_secs(time.time() - t_file)} | elapsed {_fmt_secs(elapsed)} | ETA {_fmt_secs(eta)}\")\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"Done. Processed: {n_processed}, skipped: {n_skipped}, total: {len(paths)} | elapsed: {dt/60:.1f} min\")\n",
    "\n",
    "# Write outputs\n",
    "profiles_df = pd.DataFrame([r.__dict__ for r in profile_rows])\n",
    "profiles_path = outpath(\"alpha_profiles.csv\")\n",
    "profiles_df.to_csv(profiles_path, index=False)\n",
    "print(\"Wrote:\", profiles_path)\n",
    "\n",
    "if feature_manifest_rows:\n",
    "    manifest_df = pd.DataFrame(feature_manifest_rows)\n",
    "    manifest_path = outpath(\"feature_manifest.csv\")\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    print(\"Wrote:\", manifest_path)\n",
    "\n",
    "# Write file info summary\n",
    "if file_info_rows:\n",
    "    info_df = pd.DataFrame(file_info_rows)\n",
    "    info_path = outpath('file_info.csv')\n",
    "    info_df.to_csv(info_path, index=False)\n",
    "    print('Wrote:', info_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
