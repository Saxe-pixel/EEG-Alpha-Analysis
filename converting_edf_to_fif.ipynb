{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718783a8",
   "metadata": {},
   "source": [
    "# Convert new EDF EEG files → preprocessed MNE FIF\n",
    "\n",
    "This notebook mirrors the original preprocessing pipeline (resample 200 Hz, 1–70 Hz band-pass, 50 Hz notch, keep first 19 channels, 1 s fixed-length epochs) but replaces the old I/O:\n",
    "\n",
    "- **Input:** `.edf` files in `G:\\ChristianMusaeus\\New_EEG\\Clean_raw`\n",
    "- **Output:** MNE `.fif` files in `G:\\ChristianMusaeus\\New_EEG\\Processed`\n",
    "\n",
    "Additionally, it uses EDF annotations to extract the requested segments:\n",
    "\n",
    "- **EO:** from `EO2` → `EO3`\n",
    "- **EC:** from `EC2` → `EC3`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e550e98",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- The EDF files already include event annotations (e.g., `EO1/EO2/EO3`, `EC1/EC2/EC3`).\n",
    "- This notebook saves both the **preprocessed Raw segments** and the **1-second Epochs** as `.fif`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42996386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# Avoid numba caching/JIT issues in some environments\n",
    "os.environ.setdefault(\"NUMBA_DISABLE_JIT\", \"1\")\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f431ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _guess_project_root(max_up: int = 6) -> Path:\n",
    "    \"\"\"Find repo/project root so relative paths work regardless of notebook CWD.\"\"\"\n",
    "    p = Path.cwd().resolve()\n",
    "    for _ in range(max_up + 1):\n",
    "        if (p / \"data\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "project_root = _guess_project_root()\n",
    "\n",
    "# Windows default paths (requested)\n",
    "windows_edf_dir = Path(r\"G:\\ChristianMusaeus\\New_EEG\\Clean_raw\")\n",
    "windows_output_dir = Path(r\"G:\\ChristianMusaeus\\New_EEG\\Processed\")\n",
    "\n",
    "# Keep the notebook usable on non-Windows machines by falling back to repo-relative data dirs.\n",
    "edf_dir = windows_edf_dir if os.name == \"nt\" else (project_root / \"data\" / \"New_EEG.nosync\")\n",
    "output_dir = windows_output_dir if os.name == \"nt\" else (project_root / \"data\" / \"NEW_processed.nosync\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"EDF dir:\", edf_dir, \"(exists:\", edf_dir.exists(), \")\")\n",
    "print(\"Output dir:\", output_dir)\n",
    "\n",
    "# SUPPRESS VERBOSE OUTPUT\n",
    "mne.set_log_level('ERROR')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def _first_annotation_onset(raw: mne.io.BaseRaw, description: str) -> float:\n",
    "    \"\"\"Return the first onset (seconds) for a given annotation description.\n",
    "\n",
    "    EDF annotation labels are not always perfectly consistent (e.g. case/whitespace),\n",
    "    so we match on a normalized (strip + upper) representation.\n",
    "    \"\"\"\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        raise ValueError(\"No annotations found in EDF.\")\n",
    "\n",
    "    ann = np.asarray(raw.annotations.description, dtype=str)\n",
    "    ann_norm = np.char.upper(np.char.strip(ann))\n",
    "    target = str(description).strip().upper()\n",
    "    idx = np.where(ann_norm == target)[0]\n",
    "    if idx.size == 0:\n",
    "        available = sorted({str(x).strip() for x in ann.tolist() if str(x).strip()})\n",
    "        preview = available[:30]\n",
    "        suffix = \"...\" if len(available) > len(preview) else \"\"\n",
    "        raise ValueError(\n",
    "            f\"Missing annotation: {description}. Available (n={len(available)}): {preview}{suffix}\"\n",
    "        )\n",
    "    return float(raw.annotations.onset[int(idx[0])])\n",
    "\n",
    "import re\n",
    "\n",
    "def _canonicalize_ch_name(name: str) -> str:\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r\"^EEG\\s+\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"-REF$\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    return s.upper()\n",
    "\n",
    "# Desired *stored* channel names (exact order)\n",
    "TARGET_CHANNELS = [\n",
    "    \"EEG Fp1-REF\",\n",
    "    \"EEG Fp2-REF\",\n",
    "    \"EEG F3-REF\",\n",
    "    \"EEG F4-REF\",\n",
    "    \"EEG C3-REF\",\n",
    "    \"EEG C4-REF\",\n",
    "    \"EEG P3-REF\",\n",
    "    \"EEG P4-REF\",\n",
    "    \"EEG O1-REF\",\n",
    "    \"EEG O2-REF\",\n",
    "    \"EEG F7-REF\",\n",
    "    \"EEG F8-REF\",\n",
    "    \"EEG T7-REF\",\n",
    "    \"EEG T8-REF\",\n",
    "    \"EEG P7-REF\",\n",
    "    \"EEG P8-REF\",\n",
    "    \"EEG Fz-REF\",\n",
    "    \"EEG Cz-REF\",\n",
    "    \"EEG Pz-REF\",\n",
    "]\n",
    "TARGET_CANONICAL = [_canonicalize_ch_name(x) for x in TARGET_CHANNELS]\n",
    "\n",
    "# If some recordings contain T9/T10 instead of Cz/Pz, you can substitute (not recommended unless you know it is correct).\n",
    "# By default we fail fast so all saved FIFs are truly comparable.\n",
    "ALLOW_T9_T10_SUBSTITUTE_FOR_CZ_PZ = False\n",
    "SUBSTITUTE_MAP = {\"CZ\": \"T9\", \"PZ\": \"T10\"}\n",
    "\n",
    "def _preprocess_raw_like_original(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    \"\"\"Mirror the original pipeline, but enforce a fixed 19-channel 10-20 set.\n",
    "\n",
    "    This avoids per-file channel differences (e.g. Cz/Pz vs T9/T10) that would otherwise\n",
    "    change downstream feature dimensionality.\n",
    "    \"\"\"\n",
    "    # Step 1: Resample to 200 Hz\n",
    "    raw = raw.resample(200)\n",
    "    # Step 2: Band-pass filter from 1-70 Hz\n",
    "    raw = raw.filter(l_freq=1.0, h_freq=70.0)\n",
    "    # Step 3: Notch filter at 50 Hz\n",
    "    raw = raw.notch_filter(freqs=50)\n",
    "\n",
    "    # Step 4: Pick the desired channels by *name* (not by position)\n",
    "    available = {_canonicalize_ch_name(ch): ch for ch in raw.ch_names}\n",
    "    desired_actual = []\n",
    "    missing = []\n",
    "    for canon in TARGET_CANONICAL:\n",
    "        key = canon\n",
    "        if key not in available and ALLOW_T9_T10_SUBSTITUTE_FOR_CZ_PZ:\n",
    "            sub = SUBSTITUTE_MAP.get(key, None)\n",
    "            if sub:\n",
    "                key = sub.upper()\n",
    "        actual = available.get(key, None)\n",
    "        if actual is None:\n",
    "            missing.append(canon)\n",
    "        else:\n",
    "            desired_actual.append(actual)\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{edf_path.name}: Missing required channels: {missing}. \"\n",
    "            f\"Available (canonical): {sorted(available.keys())}\"\n",
    "        )\n",
    "\n",
    "    # Enforce ordering\n",
    "    try:\n",
    "        raw.pick_channels(desired_actual, ordered=True)\n",
    "    except TypeError:\n",
    "        raw.pick_channels(desired_actual)\n",
    "        raw.reorder_channels(desired_actual)\n",
    "\n",
    "    # Rename to the desired *stored* names\n",
    "    mapping = {old: new for old, new in zip(raw.ch_names, TARGET_CHANNELS) if old != new}\n",
    "    if mapping:\n",
    "        raw.rename_channels(mapping)\n",
    "\n",
    "    if list(raw.ch_names) != list(TARGET_CHANNELS):\n",
    "        raise RuntimeError(f\"Channel standardization failed. Got: {raw.ch_names}\")\n",
    "\n",
    "    return raw\n",
    "\n",
    "def _epochs_like_original(raw: mne.io.BaseRaw) -> mne.Epochs:\n",
    "    \"\"\"Create 1-second fixed-length epochs (same as original).\"\"\"\n",
    "    events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        tmin=0.0,\n",
    "        tmax=1.0,\n",
    "        baseline=None,\n",
    "        preload=True,\n",
    "        reject_by_annotation=True,\n",
    "    )\n",
    "    return epochs\n",
    "\n",
    "def preprocess_edf_to_fif(edf_path: Path, out_dir: Path):\n",
    "    \"\"\"One output FIF per EDF: whole recording epoched into 1s windows.\n",
    "\n",
    "    Epochs that fall fully within EO2→EO3 are labeled 'EO'.\n",
    "    Epochs that fall fully within EC2→EC3 are labeled 'EC'.\n",
    "    All other epochs are labeled 'OTHER'.\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=\"ERROR\")\n",
    "    raw = _preprocess_raw_like_original(raw)\n",
    "\n",
    "    # Sanity check: show resulting channels (debug)\n",
    "    try:\n",
    "        print('  Channels after standardization (n=%d):' % len(raw.ch_names))\n",
    "        for ch in raw.ch_names:\n",
    "            print('   -', ch)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Extract EO2→EO3 and EC2→EC3 windows using annotation onsets (seconds)\n",
    "    t_eo2 = _first_annotation_onset(raw, \"EO2\")\n",
    "    t_eo3 = _first_annotation_onset(raw, \"EO3\")\n",
    "    t_ec2 = _first_annotation_onset(raw, \"EC2\")\n",
    "    t_ec3 = _first_annotation_onset(raw, \"EC3\")\n",
    "    if not (t_eo2 < t_eo3):\n",
    "        raise ValueError(f\"Invalid EO window: EO2={t_eo2} EO3={t_eo3}\")\n",
    "    if not (t_ec2 < t_ec3):\n",
    "        raise ValueError(f\"Invalid EC window: EC2={t_ec2} EC3={t_ec3}\")\n",
    "\n",
    "    # Fixed-length events over the entire recording (same logic as original)\n",
    "    events = mne.make_fixed_length_events(raw, duration=1.0)\n",
    "    sfreq = float(raw.info['sfreq'])\n",
    "\n",
    "    # Label each epoch by whether it lies inside the requested windows\n",
    "    epoch_starts = (events[:, 0] - raw.first_samp) / sfreq\n",
    "    epoch_ends = epoch_starts + 1.0\n",
    "\n",
    "    EO_CODE, EC_CODE, OTHER_CODE = 1, 2, 3\n",
    "    codes = np.full(events.shape[0], OTHER_CODE, dtype=int)\n",
    "    in_eo = (epoch_starts >= t_eo2) & (epoch_ends <= t_eo3)\n",
    "    in_ec = (epoch_starts >= t_ec2) & (epoch_ends <= t_ec3)\n",
    "    codes[in_eo] = EO_CODE\n",
    "    codes[in_ec] = EC_CODE\n",
    "    events_labeled = events.copy()\n",
    "    events_labeled[:, 2] = codes\n",
    "\n",
    "    event_id = {\"EO\": EO_CODE, \"EC\": EC_CODE, \"OTHER\": OTHER_CODE}\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events_labeled,\n",
    "        event_id=event_id,\n",
    "        tmin=0.0,\n",
    "        tmax=1.0,\n",
    "        baseline=None,\n",
    "        preload=True,\n",
    "        reject_by_annotation=True,\n",
    "    )\n",
    "\n",
    "    out_path = out_dir / f\"{edf_path.stem}_epo.fif\"\n",
    "    epochs.save(str(out_path), overwrite=True)\n",
    "    return out_path\n",
    "\n",
    "edf_files = sorted(edf_dir.glob(\"*.edf\")) + sorted(edf_dir.glob(\"*.EDF\"))\n",
    "edf_files = sorted({p.resolve() for p in edf_files})\n",
    "print(f\"Found {len(edf_files)} EDF files in {edf_dir}\")\n",
    "\n",
    "# If True, keeps the output tidy, but it also hides errors as the notebook runs.\n",
    "# For debugging missing files, keep this False.\n",
    "USE_CLEAR_OUTPUT = False\n",
    "\n",
    "processed = 0\n",
    "failures = []  # list[dict]\n",
    "\n",
    "for i, edf_path in enumerate(edf_files, 1):\n",
    "    if USE_CLEAR_OUTPUT:\n",
    "        clear_output(wait=True)\n",
    "    print(f\"[{i}/{len(edf_files)}] Processing {edf_path.name}...\")\n",
    "    try:\n",
    "        out_path = preprocess_edf_to_fif(edf_path, output_dir)\n",
    "        if not out_path.exists():\n",
    "            raise FileNotFoundError(f\"Expected output not found after save: {out_path}\")\n",
    "        processed += 1\n",
    "        print(f\"  Saved {out_path.name} ({processed} processed so far)\")\n",
    "    except Exception as e:\n",
    "        failures.append(\n",
    "            {\n",
    "                \"edf\": str(edf_path),\n",
    "                \"error\": repr(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "            }\n",
    "        )\n",
    "        print(f\"  Error: {edf_path.name}: {repr(e)}\")\n",
    "\n",
    "print(f\"\\nTotal EDF files successfully processed: {processed} / {len(edf_files)}\")\n",
    "\n",
    "# Summarize failures clearly (and persist a report file for sharing/debugging)\n",
    "if failures:\n",
    "    failed_names = [Path(x[\"edf\"]).name for x in failures]\n",
    "    print(f\"\\nFailed EDF files (n={len(failed_names)}):\")\n",
    "    for name in failed_names:\n",
    "        print(\" -\", name)\n",
    "\n",
    "    report_path = output_dir / \"edf_to_fif_failures.json\"\n",
    "    try:\n",
    "        report_path.write_text(json.dumps(failures, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"\\nWrote failure report: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not write failure report ({report_path}): {repr(e)}\")\n",
    "\n",
    "# Cross-check: which outputs are missing?\n",
    "expected_outputs = {f\"{p.stem}_epo.fif\" for p in edf_files}\n",
    "existing_outputs = {p.name for p in output_dir.glob(\"*_epo.fif\")}\n",
    "missing_outputs = sorted(expected_outputs - existing_outputs)\n",
    "if missing_outputs:\n",
    "    print(f\"\\nMissing output FIFs (n={len(missing_outputs)}):\")\n",
    "    for name in missing_outputs:\n",
    "        print(\" -\", name)\n",
    "else:\n",
    "    print(\"\\nAll expected output FIFs are present.\")\n",
    "\n",
    "# Optional: retry only the failed EDF files once more (useful if the failure was transient).\n",
    "RETRY_FAILED = True\n",
    "if RETRY_FAILED and failures:\n",
    "    print(\"\\nRetrying failed EDF files...\")\n",
    "    retry_failures = []\n",
    "    retry_success = 0\n",
    "\n",
    "    retry_targets = sorted({Path(x[\"edf\"]).resolve() for x in failures})\n",
    "    for i, edf_path in enumerate(retry_targets, 1):\n",
    "        print(f\"[retry {i}/{len(retry_targets)}] {edf_path.name}...\")\n",
    "        try:\n",
    "            out_path = preprocess_edf_to_fif(edf_path, output_dir)\n",
    "            if not out_path.exists():\n",
    "                raise FileNotFoundError(f\"Expected output not found after save: {out_path}\")\n",
    "            retry_success += 1\n",
    "            print(f\"  Saved {out_path.name}\")\n",
    "        except Exception as e:\n",
    "            retry_failures.append(\n",
    "                {\n",
    "                    \"edf\": str(edf_path),\n",
    "                    \"error\": repr(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                }\n",
    "            )\n",
    "            print(f\"  Retry error: {edf_path.name}: {repr(e)}\")\n",
    "\n",
    "    print(f\"\\nRetry summary: {retry_success} succeeded, {len(retry_failures)} still failing\")\n",
    "    if retry_failures:\n",
    "        retry_report_path = output_dir / \"edf_to_fif_retry_failures.json\"\n",
    "        try:\n",
    "            retry_report_path.write_text(json.dumps(retry_failures, indent=2), encoding=\"utf-8\")\n",
    "            print(f\"Wrote retry failure report: {retry_report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not write retry failure report ({retry_report_path}): {repr(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6b84b",
   "metadata": {},
   "source": [
    "### Sanity check: print the number of epochs per saved FIF\n",
    "\n",
    "The epoch count depends on the duration of the EO2→EO3 and EC2→EC3 windows and the fixed 1 s epoching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "epo_files = sorted(output_dir.glob(\"*_epo.fif\"))\n",
    "print(f\"Found {len(epo_files)} epoch FIF files in {output_dir}\")\n",
    "\n",
    "for i, path in enumerate(epo_files, 1):\n",
    "    try:\n",
    "        epochs = mne.read_epochs(str(path), preload=False, verbose=False)\n",
    "        counts = {name: int(np.sum(epochs.events[:, 2] == code)) for name, code in epochs.event_id.items()}\n",
    "        print(f\"[{i}/{len(epo_files)}] {path.name} — Epochs: {len(epochs)} — counts: {counts}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}/{len(epo_files)}] {path.name} — Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
